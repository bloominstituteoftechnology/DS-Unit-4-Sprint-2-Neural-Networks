{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y6SKlgYrpcym"
   },
   "source": [
    "# Neural Networks Sprint Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BrEbRrjVphPM"
   },
   "source": [
    "## 1) Define the following terms:\n",
    "\n",
    "- Neuron\n",
    "- Input Layer\n",
    "- Hidden Layer\n",
    "- Output Layer\n",
    "- Activation\n",
    "- Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q5EksLqnp4oB"
   },
   "source": [
    "#### Neuron:\n",
    "or a perceptron, is a basic unit of a neural network, it has several inputs, for each input there is a weight (weight of that spesific connection). When the artifitial neuron activates, it computes its state, by adding all the incoming inputs multiplied by its corresponding connection weight. After computing its state the neuron passes it through its activation function, which normalizes the result. (between 0:1, -1:1, or only +). The neuron / perceptron consists of 4 parts:\n",
    "Input values or One input layer\n",
    "Weights and Bias\n",
    "Net sum\n",
    "Activation Function\n",
    "\n",
    "#### Input Layer:\n",
    "is also called a visible layer, since we see and interact with it. This is where we feed a dataset into Neural Networks. The input layer passes the data directly to the first hidden layer where the data is multiplied by the first hidden layer's weights. Also, the input layer might have its own weights that multiply the incoming data.\n",
    "\n",
    "\n",
    "#### Hidden Layer:\n",
    "is a layer which transforms inputs from the previous layer into something that the output layer can use. A feed forward neural network applies a series of functions to the data. The exact function will depend on the neural network (for ex., it can be a linear transformation of the previous layer, followed by a squashing nonlinearity, or computing logical functions). This layer is responsible extracting the required features from the input data.\n",
    "\n",
    "#### Output Layer:\n",
    "The output layer of the neural network collects and transmits the information accordingly in way it has been designed to give. The pattern presented by the output layer can be directly traced back to the input layer.\n",
    "\n",
    "#### Activation:\n",
    "The activation is the result of applying activation function to a weighted sum of inputs in a neuron. The activation function is the non linear transformation that we do over weighted sum of inputs. This activation is then sent to the next layer of neurons as input. There are several activation functions, such as sigmoid, tahn, relu, LeakyRelU, softmax, and others.\n",
    "\n",
    "#### Backpropagation:\n",
    "is short for 'Backwards propagation of errors' and refers to the process/algorithm for how weights in NN are updated in reverse at the end of each training epoch. The weights are updated by comparing the desired and actual output of NN.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ri_gRA2Jp728"
   },
   "source": [
    "## 2) Create a perceptron class that can model the behavior of an AND gate. You can use the following table as your training data:\n",
    "\n",
    "| x1 | x2 | x3 | y |\n",
    "|----|----|----|---|\n",
    "| 1  | 1  | 1  | 1 |\n",
    "| 1  | 0  | 1  | 0 |\n",
    "| 0  | 1  | 1  | 0 |\n",
    "| 0  | 0  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized weights after training: \n",
      "[[ 11.84079538]\n",
      " [ 11.84079538]\n",
      " [-18.04950231]]\n",
      "Output After Training:\n",
      "[[9.96431333e-01]\n",
      " [2.00799905e-03]\n",
      " [2.00799905e-03]\n",
      " [1.44987830e-08]]\n"
     ]
    }
   ],
   "source": [
    "##### Your Code Here #####\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "inputs = np.array([[1,1,1],\n",
    "                   [1,0,1],\n",
    "                   [0,1,1],\n",
    "                   [0,0,1]])\n",
    "\n",
    "correct_outputs = [[1],\n",
    "                  [0],\n",
    "                  [0],\n",
    "                  [0]]\n",
    "\n",
    "class Perceptron(object):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "  \n",
    "    def train(self):\n",
    "        weights = 2 * np.random.random((3,1)) - 1\n",
    "        for iteration in range(10000):\n",
    "\n",
    "            # Weighted sum of inputs and weights\n",
    "            weighted_sum = np.dot(self.X, weights)\n",
    "\n",
    "            # Activate with sigmoid function\n",
    "            activated_output = sigmoid(weighted_sum)\n",
    "\n",
    "            # Calculate Error\n",
    "            error = self.y - activated_output\n",
    "\n",
    "            # Calculate weight adjustments with sigmoid_derivative\n",
    "            adjustments = error * sigmoid_derivative(activated_output)\n",
    "\n",
    "            # Update weights\n",
    "            weights += np.dot(self.X.T, adjustments)\n",
    "            \n",
    "        return weights, activated_output\n",
    "    \n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(x):\n",
    "        return sigmoid(x) * (1 - sigmoid(x))\n",
    "        \n",
    "ppn = Perceptron(inputs,correct_outputs)\n",
    "weights, activated_output = ppn.train()\n",
    "print('optimized weights after training: ')\n",
    "print(weights)\n",
    "\n",
    "print(\"Output After Training:\")\n",
    "print(activated_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "86HyRi8Osr3U"
   },
   "source": [
    "## 3) Implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. \n",
    "- Your network must have one hidden layer. \n",
    "- You do not have to update weights via gradient descent. You can use something like the derivative of the sigmoid function to update weights.\n",
    "- Train your model on the Heart Disease dataset from UCI:\n",
    "\n",
    "[Github Dataset](https://github.com/ryanleeallred/datasets/blob/master/heart.csv)\n",
    "\n",
    "[Raw File on Github](https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CNfiajv3v4Ed"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Your Code Here #####\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 303 entries, 0 to 302\n",
      "Data columns (total 14 columns):\n",
      "age         303 non-null int64\n",
      "sex         303 non-null int64\n",
      "cp          303 non-null int64\n",
      "trestbps    303 non-null int64\n",
      "chol        303 non-null int64\n",
      "fbs         303 non-null int64\n",
      "restecg     303 non-null int64\n",
      "thalach     303 non-null int64\n",
      "exang       303 non-null int64\n",
      "oldpeak     303 non-null float64\n",
      "slope       303 non-null int64\n",
      "ca          303 non-null int64\n",
      "thal        303 non-null int64\n",
      "target      303 non-null int64\n",
      "dtypes: float64(1), int64(13)\n",
      "memory usage: 33.2 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 13)\n",
      "(303, 1)\n"
     ]
    }
   ],
   "source": [
    "X = df.iloc[:, 0:13].values\n",
    "y = df.target.values.reshape([-1,1])\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.9521966 ,  0.68100522,  1.97312292, ..., -2.27457861,\n",
       "        -0.71442887, -2.14887271],\n",
       "       [-1.91531289,  0.68100522,  1.00257707, ..., -2.27457861,\n",
       "        -0.71442887, -0.51292188],\n",
       "       [-1.47415758, -1.46841752,  0.03203122, ...,  0.97635214,\n",
       "        -0.71442887, -0.51292188],\n",
       "       ...,\n",
       "       [ 1.50364073,  0.68100522, -0.93851463, ..., -0.64911323,\n",
       "         1.24459328,  1.12302895],\n",
       "       [ 0.29046364,  0.68100522, -0.93851463, ..., -0.64911323,\n",
       "         0.26508221,  1.12302895],\n",
       "       [ 0.29046364, -1.46841752,  0.03203122, ..., -0.64911323,\n",
       "         0.26508221, -0.51292188]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------- EPOCH 1 -----------+\n",
      "Loss: \n",
      "0.2509174507963789\n",
      "\n",
      "\n",
      "+---------- EPOCH 2 -----------+\n",
      "Loss: \n",
      "0.34095199016331684\n",
      "\n",
      "\n",
      "+---------- EPOCH 3 -----------+\n",
      "Loss: \n",
      "0.20542954978187786\n",
      "\n",
      "\n",
      "+---------- EPOCH 4 -----------+\n",
      "Loss: \n",
      "0.15617051744510266\n",
      "\n",
      "\n",
      "+---------- EPOCH 5 -----------+\n",
      "Loss: \n",
      "0.1446954533537302\n",
      "\n",
      "\n",
      "+---------- EPOCH 50 -----------+\n",
      "Loss: \n",
      "0.10344412123964802\n",
      "\n",
      "\n",
      "+---------- EPOCH 100 -----------+\n",
      "Loss: \n",
      "0.09217047816624396\n",
      "\n",
      "\n",
      "+---------- EPOCH 150 -----------+\n",
      "Loss: \n",
      "0.08969785329360074\n",
      "\n",
      "\n",
      "+---------- EPOCH 200 -----------+\n",
      "Loss: \n",
      "0.09496198852775192\n",
      "\n",
      "\n",
      "+---------- EPOCH 250 -----------+\n",
      "Loss: \n",
      "0.09307446845922798\n",
      "\n",
      "\n",
      "+---------- EPOCH 300 -----------+\n",
      "Loss: \n",
      "0.0965772415610863\n",
      "\n",
      "\n",
      "+---------- EPOCH 350 -----------+\n",
      "Loss: \n",
      "0.08562423557132108\n",
      "\n",
      "\n",
      "+---------- EPOCH 400 -----------+\n",
      "Loss: \n",
      "0.08928550513178575\n",
      "\n",
      "\n",
      "+---------- EPOCH 450 -----------+\n",
      "Loss: \n",
      "0.08697944828435508\n",
      "\n",
      "\n",
      "+---------- EPOCH 500 -----------+\n",
      "Loss: \n",
      "0.08671383025666864\n",
      "\n",
      "\n",
      "+---------- EPOCH 550 -----------+\n",
      "Loss: \n",
      "0.08643074608047283\n",
      "\n",
      "\n",
      "+---------- EPOCH 600 -----------+\n",
      "Loss: \n",
      "0.086276002233446\n",
      "\n",
      "\n",
      "+---------- EPOCH 650 -----------+\n",
      "Loss: \n",
      "0.08599212826078513\n",
      "\n",
      "\n",
      "+---------- EPOCH 700 -----------+\n",
      "Loss: \n",
      "0.08593002830076565\n",
      "\n",
      "\n",
      "+---------- EPOCH 750 -----------+\n",
      "Loss: \n",
      "0.08564878241609115\n",
      "\n",
      "\n",
      "+---------- EPOCH 800 -----------+\n",
      "Loss: \n",
      "0.0856385233067211\n",
      "\n",
      "\n",
      "+---------- EPOCH 850 -----------+\n",
      "Loss: \n",
      "0.08536426374264233\n",
      "\n",
      "\n",
      "+---------- EPOCH 900 -----------+\n",
      "Loss: \n",
      "0.08540654889834727\n",
      "\n",
      "\n",
      "+---------- EPOCH 950 -----------+\n",
      "Loss: \n",
      "0.08517538165615367\n",
      "\n",
      "\n",
      "+---------- EPOCH 1000 -----------+\n",
      "Loss: \n",
      "0.08331775570561091\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "class Neural_Network(object):\n",
    "    def __init__(self):\n",
    "        self.inputs = 13\n",
    "        self.hiddenNodes = 3\n",
    "        self.outputNodes = 1\n",
    "\n",
    "        # Initialize Weights:\n",
    "        self.L1_weights = np.random.randn(self.inputs, self.hiddenNodes) \n",
    "        self.L2_weights = np.random.randn(self.hiddenNodes, self.outputNodes) \n",
    "\n",
    "    def feed_forward(self, X):\n",
    "\n",
    "        # Weighted sum between inputs and hidden layer:\n",
    "        self.hidden_sum = np.dot(X, self.L1_weights)\n",
    "\n",
    "        # Activations of weighted sum:\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "\n",
    "        # Weighted sum between hidden and output:\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.L2_weights)\n",
    "\n",
    "        # final activation of output:\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "\n",
    "        return self.activated_output\n",
    "\n",
    "    def sigmoid(self, s):\n",
    "        return 1/(1+np.exp(-s))\n",
    "\n",
    "    #sigmoid derivative  \n",
    "    def sigmoidPrime(self, s):\n",
    "        return s * (1 - s)\n",
    "\n",
    "    \n",
    "    def backward(self, X, y, output):\n",
    "        # backward propgate through the network\n",
    "\n",
    "        # error in output:\n",
    "        self.output_error = y - output \n",
    "\n",
    "        # applying derivative of sigmoid to error:\n",
    "        self.output_delta = self.output_error * self.sigmoidPrime(output) \n",
    "\n",
    "        # z2 error: how much our hidden layer weights contributed to output error:\n",
    "        self.z2_error = self.output_delta.dot(self.L2_weights.T)\n",
    "\n",
    "        # applying derivative of sigmoid to z2 error:\n",
    "        self.z2_delta = self.z2_error*self.sigmoidPrime(self.activated_hidden) \n",
    "\n",
    "        # adjusting first set (input --> hidden) weights:\n",
    "        self.L1_weights += X.T.dot(self.z2_delta) \n",
    "\n",
    "        # adjusting second set (hidden --> output) weights:\n",
    "        self.L2_weights += self.activated_hidden.T.dot(self.output_delta) \n",
    "\n",
    "    def train (self, X, y):\n",
    "        output = self.feed_forward(X)\n",
    "        self.backward(X, y, output)\n",
    "    \n",
    "NN = Neural_Network()\n",
    "for i in range(1000): # trains the NN 1,000 times\n",
    "  if i+1 in [1,2,3,4,5] or (i+1) % 50 == 0:\n",
    "    print('+---------- EPOCH', i+1, '-----------+')\n",
    "#     print(\"Input: \\n\", X) \n",
    "#     print(\"Actual Output: \\n\", y)  \n",
    "#     print(\"Predicted Output: \\n\" + str(NN.feed_forward(X))) \n",
    "    print(\"Loss: \\n\" + str(np.mean(np.square(y - NN.feed_forward(X))))) # mean sum squared loss\n",
    "    print(\"\\n\")\n",
    "  NN.train(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GGT1oRzXw3H9"
   },
   "source": [
    "## 4) Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy. \n",
    "\n",
    "- Use the Heart Disease Dataset (binary classification)\n",
    "- Use an appropriate loss function for a binary classification task\n",
    "- Use an appropriate activation function on the final layer of your network. \n",
    "- Train your model using verbose output for ease of grading.\n",
    "- Use GridSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
    "- When hyperparameter tuning, show you work by adding code cells for each new experiment. \n",
    "- Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
    "- You must hyperparameter tune at least 5 parameters in order to get a 3 on this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD, Adam, Nadam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import category_encoders as ce\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split,cross_val_score, StratifiedKFold, KFold, GridSearchCV \n",
    "              \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XWw4IYxLxKwH"
   },
   "outputs": [],
   "source": [
    "##### Your Code Here #####\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD, Adam, Nadam\n",
    "\n",
    "def create_model(lr=0.05,\n",
    "                 activation='relu',                 \n",
    "                 input_shape=(X.shape[1],),\n",
    "                 optimizer=Adam,\n",
    "                 relu_alpha = 0.003,\n",
    "                 dropout_rate = 0.2,\n",
    "                weight_initializer='random_normal'):\n",
    "    \n",
    "    # initialize a model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # add input layer\n",
    "    model.add(Dense(10, input_shape=input_shape, kernel_initializer=weight_initializer,))\n",
    "    model.add(LeakyReLU(alpha=relu_alpha)) \n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "\n",
    "    \n",
    "    # add hidden layers\n",
    "    model.add(Dense(10, kernel_initializer=weight_initializer,))\n",
    "    model.add(LeakyReLU(alpha=relu_alpha)) \n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "        \n",
    "    model.add(Dense(10, kernel_initializer=weight_initializer,))\n",
    "    model.add(LeakyReLU(alpha=relu_alpha)) \n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "    \n",
    "    model.add(Dense(8, kernel_initializer=weight_initializer,))\n",
    "    model.add(LeakyReLU(alpha=relu_alpha)) \n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "\n",
    "    \n",
    "    # add final output layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # optimizer\n",
    "    optimizer=optimizer(lr=lr)\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['acc'])\n",
    "              \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.6897689856515072 using {'batch_size': 60, 'epochs': 20}\n",
      "Means: 0.5511551115772512, Stdev: 0.17901057722136335 with: {'batch_size': 20, 'epochs': 20}\n",
      "Means: 0.6897689856515072, Stdev: 0.09576645732787446 with: {'batch_size': 60, 'epochs': 20}\n",
      "Means: 0.6303630315824704, Stdev: 0.09878963523601354 with: {'batch_size': 100, 'epochs': 20}\n"
     ]
    }
   ],
   "source": [
    "model = KerasClassifier(build_fn=create_model, \n",
    "                               epochs=epochs,\n",
    "                               batch_size=100,\n",
    "                               verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "param_grid = {'batch_size': [20, 60, 100],\n",
    "              'epochs': [20]}\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")\n",
    "#Best: 0.6501650212228102 using {'batch_size': 20, 'epochs': 20}    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.6963696316523914 using {'batch_size': 60, 'epochs': 30}\n",
      "Means: 0.6666666772892766, Stdev: 0.13143437280589265 with: {'batch_size': 60, 'epochs': 20}\n",
      "Means: 0.6963696316523914, Stdev: 0.04452389308394785 with: {'batch_size': 60, 'epochs': 30}\n",
      "Means: 0.66996699669967, Stdev: 0.10550737415285161 with: {'batch_size': 60, 'epochs': 40}\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "model = KerasClassifier(build_fn=create_model, \n",
    "                               epochs=epochs,\n",
    "                               batch_size=60,\n",
    "                               verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "param_grid = {'batch_size': [60],\n",
    "              'epochs': [20, 30, 40]}\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.7194719477848645 using {'dropout_rate': 0.2}\n",
      "Means: 0.6798679891592598, Stdev: 0.11035151162175309 with: {'dropout_rate': 0.0}\n",
      "Means: 0.7194719477848645, Stdev: 0.025986831487018202 with: {'dropout_rate': 0.2}\n",
      "Means: 0.6600660036499351, Stdev: 0.14241221521980998 with: {'dropout_rate': 0.3}\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, \n",
    "                               epochs=30,\n",
    "                               batch_size=60,\n",
    "                               verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "param_grid = {'dropout_rate' : [0.0, 0.2, 0.3]}\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and learning rateÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.7392739283763142 using {'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>}\n",
      "Means: 0.6600660120103226, Stdev: 0.10765351243438732 with: {'lr': 0.05, 'optimizer': <class 'keras.optimizers.Adam'>}\n",
      "Means: 0.12211220768025212, Stdev: 0.17269274023273257 with: {'lr': 0.05, 'optimizer': <class 'keras.optimizers.SGD'>}\n",
      "Means: 0.6765676542083816, Stdev: 0.07245708078828142 with: {'lr': 0.03, 'optimizer': <class 'keras.optimizers.Adam'>}\n",
      "Means: 0.21122112162042372, Stdev: 0.29871177485526024 with: {'lr': 0.03, 'optimizer': <class 'keras.optimizers.SGD'>}\n",
      "Means: 0.7392739283763142, Stdev: 0.052598599214012005 with: {'lr': 0.01, 'optimizer': <class 'keras.optimizers.Adam'>}\n",
      "Means: 0.21122112162042372, Stdev: 0.29871177485526024 with: {'lr': 0.01, 'optimizer': <class 'keras.optimizers.SGD'>}\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, \n",
    "                               epochs=30,\n",
    "                               batch_size=60,\n",
    "                               verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "param_grid = {'optimizer': [Adam, SGD],\n",
    "              'lr': [.05, .03, 0.01]}\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeakyRelu Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.6963696471928763 using {'relu_alpha': 0.001}\n",
      "Means: 0.6963696471928763, Stdev: 0.13750934506691428 with: {'relu_alpha': 0.001}\n",
      "Means: 0.6765676622736966, Stdev: 0.09014191082830694 with: {'relu_alpha': 0.003}\n",
      "Means: 0.6831683160448232, Stdev: 0.12601901723521391 with: {'relu_alpha': 0.005}\n"
     ]
    }
   ],
   "source": [
    "model = KerasClassifier(build_fn=create_model, \n",
    "                               epochs=30,\n",
    "                               batch_size=60,\n",
    "                               verbose=0)\n",
    "# define the grid search parameters\n",
    "param_grid = {'relu_alpha': [.001, .003, 0.005]}\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best results were achieved with \n",
    "optimizer Adam, \n",
    "learning rate = 0.01, \n",
    "droupout rate = 0.2, \n",
    "epoch size = 30, \n",
    "batch size=60\n",
    "LeakyRelU alpha rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "DS43SC.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
