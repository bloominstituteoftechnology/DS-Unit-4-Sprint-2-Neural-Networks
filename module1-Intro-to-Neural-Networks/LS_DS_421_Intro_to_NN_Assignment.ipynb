{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVfaLrjLvxvQ"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Neural Networks\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 1*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "## Define the Following:\n",
    "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
    "\n",
    "### Input Layer: \n",
    "Exposed input of features (x1,x2,x3,...)\n",
    "### Hidden Layer: \n",
    "Intermediary neuronal computations between inner and output layer\n",
    "### Output Layer: \n",
    "Outputs final value computed by hypothesis\n",
    "### Neuron: \n",
    "Cells in the brain that communicate through electrical impulses. Receives input and transmits output if a threshold is met. \n",
    "### Weight: \n",
    "Parameters of a model \n",
    "### Activation Function: \n",
    "Computation of input wire that leads to the output wire by specific neuron \n",
    "### Node Map: \n",
    "Visual diagram of the architecture of \"topology\" of our neural network (like a flow chart of paths)\n",
    "### Perceptron: \n",
    "A supervised learning model of binary classifiers enabled by back-propogation. May be single or multi layered. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXuy9WcWzxa4"
   },
   "source": [
    "## Inputs -> Outputs\n",
    "\n",
    "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlSwIJMC0A8F"
   },
   "source": [
    "In a perceptron, input signals arrive to the input layer of a neural network, each parameterized with a weight and an additional offset (\"bias\"). These are computed in neuron nodes of the input layer by an activation function which determinine the output signal of the input layer. The activation function is modeled by a logistic regression using the sigmoid (most common), tanh, step, or retu functions to provide an output between 0 and 1. The output of the input layer may then be followed by a series of hidden layers which may be a set of hyperparameters using similar activation functions. This proceeds until the final computation is obtained by the output layer which deteremines the final output signal as a 0 to 1 probability for classification.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6sWR43PTwhSk"
   },
   "source": [
    "## Write your own perceptron code that can correctly classify (99.0% accuracy) a NAND gate. \n",
    "\n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x1  x2  y\n",
       "0   0   0  1\n",
       "1   1   0  1\n",
       "2   0   1  1\n",
       "3   1   1  0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = { 'x1': [0,1,0,1],\n",
    "         'x2': [0,0,1,1],\n",
    "         'y':  [1,1,1,0]\n",
    "       }\n",
    "\n",
    "df = pd.DataFrame.from_dict(data).astype('int')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "      <th>ones</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x1  x2  y  ones\n",
       "0   0   0  1     1\n",
       "1   1   0  1     1\n",
       "2   0   1  1     1\n",
       "3   1   1  0     1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ones']=np.ones(4)\n",
    "df['ones']=df['ones'].astype('int')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sgh7VFGwnXGH"
   },
   "outputs": [],
   "source": [
    "inputs = df[['x1','x2','ones']]\n",
    "correct_outputs = df[['y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.90363754],\n",
       "       [ 0.36192603],\n",
       "       [ 0.59739214]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = 2 * np.random.random((3,1)) - 1\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a sigmoid function for my perceptron\n",
    "def sigmoid(x):\n",
    "    return 1/ (1 + np.exp(-x))\n",
    "sigmoid(0)\n",
    "\n",
    "#Defining sigmoid derivative function, this is a basic way to get back prop\n",
    "def sigmoid_derivative(x):\n",
    "    sx = sigmoid(x)\n",
    "    return sx * (1 - sx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.59739214]\n",
      " [-0.30624541]\n",
      " [ 0.95931816]\n",
      " [ 0.05568062]]\n",
      "[[0.64505944]\n",
      " [0.42403146]\n",
      " [0.72298527]\n",
      " [0.51391656]]\n"
     ]
    }
   ],
   "source": [
    "weighted_sum = np.dot(np.array(inputs), weights)\n",
    "activated_output = sigmoid(weighted_sum)\n",
    "print(weighted_sum)\n",
    "print(activated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights after training\n",
      "[[-11.84042561]\n",
      " [-11.84042561]\n",
      " [ 17.80950276]]\n",
      "Output after training\n",
      "[[0.99999998]\n",
      " [0.99744966]\n",
      " [0.99744966]\n",
      " [0.00281143]]\n"
     ]
    }
   ],
   "source": [
    "#Loop through 10k epochs to allow the perceptron to backprop and correct its weights\n",
    "for iteration in range(10000):\n",
    "    \n",
    "    # Weighted sum of inputs / weights\n",
    "    weighted_sum = np.dot(inputs, weights)\n",
    "    \n",
    "    # Activate!\n",
    "    activated_output = sigmoid(weighted_sum)\n",
    "    \n",
    "    # Calc error\n",
    "    error = correct_outputs - activated_output\n",
    "    \n",
    "    adjustments = error * sigmoid_derivative(activated_output)\n",
    "    \n",
    "    # Update the Weights\n",
    "    weights += np.dot(inputs.T, adjustments)\n",
    "    \n",
    "print(\"Weights after training\")\n",
    "print(weights)\n",
    "\n",
    "print(\"Output after training\")\n",
    "print(activated_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xf7sdqVs0s4x"
   },
   "source": [
    "## Implement your own Perceptron Class and use it to classify a binary dataset: \n",
    "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
    "\n",
    "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv')\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although neural networks can handle non-normalized data, scaling or normalizing your data will improve your neural network's learning speed. Try to apply the sklearn `MinMaxScaler` or `Normalizer` to your diabetes dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "\n",
    "feats = list(diabetes)[:-1]\n",
    "\n",
    "scaler = MinMaxScaler()    # See U2-LinearModels-M4\n",
    "X = diabetes[feats]\n",
    "Y = diabetes[['Outcome']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.743719</td>\n",
       "      <td>0.590164</td>\n",
       "      <td>0.353535</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500745</td>\n",
       "      <td>0.234415</td>\n",
       "      <td>0.483333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.427136</td>\n",
       "      <td>0.540984</td>\n",
       "      <td>0.292929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.396423</td>\n",
       "      <td>0.116567</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.919598</td>\n",
       "      <td>0.524590</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.347243</td>\n",
       "      <td>0.253629</td>\n",
       "      <td>0.183333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.447236</td>\n",
       "      <td>0.540984</td>\n",
       "      <td>0.232323</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.418778</td>\n",
       "      <td>0.038002</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.688442</td>\n",
       "      <td>0.327869</td>\n",
       "      <td>0.353535</td>\n",
       "      <td>0.198582</td>\n",
       "      <td>0.642325</td>\n",
       "      <td>0.943638</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies   Glucose  BloodPressure  SkinThickness   Insulin       BMI  \\\n",
       "0     0.352941  0.743719       0.590164       0.353535  0.000000  0.500745   \n",
       "1     0.058824  0.427136       0.540984       0.292929  0.000000  0.396423   \n",
       "2     0.470588  0.919598       0.524590       0.000000  0.000000  0.347243   \n",
       "3     0.058824  0.447236       0.540984       0.232323  0.111111  0.418778   \n",
       "4     0.000000  0.688442       0.327869       0.353535  0.198582  0.642325   \n",
       "\n",
       "   DiabetesPedigreeFunction       Age  \n",
       "0                  0.234415  0.483333  \n",
       "1                  0.116567  0.166667  \n",
       "2                  0.253629  0.183333  \n",
       "3                  0.038002  0.000000  \n",
       "4                  0.943638  0.200000  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalized data\n",
    "scaled_df = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scaled_df[feats]\n",
    "Y = diabetes[['Outcome']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.743719</td>\n",
       "      <td>0.590164</td>\n",
       "      <td>0.353535</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500745</td>\n",
       "      <td>0.234415</td>\n",
       "      <td>0.483333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.427136</td>\n",
       "      <td>0.540984</td>\n",
       "      <td>0.292929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.396423</td>\n",
       "      <td>0.116567</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.919598</td>\n",
       "      <td>0.524590</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.347243</td>\n",
       "      <td>0.253629</td>\n",
       "      <td>0.183333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.447236</td>\n",
       "      <td>0.540984</td>\n",
       "      <td>0.232323</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.418778</td>\n",
       "      <td>0.038002</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.688442</td>\n",
       "      <td>0.327869</td>\n",
       "      <td>0.353535</td>\n",
       "      <td>0.198582</td>\n",
       "      <td>0.642325</td>\n",
       "      <td>0.943638</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.582915</td>\n",
       "      <td>0.606557</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.381520</td>\n",
       "      <td>0.052519</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.391960</td>\n",
       "      <td>0.409836</td>\n",
       "      <td>0.323232</td>\n",
       "      <td>0.104019</td>\n",
       "      <td>0.461997</td>\n",
       "      <td>0.072588</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.577889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.526080</td>\n",
       "      <td>0.023911</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.989950</td>\n",
       "      <td>0.573770</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.641844</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.034159</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.628141</td>\n",
       "      <td>0.786885</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065756</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.552764</td>\n",
       "      <td>0.754098</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.560358</td>\n",
       "      <td>0.048249</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.844221</td>\n",
       "      <td>0.606557</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.566319</td>\n",
       "      <td>0.195986</td>\n",
       "      <td>0.216667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.698492</td>\n",
       "      <td>0.655738</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.403875</td>\n",
       "      <td>0.581981</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.949749</td>\n",
       "      <td>0.491803</td>\n",
       "      <td>0.232323</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.448584</td>\n",
       "      <td>0.136635</td>\n",
       "      <td>0.633333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.834171</td>\n",
       "      <td>0.590164</td>\n",
       "      <td>0.191919</td>\n",
       "      <td>0.206856</td>\n",
       "      <td>0.384501</td>\n",
       "      <td>0.217336</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.502513</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.447094</td>\n",
       "      <td>0.173356</td>\n",
       "      <td>0.183333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.592965</td>\n",
       "      <td>0.688525</td>\n",
       "      <td>0.474747</td>\n",
       "      <td>0.271868</td>\n",
       "      <td>0.682563</td>\n",
       "      <td>0.201964</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.537688</td>\n",
       "      <td>0.606557</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.441133</td>\n",
       "      <td>0.075149</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.517588</td>\n",
       "      <td>0.245902</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>0.098109</td>\n",
       "      <td>0.645306</td>\n",
       "      <td>0.044833</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.577889</td>\n",
       "      <td>0.573770</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.113475</td>\n",
       "      <td>0.515648</td>\n",
       "      <td>0.192570</td>\n",
       "      <td>0.183333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.633166</td>\n",
       "      <td>0.721311</td>\n",
       "      <td>0.414141</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.585693</td>\n",
       "      <td>0.267293</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.497487</td>\n",
       "      <td>0.688525</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.527571</td>\n",
       "      <td>0.132365</td>\n",
       "      <td>0.483333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.984925</td>\n",
       "      <td>0.737705</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.593145</td>\n",
       "      <td>0.159266</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.597990</td>\n",
       "      <td>0.655738</td>\n",
       "      <td>0.353535</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.432191</td>\n",
       "      <td>0.078992</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.718593</td>\n",
       "      <td>0.770492</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.172577</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.075149</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.628141</td>\n",
       "      <td>0.573770</td>\n",
       "      <td>0.262626</td>\n",
       "      <td>0.135934</td>\n",
       "      <td>0.463487</td>\n",
       "      <td>0.054227</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.738693</td>\n",
       "      <td>0.622951</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.587183</td>\n",
       "      <td>0.076430</td>\n",
       "      <td>0.366667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.487437</td>\n",
       "      <td>0.540984</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.165485</td>\n",
       "      <td>0.345753</td>\n",
       "      <td>0.174637</td>\n",
       "      <td>0.016667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.728643</td>\n",
       "      <td>0.672131</td>\n",
       "      <td>0.191919</td>\n",
       "      <td>0.130024</td>\n",
       "      <td>0.330849</td>\n",
       "      <td>0.071307</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.587940</td>\n",
       "      <td>0.754098</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.508197</td>\n",
       "      <td>0.110589</td>\n",
       "      <td>0.283333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.326633</td>\n",
       "      <td>0.590164</td>\n",
       "      <td>0.232323</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.476900</td>\n",
       "      <td>0.222886</td>\n",
       "      <td>0.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.497487</td>\n",
       "      <td>0.491803</td>\n",
       "      <td>0.171717</td>\n",
       "      <td>0.189125</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.160120</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.512563</td>\n",
       "      <td>0.606557</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.588674</td>\n",
       "      <td>0.091802</td>\n",
       "      <td>0.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.603015</td>\n",
       "      <td>0.655738</td>\n",
       "      <td>0.373737</td>\n",
       "      <td>0.177305</td>\n",
       "      <td>0.630402</td>\n",
       "      <td>0.301879</td>\n",
       "      <td>0.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.512563</td>\n",
       "      <td>0.360656</td>\n",
       "      <td>0.202020</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.459016</td>\n",
       "      <td>0.137489</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.547739</td>\n",
       "      <td>0.475410</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.137116</td>\n",
       "      <td>0.424739</td>\n",
       "      <td>0.060205</td>\n",
       "      <td>0.016667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.703518</td>\n",
       "      <td>0.770492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.487332</td>\n",
       "      <td>0.280102</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.768844</td>\n",
       "      <td>0.721311</td>\n",
       "      <td>0.373737</td>\n",
       "      <td>0.165485</td>\n",
       "      <td>0.605067</td>\n",
       "      <td>0.467976</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.502513</td>\n",
       "      <td>0.688525</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.124113</td>\n",
       "      <td>0.447094</td>\n",
       "      <td>0.175064</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.738693</td>\n",
       "      <td>0.770492</td>\n",
       "      <td>0.414141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.734724</td>\n",
       "      <td>0.119556</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.407035</td>\n",
       "      <td>0.606557</td>\n",
       "      <td>0.414141</td>\n",
       "      <td>0.067376</td>\n",
       "      <td>0.690015</td>\n",
       "      <td>0.434671</td>\n",
       "      <td>0.183333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.939698</td>\n",
       "      <td>0.573770</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.236407</td>\n",
       "      <td>0.542474</td>\n",
       "      <td>0.140905</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.814070</td>\n",
       "      <td>0.508197</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.362146</td>\n",
       "      <td>0.042699</td>\n",
       "      <td>0.483333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.683417</td>\n",
       "      <td>0.573770</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.464978</td>\n",
       "      <td>0.471392</td>\n",
       "      <td>0.016667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.608040</td>\n",
       "      <td>0.639344</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>0.087470</td>\n",
       "      <td>0.581222</td>\n",
       "      <td>0.078138</td>\n",
       "      <td>0.116667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.542714</td>\n",
       "      <td>0.508197</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.387481</td>\n",
       "      <td>0.061913</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.909548</td>\n",
       "      <td>0.721311</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.602837</td>\n",
       "      <td>0.645306</td>\n",
       "      <td>0.061486</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.773869</td>\n",
       "      <td>0.639344</td>\n",
       "      <td>0.323232</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.482861</td>\n",
       "      <td>0.155850</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.643216</td>\n",
       "      <td>0.721311</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>0.130024</td>\n",
       "      <td>0.543964</td>\n",
       "      <td>0.418019</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.688442</td>\n",
       "      <td>0.737705</td>\n",
       "      <td>0.414141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.476900</td>\n",
       "      <td>0.133646</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.618090</td>\n",
       "      <td>0.590164</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.540984</td>\n",
       "      <td>0.076857</td>\n",
       "      <td>0.516667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.532663</td>\n",
       "      <td>0.622951</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.558867</td>\n",
       "      <td>0.050811</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.954774</td>\n",
       "      <td>0.754098</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.529061</td>\n",
       "      <td>0.085397</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.442211</td>\n",
       "      <td>0.475410</td>\n",
       "      <td>0.262626</td>\n",
       "      <td>0.018913</td>\n",
       "      <td>0.423249</td>\n",
       "      <td>0.293766</td>\n",
       "      <td>0.016667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.854271</td>\n",
       "      <td>0.606557</td>\n",
       "      <td>0.313131</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.655738</td>\n",
       "      <td>0.138770</td>\n",
       "      <td>0.366667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.447236</td>\n",
       "      <td>0.508197</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.335320</td>\n",
       "      <td>0.027327</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.507538</td>\n",
       "      <td>0.622951</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.212766</td>\n",
       "      <td>0.490313</td>\n",
       "      <td>0.039710</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.613065</td>\n",
       "      <td>0.573770</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.548435</td>\n",
       "      <td>0.111870</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.608040</td>\n",
       "      <td>0.590164</td>\n",
       "      <td>0.232323</td>\n",
       "      <td>0.132388</td>\n",
       "      <td>0.390462</td>\n",
       "      <td>0.071307</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.633166</td>\n",
       "      <td>0.491803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.448584</td>\n",
       "      <td>0.115713</td>\n",
       "      <td>0.433333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>767 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pregnancies   Glucose  BloodPressure  SkinThickness   Insulin       BMI  \\\n",
       "0       0.352941  0.743719       0.590164       0.353535  0.000000  0.500745   \n",
       "1       0.058824  0.427136       0.540984       0.292929  0.000000  0.396423   \n",
       "2       0.470588  0.919598       0.524590       0.000000  0.000000  0.347243   \n",
       "3       0.058824  0.447236       0.540984       0.232323  0.111111  0.418778   \n",
       "4       0.000000  0.688442       0.327869       0.353535  0.198582  0.642325   \n",
       "5       0.294118  0.582915       0.606557       0.000000  0.000000  0.381520   \n",
       "6       0.176471  0.391960       0.409836       0.323232  0.104019  0.461997   \n",
       "7       0.588235  0.577889       0.000000       0.000000  0.000000  0.526080   \n",
       "8       0.117647  0.989950       0.573770       0.454545  0.641844  0.454545   \n",
       "9       0.470588  0.628141       0.786885       0.000000  0.000000  0.000000   \n",
       "10      0.235294  0.552764       0.754098       0.000000  0.000000  0.560358   \n",
       "11      0.588235  0.844221       0.606557       0.000000  0.000000  0.566319   \n",
       "12      0.588235  0.698492       0.655738       0.000000  0.000000  0.403875   \n",
       "13      0.058824  0.949749       0.491803       0.232323  1.000000  0.448584   \n",
       "14      0.294118  0.834171       0.590164       0.191919  0.206856  0.384501   \n",
       "15      0.411765  0.502513       0.000000       0.000000  0.000000  0.447094   \n",
       "16      0.000000  0.592965       0.688525       0.474747  0.271868  0.682563   \n",
       "17      0.411765  0.537688       0.606557       0.000000  0.000000  0.441133   \n",
       "18      0.058824  0.517588       0.245902       0.383838  0.098109  0.645306   \n",
       "19      0.058824  0.577889       0.573770       0.303030  0.113475  0.515648   \n",
       "20      0.176471  0.633166       0.721311       0.414141  0.277778  0.585693   \n",
       "21      0.470588  0.497487       0.688525       0.000000  0.000000  0.527571   \n",
       "22      0.411765  0.984925       0.737705       0.000000  0.000000  0.593145   \n",
       "23      0.529412  0.597990       0.655738       0.353535  0.000000  0.432191   \n",
       "24      0.647059  0.718593       0.770492       0.333333  0.172577  0.545455   \n",
       "25      0.588235  0.628141       0.573770       0.262626  0.135934  0.463487   \n",
       "26      0.411765  0.738693       0.622951       0.000000  0.000000  0.587183   \n",
       "27      0.058824  0.487437       0.540984       0.151515  0.165485  0.345753   \n",
       "28      0.764706  0.728643       0.672131       0.191919  0.130024  0.330849   \n",
       "29      0.294118  0.587940       0.754098       0.000000  0.000000  0.508197   \n",
       "..           ...       ...            ...            ...       ...       ...   \n",
       "737     0.470588  0.326633       0.590164       0.232323  0.000000  0.476900   \n",
       "738     0.117647  0.497487       0.491803       0.171717  0.189125  0.545455   \n",
       "739     0.058824  0.512563       0.606557       0.000000  0.000000  0.588674   \n",
       "740     0.647059  0.603015       0.655738       0.373737  0.177305  0.630402   \n",
       "741     0.176471  0.512563       0.360656       0.202020  0.111111  0.459016   \n",
       "742     0.058824  0.547739       0.475410       0.181818  0.137116  0.424739   \n",
       "743     0.529412  0.703518       0.770492       0.000000  0.000000  0.487332   \n",
       "744     0.764706  0.768844       0.721311       0.373737  0.165485  0.605067   \n",
       "745     0.705882  0.502513       0.688525       0.333333  0.124113  0.447094   \n",
       "746     0.058824  0.738693       0.770492       0.414141  0.000000  0.734724   \n",
       "747     0.058824  0.407035       0.606557       0.414141  0.067376  0.690015   \n",
       "748     0.176471  0.939698       0.573770       0.222222  0.236407  0.542474   \n",
       "749     0.352941  0.814070       0.508197       0.000000  0.000000  0.362146   \n",
       "750     0.235294  0.683417       0.573770       0.000000  0.000000  0.464978   \n",
       "751     0.058824  0.608040       0.639344       0.393939  0.087470  0.581222   \n",
       "752     0.176471  0.542714       0.508197       0.242424  0.000000  0.387481   \n",
       "753     0.000000  0.909548       0.721311       0.444444  0.602837  0.645306   \n",
       "754     0.470588  0.773869       0.639344       0.323232  0.000000  0.482861   \n",
       "755     0.058824  0.643216       0.721311       0.393939  0.130024  0.543964   \n",
       "756     0.411765  0.688442       0.737705       0.414141  0.000000  0.476900   \n",
       "757     0.000000  0.618090       0.590164       0.000000  0.000000  0.540984   \n",
       "758     0.058824  0.532663       0.622951       0.000000  0.000000  0.558867   \n",
       "759     0.352941  0.954774       0.754098       0.000000  0.000000  0.529061   \n",
       "760     0.117647  0.442211       0.475410       0.262626  0.018913  0.423249   \n",
       "761     0.529412  0.854271       0.606557       0.313131  0.000000  0.655738   \n",
       "762     0.529412  0.447236       0.508197       0.000000  0.000000  0.335320   \n",
       "763     0.588235  0.507538       0.622951       0.484848  0.212766  0.490313   \n",
       "764     0.117647  0.613065       0.573770       0.272727  0.000000  0.548435   \n",
       "765     0.294118  0.608040       0.590164       0.232323  0.132388  0.390462   \n",
       "766     0.058824  0.633166       0.491803       0.000000  0.000000  0.448584   \n",
       "\n",
       "     DiabetesPedigreeFunction       Age  \n",
       "0                    0.234415  0.483333  \n",
       "1                    0.116567  0.166667  \n",
       "2                    0.253629  0.183333  \n",
       "3                    0.038002  0.000000  \n",
       "4                    0.943638  0.200000  \n",
       "5                    0.052519  0.150000  \n",
       "6                    0.072588  0.083333  \n",
       "7                    0.023911  0.133333  \n",
       "8                    0.034159  0.533333  \n",
       "9                    0.065756  0.550000  \n",
       "10                   0.048249  0.150000  \n",
       "11                   0.195986  0.216667  \n",
       "12                   0.581981  0.600000  \n",
       "13                   0.136635  0.633333  \n",
       "14                   0.217336  0.500000  \n",
       "15                   0.173356  0.183333  \n",
       "16                   0.201964  0.166667  \n",
       "17                   0.075149  0.166667  \n",
       "18                   0.044833  0.200000  \n",
       "19                   0.192570  0.183333  \n",
       "20                   0.267293  0.100000  \n",
       "21                   0.132365  0.483333  \n",
       "22                   0.159266  0.333333  \n",
       "23                   0.078992  0.133333  \n",
       "24                   0.075149  0.500000  \n",
       "25                   0.054227  0.333333  \n",
       "26                   0.076430  0.366667  \n",
       "27                   0.174637  0.016667  \n",
       "28                   0.071307  0.600000  \n",
       "29                   0.110589  0.283333  \n",
       "..                        ...       ...  \n",
       "737                  0.222886  0.350000  \n",
       "738                  0.160120  0.000000  \n",
       "739                  0.091802  0.350000  \n",
       "740                  0.301879  0.450000  \n",
       "741                  0.137489  0.083333  \n",
       "742                  0.060205  0.016667  \n",
       "743                  0.280102  0.400000  \n",
       "744                  0.467976  0.300000  \n",
       "745                  0.175064  0.416667  \n",
       "746                  0.119556  0.100000  \n",
       "747                  0.434671  0.183333  \n",
       "748                  0.140905  0.250000  \n",
       "749                  0.042699  0.483333  \n",
       "750                  0.471392  0.016667  \n",
       "751                  0.078138  0.116667  \n",
       "752                  0.061913  0.066667  \n",
       "753                  0.061486  0.083333  \n",
       "754                  0.155850  0.400000  \n",
       "755                  0.418019  0.266667  \n",
       "756                  0.133646  0.300000  \n",
       "757                  0.076857  0.516667  \n",
       "758                  0.050811  0.083333  \n",
       "759                  0.085397  0.750000  \n",
       "760                  0.293766  0.016667  \n",
       "761                  0.138770  0.366667  \n",
       "762                  0.027327  0.200000  \n",
       "763                  0.039710  0.700000  \n",
       "764                  0.111870  0.100000  \n",
       "765                  0.071307  0.150000  \n",
       "766                  0.115713  0.433333  \n",
       "\n",
       "[767 rows x 8 columns]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6510416666666666"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get baseline score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "majority_class = diabetes['Outcome'].mode()[0]\n",
    "accuracy_score(Y,[majority_class]*len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code largely borrowed from lecture notes\n",
    "# This code should probably utilize a sigmoid activation function\n",
    "# or something but I'm too lazy and this can be added later, whatever\n",
    "\n",
    "class Perceptron(object):\n",
    "    def __init__(self, rate=0.01, iters = 10):\n",
    "        self.rate = rate \n",
    "        self.iters = iters \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        '''Fits the training data\n",
    "        X =  Training Vectors \n",
    "        X.shape : [#samples , #features ]\n",
    "        y = Target values \n",
    "        y.shape = [# samples]\n",
    "        '''\n",
    "        \n",
    "        #initalize weights to random (-1,1)\n",
    "        self.weights = 2 * np.random.random((1+X.shape[1])) - 1\n",
    "        #initialize errors of every iteration\n",
    "        self.errors = []\n",
    "               \n",
    "        \n",
    "        #loop till iteration limit is met \n",
    "        for i in range (self.iters):\n",
    "            errs = 0 \n",
    "            # for every row of x and matching y \n",
    "            for xi, target in zip(X,y):      \n",
    "                # Get error (will be 0 or 1)\n",
    "                error = target - self.predict(xi)\n",
    "                \n",
    "                # Make adjustment equal to error multiplied by learning rate.\n",
    "                adjustment = self.rate * error\n",
    "                \n",
    "                # Update weights \n",
    "                self.weights[:-1] += adjustment * xi\n",
    "                self.weights[-1] += adjustment    #bias\n",
    "                \n",
    "                # Count errors\n",
    "                errs += int(adjustment != 0.0)  \n",
    "\n",
    "            #Save error of every iteration\n",
    "            self.errors.append(errs)\n",
    "        return self\n",
    "    \n",
    "    def net_input(self, X):\n",
    "        \"\"\"Calculate net input\n",
    "        aka weighted sum\n",
    "        \"\"\"\n",
    "        return np.dot(X, self.weights[:-1]) + self.weights[-1]\n",
    "    \n",
    "    def sigmoid(x):\n",
    "        return (1 / (1 + np.exp(-x)))\n",
    "    \n",
    "    def sigmoid_derivative(x):\n",
    "        sx = sigmoid(x)\n",
    "        return sx * (1 - sx)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step\n",
    "        If probability is over 0.5, then classify as 1 \n",
    "        otherwise, classify as 0\n",
    "        \"\"\"\n",
    "        \n",
    "        return np.where(self.net_input(X) >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pn = Perceptron(0.0002,500);\n",
    "pn.fit(X.values,Y.values);\n",
    "plt.plot(range(1, len(pn.errors) + 1), pn.errors, marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Number of misclassifications')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.6783854166666666\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy =\", (X.shape[0]-pn.errors[-1])/X.shape[0])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QR4oAW1xdyu"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
    "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
    "- Try and implement your own backpropagation algorithm.\n",
    "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_431_Intro_to_NN_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
