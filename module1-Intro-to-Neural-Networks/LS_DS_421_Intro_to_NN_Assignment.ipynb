{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVfaLrjLvxvQ"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Neural Networks\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 1*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "## Define the Following:\n",
    "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
    "\n",
    "### Input Layer:\n",
    ">The input layer is the input to the neural network. It is the data being fed in. Sometimes the input layer is backfed. \n",
    "\n",
    "### Hidden Layer:\n",
    ">The hidden layer is the \"black box\" of Neural Networks. This is where the weights and responses for data to be established. \n",
    "\n",
    "### Output Layer:\n",
    ">This is the neural network parallel to the model prediction in a classic linear or logistic regression. \n",
    "\n",
    "### Neuron:\n",
    ">Neurons or \"nodes\" recieve input and pass a signal on if a certain condition or threshold is met. \n",
    "\n",
    "### Weight:\n",
    ">Weights are the preference trained into a node based on the input. Data with greater association with better predictions is given a higher weight.\n",
    "\n",
    "### Activation Function:\n",
    ">The biology inspired portion of the Neural Network. Each node in a given layer typically has the same activation function. This activation function determines whether or not a particular node is fired. In Artificial Neural Networks activation functions decide how much signal to pass onto the next layer. This is why they are sometimes referred to as transfer functions because they determine how much signal is transferred to the next layer.\n",
    "\n",
    "### Node Map:\n",
    ">Node map refers to the general architecture of a neural network. It determines what types of nodes and the order of layers occur in a given model. \n",
    "\n",
    "### Perceptron:\n",
    ">Once of the most simple neural networks, a perceptron is just a single node or neuron of a neural network with nothing else. It can take any number of inputs and spit out an output. What a neuron does is it takes each of the input values, multplies each of them by a weight, sums all of these products up, and then passes the sum through what is called an \"activation function\" the result of which is the final value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXuy9WcWzxa4"
   },
   "source": [
    "## Inputs -> Outputs\n",
    "\n",
    "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlSwIJMC0A8F"
   },
   "source": [
    "When data is input into the neural network, it enters through the input nodes. The information then enters the the hidden layer. The hidden layers apply specific weights and biases to the information and if the resulting value is meets a threshold determined by the activation function, it passes on to either another hidden layer or an output layer. The output layer provides the desired result, often a prediciton or generation of some kind. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6sWR43PTwhSk"
   },
   "source": [
    "## Write your own perceptron code that can correctly classify a NAND gate. \n",
    "\n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sgh7VFGwnXGH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Establish Inputs\n",
    "inputs = np.array([\n",
    "    [0,0,1],\n",
    "    [1,0,1],\n",
    "    [0,1,1],\n",
    "    [1,1,1]\n",
    "])\n",
    "\n",
    "# Establish Target \n",
    "target = [[1], \n",
    "          [1], \n",
    "          [1], \n",
    "          [0]]\n",
    "\n",
    "# Sigmoid functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    sx = sigmoid(x)\n",
    "    return sx* (1-sx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \n",
      " (4, 3) \n",
      "\n",
      "weights: \n",
      " [[-0.02964715]\n",
      " [-0.16796537]\n",
      " [-0.17563552]] \n",
      "\n",
      "weighted sums: \n",
      " [[-0.17563552]\n",
      " [-0.20528267]\n",
      " [-0.34360089]\n",
      " [-0.37324804]] \n",
      "\n",
      "activation function output: \n",
      " [[0.45620365]\n",
      " [0.4488588 ]\n",
      " [0.41493504]\n",
      " [0.40775642]] \n",
      "\n",
      "errors: \n",
      " [[ 0.54379635]\n",
      " [ 0.5511412 ]\n",
      " [ 0.58506496]\n",
      " [-0.40775642]] \n",
      "\n",
      "backpropagation adjustments: \n",
      " [[ 0.12911391]\n",
      " [ 0.13107181]\n",
      " [ 0.14014688]\n",
      " [-0.09781658]] \n",
      "\n",
      "backprop weights \n",
      " [[ 0.00360808]\n",
      " [-0.12563507]\n",
      " [ 0.12688051]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('input:','\\n',inputs.shape,'\\n')\n",
    "\n",
    "# Establish random weights for each input\n",
    "weights = np.random.random((3,1)) - 1\n",
    "print('weights:','\\n', weights,'\\n')\n",
    "\n",
    "# Weighted Sum of Input and Weights\n",
    "weighted_sum = np.dot(inputs, weights)\n",
    "print('weighted sums:','\\n', weighted_sum,'\\n')\n",
    "\n",
    "# Apply the sigmoid activatin function\n",
    "activated_output = sigmoid(weighted_sum)\n",
    "print('activation function output:','\\n',activated_output,'\\n')\n",
    "\n",
    "# Correct with the error\n",
    "error = target - activated_output\n",
    "print('errors:','\\n', error, '\\n')\n",
    "\n",
    "adjustments = error * sigmoid_derivative(activated_output)\n",
    "print('backpropagation adjustments:','\\n',adjustments,'\\n')\n",
    "\n",
    "weights += np.dot(inputs.T, adjustments)\n",
    "print('backprop weights','\\n',weights,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights after Training\n",
      "[[-11.84087442]\n",
      " [-11.84087442]\n",
      " [ 17.81017581]]\n",
      "Outputs after training\n",
      "[[0.99999998]\n",
      " [0.99745024]\n",
      " [0.99745024]\n",
      " [0.0028108 ]]\n"
     ]
    }
   ],
   "source": [
    "weights = np.random.random((3,1)) - 1\n",
    "\n",
    "for iteration in range(10000):\n",
    "    \n",
    "    # Weighted Sum of inputs/weights\n",
    "    \n",
    "    weighted_sum = np.dot(inputs, weights)\n",
    "    \n",
    "    # Activate!\n",
    "    activated_output = sigmoid(weighted_sum)\n",
    "    \n",
    "    # Calculate the Error\n",
    "    error = target - activated_output\n",
    "    \n",
    "    # Adjustments\n",
    "    adjustments = error * sigmoid_derivative(activated_output)\n",
    "    \n",
    "    # New weights. \n",
    "    weights += np.dot(inputs.T, adjustments)\n",
    "    \n",
    "print('Weights after Training')\n",
    "print(weights)\n",
    "\n",
    "print('Outputs after training')\n",
    "print(activated_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xf7sdqVs0s4x"
   },
   "source": [
    "## Implement your own Perceptron Class and use it to classify a binary dataset like: \n",
    "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
    "- [Titanic](https://raw.githubusercontent.com/ryanleeallred/datasets/master/titanic.csv)\n",
    "- [A two-class version of the Iris dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/Iris.csv)\n",
    "\n",
    "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-W0tiX1F1hh2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  (768, 9) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('shape: ', df.shape,'\\n')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = df.drop(columns = 'Outcome')\n",
    "\n",
    "target = df[['Outcome']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 1)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = np.random.random((inputs.shape[1],1)) - 1\n",
    "\n",
    "for iteration in range(1000):\n",
    "    \n",
    "    # Weighted Sum of inputs/weights\n",
    "    \n",
    "    weighted_sum = np.dot(inputs, weights)\n",
    "    \n",
    "    # Activate!\n",
    "    activated_output = sigmoid(weighted_sum)\n",
    "    \n",
    "    # Calculate the Error\n",
    "    error = target - activated_output\n",
    "    \n",
    "    # Adjustments\n",
    "    adjustments = error * sigmoid_derivative(activated_output)\n",
    "    \n",
    "    # New weights. \n",
    "    weights += np.dot(inputs.T, adjustments)\n",
    "\n",
    "prediction = activated_output\n",
    "\n",
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "651.0"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Outcome'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QR4oAW1xdyu"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
    "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
    "- Try and implement your own backpropagation algorithm.\n",
    "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_431_Intro_to_NN_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
