{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVfaLrjLvxvQ"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Neural Networks\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 1*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "## Define the Following:\n",
    "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
    "\n",
    "### Input Layer:\n",
    "The set of neurons whose inputs are the actual data inputs of the model\n",
    "\n",
    "### Hidden Layer:\n",
    "The set of neurons which are in contact with neurons at both ends, rather than data inputs or the model outputs.\n",
    "\n",
    "### Output Layer:\n",
    "The set of neurons whose outputs are the actual model output.\n",
    "\n",
    "### Neuron:\n",
    "The basic unit of a neural network - also known as a node. Takes some input, applies some linear transformation, \n",
    "applies an activation function, and returns an output.\n",
    "\n",
    "### Weight:\n",
    "A part of the linear transformation applied by a given node - weights multiply (or divide!) the input.\n",
    "\n",
    "### Activation Function:\n",
    "A function that decides whether or not, and how, the neuron transmits it's transformed data to the next layer.\n",
    "Generally bounded between zero and one, or one and negative one. Oftentimes values below some threshold are mapped\n",
    "to zero.\n",
    "\n",
    "### Node Map:\n",
    "A diagram that shows the way in which the nodes of a neural network are connected to one another.\n",
    "\n",
    "### Perceptron:\n",
    "The definition can vary, but generally indicates a single-layer, feed-forward neural network. No hidden layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXuy9WcWzxa4"
   },
   "source": [
    "## Inputs -> Outputs\n",
    "\n",
    "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlSwIJMC0A8F"
   },
   "source": [
    "#### Your Answer Here\n",
    "\n",
    ") Some set of feature values are given to the input layer of the neural network.\n",
    "\n",
    ") These feature values have some linear transformation applied to them - they are each *multiplied* by a weight, and have a bias *added* to them.\n",
    "\n",
    ") The results of those linear transformations are then passed to an activation function, which decides both whether to, and how much of, the transformed input should be passed on to the next layer. For example, a sigmoid function returns one for an input at infinity, and zero for an input at negative infinity.\n",
    "\n",
    ") The output of the activation functions are then passed to the next layer, and the process repeats for each.\n",
    "\n",
    ") That being said, the connectivity of each layer, and number of nodes in each layer, can differ. Layers may be fully connected, which is to say each node has a connection to all the nodes in the previous layer, or they may have some form of connectivity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6sWR43PTwhSk"
   },
   "source": [
    "## Write your own perceptron code that can correctly classify (99.0% accuracy) a NAND gate. \n",
    "\n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "      <th>bias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x1  x2  y  bias\n",
       "0   0   0  1     1\n",
       "1   1   0  1     1\n",
       "2   0   1  1     1\n",
       "3   1   1  0     1"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = { 'x1': [0,1,0,1],\n",
    "         'x2': [0,0,1,1],\n",
    "         'y':  [1,1,1,0]\n",
    "       }\n",
    "\n",
    "df = pd.DataFrame.from_dict(data).astype('int')\n",
    "# adding a bias term\n",
    "df['bias'] = 1\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1]\n",
      " [1 0 1]\n",
      " [0 1 1]\n",
      " [1 1 1]]\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "inputs = df[['x1','x2','bias']].to_numpy()\n",
    "correct_outputs = np.array([[1],[1],[1],[0]])\n",
    "\n",
    "print(inputs)\n",
    "print(correct_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sgh7VFGwnXGH"
   },
   "outputs": [],
   "source": [
    "##### Your Code Here #####\n",
    "# defining functions\n",
    "\n",
    "def sigmoid(x):\n",
    "    y = 1/(1+(np.exp(-x)))\n",
    "    return y\n",
    "\n",
    "def d_sigmoid(x):\n",
    "    sx = sigmoid(x)\n",
    "    y = sx/(1-sx)\n",
    "    return y\n",
    "\n",
    "def random_weights():\n",
    "    np.random.seed(42)\n",
    "    weights = np.random.random((3,1))\n",
    "    return weights\n",
    "\n",
    "def run_perceptron(inputs, correct_outputs, iterations):\n",
    "    \n",
    "    weights = random_weights()\n",
    "    \n",
    "    for ii in range(0,iterations):\n",
    "        # apply the weights\n",
    "        weighted_sum = np.dot(inputs, weights)\n",
    "        # apply activation\n",
    "        activated_output = sigmoid(weighted_sum)\n",
    "        # get the error\n",
    "        error = correct_outputs - activated_output\n",
    "        # find adjustments from error\n",
    "        adjustments = error * d_sigmoid(activated_output)\n",
    "        \n",
    "        \"\"\"print('input shape: ',inputs.shape)\n",
    "        print('weights shape: ',weights.shape)\n",
    "        print('adjustments shape: ',adjustments.shape)\n",
    "        \"\"\"\n",
    "        # Update Weights\n",
    "        weights += np.dot(inputs.T, adjustments)\n",
    "        \n",
    "    return {'weights': weights, 'output': activated_output}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weights': array([[-12.64397375],\n",
       "        [-12.64397375],\n",
       "        [ 19.6332523 ]]), 'output': array([[1.        ],\n",
       "        [0.99907868],\n",
       "        [0.99907868],\n",
       "        [0.00349056]])}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run it!\n",
    "results = run_perceptron(inputs, correct_outputs, 2000)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xf7sdqVs0s4x"
   },
   "source": [
    "## Implement your own Perceptron Class and use it to classify a binary dataset: \n",
    "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
    "\n",
    "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv')\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although neural networks can handle non-normalized data, scaling or normalizing your data will improve your neural network's learning speed. Try to apply the sklearn `MinMaxScaler` or `Normalizer` to your diabetes dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "\n",
    "feats = list(diabetes)[:-1]\n",
    "\n",
    "diabetesarray = diabetes[feats]\n",
    "\n",
    "scaler = Normalizer()\n",
    "X = scaler.fit_transform(diabetesarray)\n",
    "y = diabetes['Outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03355237, 0.82762513, 0.40262844, ..., 0.18789327, 0.00350622,\n",
       "        0.27960308],\n",
       "       [0.008424  , 0.71604034, 0.55598426, ..., 0.22407851, 0.00295683,\n",
       "        0.26114412],\n",
       "       [0.04039768, 0.92409698, 0.32318146, ..., 0.11765825, 0.00339341,\n",
       "        0.16159073],\n",
       "       ...,\n",
       "       [0.02691539, 0.65135243, 0.38758161, ..., 0.14103664, 0.00131885,\n",
       "        0.16149234],\n",
       "       [0.00665306, 0.83828547, 0.39918356, ..., 0.20025708, 0.00232192,\n",
       "        0.31269379],\n",
       "       [0.00791454, 0.73605211, 0.55401772, ..., 0.24060198, 0.00249308,\n",
       "        0.18203439]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-W0tiX1F1hh2"
   },
   "outputs": [],
   "source": [
    "##### Update this Class #####\n",
    "\n",
    "class Perceptron(object):\n",
    "    \n",
    "    def __init__(self, niter = 10, learning_rate = 0.01,):\n",
    "        self.niter = niter\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def __sigmoid(self, x):\n",
    "        y = 1/(1+(np.exp(-x)))\n",
    "        return y\n",
    "    \n",
    "    def __sigmoid_derivative(self, x):\n",
    "        sx = sigmoid(x)\n",
    "        y = sx/(1-sx)\n",
    "        return y\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit training data\n",
    "        X : Training vectors, X.shape : [#samples, #features]\n",
    "        y : Target values, y.shape : [#samples]\n",
    "        \"\"\"\n",
    "\n",
    "        # Randomly Initialize Weights\n",
    "        self.weights = np.zeros(X.shape[1])\n",
    "\n",
    "        for i in range(self.niter):\n",
    "            # Weighted sum of inputs / weights\n",
    "            weighted_sum = np.dot(X, self.weights)\n",
    "\n",
    "            # Activate!\n",
    "            activated_output = self.__sigmoid(weighted_sum)\n",
    "\n",
    "            # Cac error\n",
    "            error = y - activated_output\n",
    "\n",
    "            # Update the Weights\n",
    "            adjustments = error * self.__sigmoid_derivative(activated_output)\n",
    "            self.weights += np.dot(X.T, adjustments)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step\"\"\"\n",
    "        return self.__sigmoid(np.dot(X,self.weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\conno\\.virtualenvs\\DS-Unit-4-Sprint-2-Neural-Networks-mjziinPq\\lib\\site-packages\\ipykernel_launcher.py:10: RuntimeWarning: overflow encountered in exp\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "54.0"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eye = Perceptron(niter=10000)\n",
    "eye.fit(X,y)\n",
    "output = eye.predict(X)\n",
    "sum(np.round(output, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QR4oAW1xdyu"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
    "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
    "- Try and implement your own backpropagation algorithm.\n",
    "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_431_Intro_to_NN_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python (sprint 2, NN)",
   "language": "python",
   "name": "python-sprint2-nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
