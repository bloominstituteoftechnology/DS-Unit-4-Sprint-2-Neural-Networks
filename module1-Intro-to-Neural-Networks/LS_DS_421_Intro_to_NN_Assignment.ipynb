{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVfaLrjLvxvQ"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Neural Networks\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 1*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "## Define the Following:\n",
    "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
    "\n",
    "### Input Layer:take in the information\n",
    "### Hidden Layer: change/adjust the input\n",
    "### Output Layer: output the information after adjustments\n",
    "### Neuron: A neuron applies the activation function to the sum of the inputs multiplied by their weights\n",
    "### Weight: A weight adjusts the \"importance\" of a given input\n",
    "### Activation Function: An activation function takes in the input and \"decides\" how to classify the output\n",
    "### Node Map: \n",
    "### Perceptron: A single layer neural network or binary classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXuy9WcWzxa4"
   },
   "source": [
    "## Inputs -> Outputs\n",
    "\n",
    "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlSwIJMC0A8F"
   },
   "source": [
    "Simply put, the information is input, is changed/adjusted by weights, and is returned as the output. However, there is A LOT more going on than just that! The input values (information) first come in through the input layer and are then transformed through the hidden layers to give a certain output. The way the network 'decides' the output is based on activation functions, which can be imagined as functions that activate or unlock a path if certain criteria are met. Two possible hidden layers are weights and biases. The way I imagine weight is like slope. The weight would account for a sharper or flatter rise, which would cause our activation function to trigger more quickly or slowly. On the other hand, bias acts like shifting the entire curve left or right, meaning the activation function recieves the same information, just sooner or later depending on the bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6sWR43PTwhSk"
   },
   "source": [
    "## Write your own perceptron code that can correctly classify (99.0% accuracy) a NAND gate. \n",
    "\n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = { 'x1': [0,1,0,1],\n",
    "         'x2': [0,0,1,1],\n",
    "         'y':  [1,1,1,0]\n",
    "       }\n",
    "\n",
    "df = pd.DataFrame.from_dict(data).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x1</th>\n      <th>x2</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   x1  x2  y\n0   0   0  1\n1   1   0  1\n2   0   1  1\n3   1   1  0"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([0., 0., 0., 0.])"
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Your Code Here #####\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    sx = sigmoid(x)\n",
    "    return sx * (1 - sx)\n",
    "\n",
    "def classify(inputs, correct_outputs, iters=1000):\n",
    "    '''\n",
    "    Take inputs and over 'iters' iterations find correct weights\n",
    "    Then return output after training\n",
    "    '''\n",
    "    \n",
    "    inputs = np.array(inputs)\n",
    "    length = len(inputs[0])\n",
    "    weights = np.zeros((length, 1))\n",
    "    correct = [i for i in correct_outputs]\n",
    "\n",
    "    for _ in range(iters):\n",
    "        weighted_sum = np.dot(inputs, weights)\n",
    "        activated_output = sigmoid(weighted_sum)\n",
    "        error = correct - activated_output\n",
    "        adjustments = error * sigmoid_derivative(activated_output)\n",
    "        weights = weights + np.dot(inputs.T, adjustments)\n",
    "\n",
    "    return activated_output[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([9.99975037e-01, 9.99975037e-01, 9.99975037e-01, 1.57878582e-05])"
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats = list(df)[:-1]\n",
    "target = list(df)[-1]\n",
    "X = df[feats]\n",
    "y = df[target]\n",
    "classify(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "^ Within 99% ^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xf7sdqVs0s4x"
   },
   "source": [
    "## Implement your own Perceptron Class and use it to classify a binary dataset: \n",
    "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
    "\n",
    "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(768, 9)\n"
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pregnancies</th>\n      <th>Glucose</th>\n      <th>BloodPressure</th>\n      <th>SkinThickness</th>\n      <th>Insulin</th>\n      <th>BMI</th>\n      <th>DiabetesPedigreeFunction</th>\n      <th>Age</th>\n      <th>Outcome</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6</td>\n      <td>148</td>\n      <td>72</td>\n      <td>35</td>\n      <td>0</td>\n      <td>33.6</td>\n      <td>0.627</td>\n      <td>50</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>85</td>\n      <td>66</td>\n      <td>29</td>\n      <td>0</td>\n      <td>26.6</td>\n      <td>0.351</td>\n      <td>31</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8</td>\n      <td>183</td>\n      <td>64</td>\n      <td>0</td>\n      <td>0</td>\n      <td>23.3</td>\n      <td>0.672</td>\n      <td>32</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>89</td>\n      <td>66</td>\n      <td>23</td>\n      <td>94</td>\n      <td>28.1</td>\n      <td>0.167</td>\n      <td>21</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>137</td>\n      <td>40</td>\n      <td>35</td>\n      <td>168</td>\n      <td>43.1</td>\n      <td>2.288</td>\n      <td>33</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n0            6      148             72             35        0  33.6   \n1            1       85             66             29        0  26.6   \n2            8      183             64              0        0  23.3   \n3            1       89             66             23       94  28.1   \n4            0      137             40             35      168  43.1   \n\n   DiabetesPedigreeFunction  Age  Outcome  \n0                     0.627   50        1  \n1                     0.351   31        0  \n2                     0.672   32        1  \n3                     0.167   21        0  \n4                     2.288   33        1  "
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using function from above\n",
    "diabetes = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv')\n",
    "print(diabetes.shape)\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(diabetes)[:-1]\n",
    "target = 'Outcome'\n",
    "X = diabetes[features]\n",
    "y = diabetes[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pregnancies</th>\n      <th>Glucose</th>\n      <th>BloodPressure</th>\n      <th>SkinThickness</th>\n      <th>Insulin</th>\n      <th>BMI</th>\n      <th>DiabetesPedigreeFunction</th>\n      <th>Age</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6</td>\n      <td>148</td>\n      <td>72</td>\n      <td>35</td>\n      <td>0</td>\n      <td>33.6</td>\n      <td>0.627</td>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>85</td>\n      <td>66</td>\n      <td>29</td>\n      <td>0</td>\n      <td>26.6</td>\n      <td>0.351</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8</td>\n      <td>183</td>\n      <td>64</td>\n      <td>0</td>\n      <td>0</td>\n      <td>23.3</td>\n      <td>0.672</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>89</td>\n      <td>66</td>\n      <td>23</td>\n      <td>94</td>\n      <td>28.1</td>\n      <td>0.167</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>137</td>\n      <td>40</td>\n      <td>35</td>\n      <td>168</td>\n      <td>43.1</td>\n      <td>2.288</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>116</td>\n      <td>74</td>\n      <td>0</td>\n      <td>0</td>\n      <td>25.6</td>\n      <td>0.201</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>3</td>\n      <td>78</td>\n      <td>50</td>\n      <td>32</td>\n      <td>88</td>\n      <td>31.0</td>\n      <td>0.248</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>10</td>\n      <td>115</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>35.3</td>\n      <td>0.134</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2</td>\n      <td>197</td>\n      <td>70</td>\n      <td>45</td>\n      <td>543</td>\n      <td>30.5</td>\n      <td>0.158</td>\n      <td>53</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>8</td>\n      <td>125</td>\n      <td>96</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.232</td>\n      <td>54</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>4</td>\n      <td>110</td>\n      <td>92</td>\n      <td>0</td>\n      <td>0</td>\n      <td>37.6</td>\n      <td>0.191</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>10</td>\n      <td>168</td>\n      <td>74</td>\n      <td>0</td>\n      <td>0</td>\n      <td>38.0</td>\n      <td>0.537</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>10</td>\n      <td>139</td>\n      <td>80</td>\n      <td>0</td>\n      <td>0</td>\n      <td>27.1</td>\n      <td>1.441</td>\n      <td>57</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>1</td>\n      <td>189</td>\n      <td>60</td>\n      <td>23</td>\n      <td>846</td>\n      <td>30.1</td>\n      <td>0.398</td>\n      <td>59</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>5</td>\n      <td>166</td>\n      <td>72</td>\n      <td>19</td>\n      <td>175</td>\n      <td>25.8</td>\n      <td>0.587</td>\n      <td>51</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>7</td>\n      <td>100</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30.0</td>\n      <td>0.484</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0</td>\n      <td>118</td>\n      <td>84</td>\n      <td>47</td>\n      <td>230</td>\n      <td>45.8</td>\n      <td>0.551</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>7</td>\n      <td>107</td>\n      <td>74</td>\n      <td>0</td>\n      <td>0</td>\n      <td>29.6</td>\n      <td>0.254</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>1</td>\n      <td>103</td>\n      <td>30</td>\n      <td>38</td>\n      <td>83</td>\n      <td>43.3</td>\n      <td>0.183</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>1</td>\n      <td>115</td>\n      <td>70</td>\n      <td>30</td>\n      <td>96</td>\n      <td>34.6</td>\n      <td>0.529</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>3</td>\n      <td>126</td>\n      <td>88</td>\n      <td>41</td>\n      <td>235</td>\n      <td>39.3</td>\n      <td>0.704</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>8</td>\n      <td>99</td>\n      <td>84</td>\n      <td>0</td>\n      <td>0</td>\n      <td>35.4</td>\n      <td>0.388</td>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>7</td>\n      <td>196</td>\n      <td>90</td>\n      <td>0</td>\n      <td>0</td>\n      <td>39.8</td>\n      <td>0.451</td>\n      <td>41</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>9</td>\n      <td>119</td>\n      <td>80</td>\n      <td>35</td>\n      <td>0</td>\n      <td>29.0</td>\n      <td>0.263</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>11</td>\n      <td>143</td>\n      <td>94</td>\n      <td>33</td>\n      <td>146</td>\n      <td>36.6</td>\n      <td>0.254</td>\n      <td>51</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>10</td>\n      <td>125</td>\n      <td>70</td>\n      <td>26</td>\n      <td>115</td>\n      <td>31.1</td>\n      <td>0.205</td>\n      <td>41</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>7</td>\n      <td>147</td>\n      <td>76</td>\n      <td>0</td>\n      <td>0</td>\n      <td>39.4</td>\n      <td>0.257</td>\n      <td>43</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>1</td>\n      <td>97</td>\n      <td>66</td>\n      <td>15</td>\n      <td>140</td>\n      <td>23.2</td>\n      <td>0.487</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>13</td>\n      <td>145</td>\n      <td>82</td>\n      <td>19</td>\n      <td>110</td>\n      <td>22.2</td>\n      <td>0.245</td>\n      <td>57</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>5</td>\n      <td>117</td>\n      <td>92</td>\n      <td>0</td>\n      <td>0</td>\n      <td>34.1</td>\n      <td>0.337</td>\n      <td>38</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>738</th>\n      <td>2</td>\n      <td>99</td>\n      <td>60</td>\n      <td>17</td>\n      <td>160</td>\n      <td>36.6</td>\n      <td>0.453</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>739</th>\n      <td>1</td>\n      <td>102</td>\n      <td>74</td>\n      <td>0</td>\n      <td>0</td>\n      <td>39.5</td>\n      <td>0.293</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>740</th>\n      <td>11</td>\n      <td>120</td>\n      <td>80</td>\n      <td>37</td>\n      <td>150</td>\n      <td>42.3</td>\n      <td>0.785</td>\n      <td>48</td>\n    </tr>\n    <tr>\n      <th>741</th>\n      <td>3</td>\n      <td>102</td>\n      <td>44</td>\n      <td>20</td>\n      <td>94</td>\n      <td>30.8</td>\n      <td>0.400</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>742</th>\n      <td>1</td>\n      <td>109</td>\n      <td>58</td>\n      <td>18</td>\n      <td>116</td>\n      <td>28.5</td>\n      <td>0.219</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>743</th>\n      <td>9</td>\n      <td>140</td>\n      <td>94</td>\n      <td>0</td>\n      <td>0</td>\n      <td>32.7</td>\n      <td>0.734</td>\n      <td>45</td>\n    </tr>\n    <tr>\n      <th>744</th>\n      <td>13</td>\n      <td>153</td>\n      <td>88</td>\n      <td>37</td>\n      <td>140</td>\n      <td>40.6</td>\n      <td>1.174</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>745</th>\n      <td>12</td>\n      <td>100</td>\n      <td>84</td>\n      <td>33</td>\n      <td>105</td>\n      <td>30.0</td>\n      <td>0.488</td>\n      <td>46</td>\n    </tr>\n    <tr>\n      <th>746</th>\n      <td>1</td>\n      <td>147</td>\n      <td>94</td>\n      <td>41</td>\n      <td>0</td>\n      <td>49.3</td>\n      <td>0.358</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>747</th>\n      <td>1</td>\n      <td>81</td>\n      <td>74</td>\n      <td>41</td>\n      <td>57</td>\n      <td>46.3</td>\n      <td>1.096</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>748</th>\n      <td>3</td>\n      <td>187</td>\n      <td>70</td>\n      <td>22</td>\n      <td>200</td>\n      <td>36.4</td>\n      <td>0.408</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>749</th>\n      <td>6</td>\n      <td>162</td>\n      <td>62</td>\n      <td>0</td>\n      <td>0</td>\n      <td>24.3</td>\n      <td>0.178</td>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>750</th>\n      <td>4</td>\n      <td>136</td>\n      <td>70</td>\n      <td>0</td>\n      <td>0</td>\n      <td>31.2</td>\n      <td>1.182</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>751</th>\n      <td>1</td>\n      <td>121</td>\n      <td>78</td>\n      <td>39</td>\n      <td>74</td>\n      <td>39.0</td>\n      <td>0.261</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>752</th>\n      <td>3</td>\n      <td>108</td>\n      <td>62</td>\n      <td>24</td>\n      <td>0</td>\n      <td>26.0</td>\n      <td>0.223</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>753</th>\n      <td>0</td>\n      <td>181</td>\n      <td>88</td>\n      <td>44</td>\n      <td>510</td>\n      <td>43.3</td>\n      <td>0.222</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>754</th>\n      <td>8</td>\n      <td>154</td>\n      <td>78</td>\n      <td>32</td>\n      <td>0</td>\n      <td>32.4</td>\n      <td>0.443</td>\n      <td>45</td>\n    </tr>\n    <tr>\n      <th>755</th>\n      <td>1</td>\n      <td>128</td>\n      <td>88</td>\n      <td>39</td>\n      <td>110</td>\n      <td>36.5</td>\n      <td>1.057</td>\n      <td>37</td>\n    </tr>\n    <tr>\n      <th>756</th>\n      <td>7</td>\n      <td>137</td>\n      <td>90</td>\n      <td>41</td>\n      <td>0</td>\n      <td>32.0</td>\n      <td>0.391</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>757</th>\n      <td>0</td>\n      <td>123</td>\n      <td>72</td>\n      <td>0</td>\n      <td>0</td>\n      <td>36.3</td>\n      <td>0.258</td>\n      <td>52</td>\n    </tr>\n    <tr>\n      <th>758</th>\n      <td>1</td>\n      <td>106</td>\n      <td>76</td>\n      <td>0</td>\n      <td>0</td>\n      <td>37.5</td>\n      <td>0.197</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>759</th>\n      <td>6</td>\n      <td>190</td>\n      <td>92</td>\n      <td>0</td>\n      <td>0</td>\n      <td>35.5</td>\n      <td>0.278</td>\n      <td>66</td>\n    </tr>\n    <tr>\n      <th>760</th>\n      <td>2</td>\n      <td>88</td>\n      <td>58</td>\n      <td>26</td>\n      <td>16</td>\n      <td>28.4</td>\n      <td>0.766</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>761</th>\n      <td>9</td>\n      <td>170</td>\n      <td>74</td>\n      <td>31</td>\n      <td>0</td>\n      <td>44.0</td>\n      <td>0.403</td>\n      <td>43</td>\n    </tr>\n    <tr>\n      <th>762</th>\n      <td>9</td>\n      <td>89</td>\n      <td>62</td>\n      <td>0</td>\n      <td>0</td>\n      <td>22.5</td>\n      <td>0.142</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>763</th>\n      <td>10</td>\n      <td>101</td>\n      <td>76</td>\n      <td>48</td>\n      <td>180</td>\n      <td>32.9</td>\n      <td>0.171</td>\n      <td>63</td>\n    </tr>\n    <tr>\n      <th>764</th>\n      <td>2</td>\n      <td>122</td>\n      <td>70</td>\n      <td>27</td>\n      <td>0</td>\n      <td>36.8</td>\n      <td>0.340</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>765</th>\n      <td>5</td>\n      <td>121</td>\n      <td>72</td>\n      <td>23</td>\n      <td>112</td>\n      <td>26.2</td>\n      <td>0.245</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>766</th>\n      <td>1</td>\n      <td>126</td>\n      <td>60</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30.1</td>\n      <td>0.349</td>\n      <td>47</td>\n    </tr>\n    <tr>\n      <th>767</th>\n      <td>1</td>\n      <td>93</td>\n      <td>70</td>\n      <td>31</td>\n      <td>0</td>\n      <td>30.4</td>\n      <td>0.315</td>\n      <td>23</td>\n    </tr>\n  </tbody>\n</table>\n<p>768 rows Ã— 8 columns</p>\n</div>",
      "text/plain": "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n0              6      148             72             35        0  33.6   \n1              1       85             66             29        0  26.6   \n2              8      183             64              0        0  23.3   \n3              1       89             66             23       94  28.1   \n4              0      137             40             35      168  43.1   \n5              5      116             74              0        0  25.6   \n6              3       78             50             32       88  31.0   \n7             10      115              0              0        0  35.3   \n8              2      197             70             45      543  30.5   \n9              8      125             96              0        0   0.0   \n10             4      110             92              0        0  37.6   \n11            10      168             74              0        0  38.0   \n12            10      139             80              0        0  27.1   \n13             1      189             60             23      846  30.1   \n14             5      166             72             19      175  25.8   \n15             7      100              0              0        0  30.0   \n16             0      118             84             47      230  45.8   \n17             7      107             74              0        0  29.6   \n18             1      103             30             38       83  43.3   \n19             1      115             70             30       96  34.6   \n20             3      126             88             41      235  39.3   \n21             8       99             84              0        0  35.4   \n22             7      196             90              0        0  39.8   \n23             9      119             80             35        0  29.0   \n24            11      143             94             33      146  36.6   \n25            10      125             70             26      115  31.1   \n26             7      147             76              0        0  39.4   \n27             1       97             66             15      140  23.2   \n28            13      145             82             19      110  22.2   \n29             5      117             92              0        0  34.1   \n..           ...      ...            ...            ...      ...   ...   \n738            2       99             60             17      160  36.6   \n739            1      102             74              0        0  39.5   \n740           11      120             80             37      150  42.3   \n741            3      102             44             20       94  30.8   \n742            1      109             58             18      116  28.5   \n743            9      140             94              0        0  32.7   \n744           13      153             88             37      140  40.6   \n745           12      100             84             33      105  30.0   \n746            1      147             94             41        0  49.3   \n747            1       81             74             41       57  46.3   \n748            3      187             70             22      200  36.4   \n749            6      162             62              0        0  24.3   \n750            4      136             70              0        0  31.2   \n751            1      121             78             39       74  39.0   \n752            3      108             62             24        0  26.0   \n753            0      181             88             44      510  43.3   \n754            8      154             78             32        0  32.4   \n755            1      128             88             39      110  36.5   \n756            7      137             90             41        0  32.0   \n757            0      123             72              0        0  36.3   \n758            1      106             76              0        0  37.5   \n759            6      190             92              0        0  35.5   \n760            2       88             58             26       16  28.4   \n761            9      170             74             31        0  44.0   \n762            9       89             62              0        0  22.5   \n763           10      101             76             48      180  32.9   \n764            2      122             70             27        0  36.8   \n765            5      121             72             23      112  26.2   \n766            1      126             60              0        0  30.1   \n767            1       93             70             31        0  30.4   \n\n     DiabetesPedigreeFunction  Age  \n0                       0.627   50  \n1                       0.351   31  \n2                       0.672   32  \n3                       0.167   21  \n4                       2.288   33  \n5                       0.201   30  \n6                       0.248   26  \n7                       0.134   29  \n8                       0.158   53  \n9                       0.232   54  \n10                      0.191   30  \n11                      0.537   34  \n12                      1.441   57  \n13                      0.398   59  \n14                      0.587   51  \n15                      0.484   32  \n16                      0.551   31  \n17                      0.254   31  \n18                      0.183   33  \n19                      0.529   32  \n20                      0.704   27  \n21                      0.388   50  \n22                      0.451   41  \n23                      0.263   29  \n24                      0.254   51  \n25                      0.205   41  \n26                      0.257   43  \n27                      0.487   22  \n28                      0.245   57  \n29                      0.337   38  \n..                        ...  ...  \n738                     0.453   21  \n739                     0.293   42  \n740                     0.785   48  \n741                     0.400   26  \n742                     0.219   22  \n743                     0.734   45  \n744                     1.174   39  \n745                     0.488   46  \n746                     0.358   27  \n747                     1.096   32  \n748                     0.408   36  \n749                     0.178   50  \n750                     1.182   22  \n751                     0.261   28  \n752                     0.223   25  \n753                     0.222   26  \n754                     0.443   45  \n755                     1.057   37  \n756                     0.391   39  \n757                     0.258   52  \n758                     0.197   26  \n759                     0.278   66  \n760                     0.766   22  \n761                     0.403   43  \n762                     0.142   33  \n763                     0.171   63  \n764                     0.340   27  \n765                     0.245   30  \n766                     0.349   47  \n767                     0.315   23  \n\n[768 rows x 8 columns]"
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n       1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0.,\n       0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0.,\n       0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0.,\n       0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n       0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0.,\n       0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0.,\n       0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n       0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n       1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n       1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0.,\n       0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,\n       1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1.,\n       1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n       1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0.,\n       0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0.,\n       0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0.,\n       1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n       1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n       1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0.,\n       0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n       1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0.,\n       1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1.,\n       1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n       0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n       1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n       1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n       1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0.,\n       0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n       1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n       0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0.,\n       1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1.,\n       1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n       1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1.,\n       1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n       0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1.,\n       0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n       0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n       1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0.,\n       1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0.,\n       0., 1., 0.])"
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.seterr(over='ignore')\n",
    "y_pred = classify(X, y, 10)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1.0"
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although neural networks can handle non-normalized data, scaling or normalizing your data will improve your neural network's learning speed. Try to apply the sklearn `MinMaxScaler` or `Normalizer` to your diabetes dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "\n",
    "feats = list(diabetes)[:-1]\n",
    "target = list(diabetes)[-1]\n",
    "X = np.array(diabetes[feats])\n",
    "y = np.array(diabetes[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[  6.   , 148.   ,  72.   , ...,  33.6  ,   0.627,  50.   ],\n       [  1.   ,  85.   ,  66.   , ...,  26.6  ,   0.351,  31.   ],\n       [  8.   , 183.   ,  64.   , ...,  23.3  ,   0.672,  32.   ],\n       ...,\n       [  5.   , 121.   ,  72.   , ...,  26.2  ,   0.245,  30.   ],\n       [  1.   , 126.   ,  60.   , ...,  30.1  ,   0.349,  47.   ],\n       [  1.   ,  93.   ,  70.   , ...,  30.4  ,   0.315,  23.   ]])"
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,\n       1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n       1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n       1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n       1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,\n       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n       1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n       0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,\n       1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n       1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n       1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n       1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n       0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n       1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n       0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n       0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n       0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n       0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n       1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n       1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n       1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n       0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n       1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n       0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n       0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,\n       0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n       0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n       1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0],\n      dtype=int64)"
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Update this Class #####\n",
    "\n",
    "class Perceptron(object):\n",
    "    \n",
    "    def __init__(self, rate = 0.01, niter = 10):\n",
    "        self.rate = rate\n",
    "        self.niter = niter\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        sx = sigmoid(x)\n",
    "        return sx * (1 - sx)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit training data\n",
    "        X : Training vectors, X.shape : [#samples, #features]\n",
    "        y : Target values, y.shape : [#samples]\n",
    "        \"\"\"\n",
    "\n",
    "        # weights\n",
    "        self.weight = np.zeros(1 + X.shape[1])\n",
    "\n",
    "        # Number of misclassifications\n",
    "        self.errors = []  # Number of misclassifications\n",
    "\n",
    "        for i in range(self.niter):\n",
    "            err = 0\n",
    "            for xi, target in zip(X, y):\n",
    "                delta_w = self.rate * (target - self.predict(xi))\n",
    "                self.weight[1:] += delta_w * xi\n",
    "                self.weight[0] += delta_w\n",
    "                err += int(delta_w != 0.0)\n",
    "            self.errors.append(err)\n",
    "        return self\n",
    "\n",
    "    def net_input(self, X):\n",
    "        \"\"\"Calculate net input\"\"\"\n",
    "        return np.dot(X, self.weight[1:]) + self.weight[0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step\"\"\"\n",
    "        return np.where(self.net_input(X) <= .5, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn = Perceptron(niter = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<__main__.Perceptron at 0x1bf769552e8>"
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pn.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,\n       1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,\n       1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n       0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n       1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n       1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n       1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n       0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n       1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n       1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n       0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n       1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,\n       1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,\n       0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n       0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n       1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,\n       0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n       0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n       0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n       1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n       0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n       1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n       0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n       1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,\n       0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n       0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n       1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,\n       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,\n       0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n       1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0])"
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = pn.predict(X)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.69921875"
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QR4oAW1xdyu"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
    "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
    "- Try and implement your own backpropagation algorithm.\n",
    "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_431_Intro_to_NN_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}