{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVfaLrjLvxvQ"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Neural Networks\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 1*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "## Define the Following:\n",
    "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
    "\n",
    "### Input Layer:\n",
    "* An input layer is the information that you feed in to a system.  Whatever the data is.  This is what the neural network will use to determine outputs.\n",
    "### Hidden Layer:\n",
    "* A hidden layer in a neural network is a layer that is doing some work on the inputs, but cannot be seen or desribed by looking at it.  They are the steps between the input and output of a network.  It uses weights and biases to perform work on the data. The outputs are also determined by a given activation function of the network.\n",
    "### Output Layer:\n",
    "* The output layer are the predictions or classification of the data after it has gone through the network. As stated previously, it is helped to be determined by the activation function used in the neural network.\n",
    "### Neuron:\n",
    "* A Neuron (or node) in a neural network is the what the basic make-up of the network is. Each input, bias, hidden node, and output is a Neuron in the network.  Things get passed on, again, determined by the activation function through each layer, to determine the final Neurons (outputs).\n",
    "### Weight:\n",
    "* A weight is something that is attached to an input of the network.  They are set at the beginnning and adjusted throughout the training period of the network.  Weights essentially determine how influential an input is in determining the ouput.\n",
    "### Activation Function:\n",
    "* Have been talknig a lot about activation functions.  Their main function .. tee hee .. is to deal with non-linear data.  It can shrink information down to lower dimensions to then get a number that the network can use to determine the output.  In a classification case, the network uses the number produced by the activation function to determine what class the input is.\n",
    "### Node Map:\n",
    "![Neural Network Zoo](http://www.asimovinstitute.org/wp-content/uploads/2016/09/neuralnetworks.png)\n",
    "### Perceptron:\n",
    "* A Perceptron is one of the more basic neural networks.  It is typically used to classify binary outputs. It consists of 1 working neuron, that takes information from data (inputs) and tosses it out so that the activation function can determine it's value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXuy9WcWzxa4"
   },
   "source": [
    "## Inputs -> Outputs\n",
    "\n",
    "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlSwIJMC0A8F"
   },
   "source": [
    "#### A neural network begins with the inputs. Each input has a corresponding weight that is given to it initially.  The weights can be arbitrary at first.  The inputs are then fed into the neurons that perform work on the data.  The goal for the layers is to adjust the weights in order to minimize the error of predictions from real data. The next step is for the processed data to go through the activation fucntion. Which is a function that is able to deal with multi-dimensional data and allow the network to determine the class it will be in the output.  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6sWR43PTwhSk"
   },
   "source": [
    "## Write your own perceptron code that can correctly classify a NAND gate. \n",
    "\n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sgh7VFGwnXGH"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "inputs = np.array([\n",
    "    [0, 0, 1],\n",
    "    [1, 0, 1],\n",
    "    [0, 1, 1],\n",
    "    [1, 1, 1]\n",
    "])\n",
    "\n",
    "ground_truth = [[1], [1], [1], [0]]\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    sx = sigmoid(x)\n",
    "    return sx * (1-sx)\n",
    "\n",
    "weights = 2 * np.random.random((3,1)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.39708544],\n",
       "       [-0.98368861],\n",
       "       [ 0.40078636]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights after training:  [[-11.84095422]\n",
      " [-11.84095422]\n",
      " [ 17.81029548]]\n",
      "Output after training:  [[0.99999998]\n",
      " [0.99745034]\n",
      " [0.99745034]\n",
      " [0.00281068]]\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(10000):\n",
    "    weighted_sum = np.dot(inputs, weights)\n",
    "    \n",
    "    activated_outputs = sigmoid(weighted_sum)\n",
    "    \n",
    "    error = ground_truth - activated_outputs\n",
    "    \n",
    "    adjustments = error * sigmoid_derivative(activated_outputs)\n",
    "    \n",
    "    weights += np.dot(inputs.T, adjustments)\n",
    "    \n",
    "print(\"Weights after training: \", weights)\n",
    "print(\"Output after training: \", activated_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xf7sdqVs0s4x"
   },
   "source": [
    "## Implement your own Perceptron Class and use it to classify a binary dataset like: \n",
    "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
    "- [Titanic](https://raw.githubusercontent.com/ryanleeallred/datasets/master/titanic.csv)\n",
    "- [A two-class version of the Iris dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/Iris.csv)\n",
    "\n",
    "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-W0tiX1F1hh2"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from random import randrange\n",
    "\n",
    "diabetes_df = pd.read_csv(\"https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = diabetes_df.drop(columns=[\"Outcome\"]).values\n",
    "y = diabetes_df[\"Outcome\"].values\n",
    "\n",
    "X_train, X_test, y_train, t_test = train_test_split(X, y, test_size=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with weights\n",
    "\n",
    "def predict(row, weights, bias):\n",
    "    activation = bias\n",
    "    for i in range(len(row)):\n",
    "        activation += weights[i] * row[i]\n",
    "    return 1.0 if activation >= 0.0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate Perceptron weights \n",
    "def train_weights(X_train, y_train, l_rate, n_epoch):\n",
    "    \n",
    "    #initialize weights and bias w/ value = 0\n",
    "    weights = [0.0 for i in range(len(X_train[0]))]\n",
    "    bias = 0.0\n",
    "    \n",
    "    #initialize min_error as inifinite\n",
    "    min_error = float(\"inf\")\n",
    "    \n",
    "    #iterate n_epoch times:\n",
    "    for epoch in range(n_epoch):\n",
    "        \n",
    "        #initialize sum squared error for this iteration as 0\n",
    "        sum_error = 0.0\n",
    "        \n",
    "        #iterate thru each row in X_train\n",
    "        for i in range(len(X_train)):\n",
    "            row = X_train[i]\n",
    "            \n",
    "            #call the predict function to get the predicted outcome for each row\n",
    "            prediction = predict(row, weights, bias)\n",
    "            \n",
    "            #calc the error\n",
    "            error = y_train[i] - prediction\n",
    "            \n",
    "            #update the sum squared error for this iteration w/ current weights and bias\n",
    "            sum_error += error**2\n",
    "            \n",
    "            #update the bias\n",
    "            bias += l_rate * error\n",
    "            \n",
    "            #iterate thru each feature and update the corresponding weight:\n",
    "            for j in range(len(row)):\n",
    "                weights[j] += l_rate * error * row[j]\n",
    "                \n",
    "        #if the sum squared error is less than prev minimum, store the weights, bias, and min_error in        \n",
    "        if sum_error < min_error:\n",
    "            min_error = sum_error\n",
    "            best = [weights, bias]  #, min_error]\n",
    "    return best                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate algorithm\n",
    "best = train_weights(X_train, y_train, .01, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10.130000000039551,\n",
       "  4.380000000002776,\n",
       "  -1.3899999999927362,\n",
       "  0.18000000000005395,\n",
       "  0.8199999999987206,\n",
       "  3.0539999999976604,\n",
       "  159.45577000004067,\n",
       "  -0.3399999999997484],\n",
       " -493.7899999996959]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab the weights and bias from the best variable\n",
    "weights = best[0]\n",
    "bias = best[1]\n",
    "\n",
    "#generate predictions on the withheld test data using those weights and bias, row by row:\n",
    "y_pred = []\n",
    "for i in range(len(X_test)):\n",
    "    row = X_test[i]\n",
    "    y_pred.append(predict(row, weights, bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.40625\n",
      "Precision: 0.31901840490797545\n",
      "Recall: 0.9454545454545454\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QR4oAW1xdyu"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
    "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
    "- Try and implement your own backpropagation algorithm.\n",
    "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_431_Intro_to_NN_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
