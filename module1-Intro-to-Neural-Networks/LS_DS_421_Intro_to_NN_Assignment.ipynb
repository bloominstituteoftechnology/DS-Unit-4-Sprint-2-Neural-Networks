{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVfaLrjLvxvQ"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Neural Networks\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 1*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "## Define the Following:\n",
    "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
    "\n",
    "### Input Layer: This is what takes in the data that is being fed into the model. \n",
    "### Hidden Layer: This is where the neural network takes the input and processes it into the output given weights and biases.\n",
    "### Output Layer: This is what comes out of the model, either a classification or a prediction value or set of values. \n",
    "### Neuron: This takes the inputs, multiplies them by the weights, and does the sum. This is basically what does the work for the hidden layers. \n",
    "### Weight: This is a measure of how important a feature is. The heavier the weight, the more important it is. \n",
    "### Activation Function: This defines the output of the neuron given a particular input. This is what is used to do the calculations. \n",
    "### Node Map: A diagram illustrating the different layers of a neural network. \n",
    "### Perceptron: A single-layer neural network. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXuy9WcWzxa4"
   },
   "source": [
    "## Inputs -> Outputs\n",
    "\n",
    "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlSwIJMC0A8F"
   },
   "source": [
    "The flow of a neural network stars with both the input and the output when training. The network sees both the input data and the output classification or value and creates a network based on layers which interpret the data using the following: Weights, which give more or less importance to the features, biases, which are values that are created to account for values in the data that don't adhere exactly to a linear model (think of a sigmoid graph), and activation functions, which are the method used to construct the model, which can be linear or non-linear. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6sWR43PTwhSk"
   },
   "source": [
    "## Write your own perceptron code that can correctly classify (99.0% accuracy) a NAND gate. \n",
    "\n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = { 'x1': [0,1,0,1],\n",
    "         'x2': [0,0,1,1],\n",
    "         'y':  [1,1,1,0]\n",
    "       }\n",
    "\n",
    "df = pd.DataFrame.from_dict(data).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(4, 3)\n"
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x1</th>\n      <th>x2</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   x1  x2  y\n0   0   0  1\n1   1   0  1\n2   0   1  1\n3   1   1  0"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return 1/(1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([1, 1, 1, 0])"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_output = np.array(df['y'])\n",
    "correct_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "weights:\n [[0.37454012 0.95071431 0.73199394 0.59865848]\n [0.15601864 0.15599452 0.05808361 0.86617615]]\n\n(4, 2) (2, 4) \n\nweighted_sum:\n [[0.         0.         0.         0.        ]\n [0.37454012 0.95071431 0.73199394 0.59865848]\n [0.15601864 0.15599452 0.05808361 0.86617615]\n [0.53055876 1.10670883 0.79007755 1.46483463]]\n\nactivated_outputs:\n [[0.5        0.5        0.5        0.5       ]\n [0.59255557 0.72125881 0.67524268 0.64534933]\n [0.53892573 0.53891974 0.51451682 0.70394941]\n [0.62961342 0.75151503 0.68784798 0.81227101]]\n\nerror:\n [[ 0.5         0.5         0.5        -0.5       ]\n [ 0.40744443  0.27874119  0.32475732 -0.64534933]\n [ 0.46107427  0.46108026  0.48548318 -0.70394941]\n [ 0.37038658  0.24848497  0.31215202 -0.81227101]]\n\nadjustments:\n [[ 1.32436064  1.32436064  1.32436064 -1.32436064]\n [ 1.14435026  0.85211767  0.96274622 -1.87580654]\n [ 1.25143136  1.25144289  1.29761402 -2.12713912]\n [ 1.06555954  0.77532524  0.93315649 -2.64233285]]\n\nweights:\n [[ 2.58444992  2.57815722  2.62789665 -3.9194809 ]\n [ 2.47300954  2.18276266  2.28885412 -3.90329582]]\n\n"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "weights = np.random.random((2,4))\n",
    "print(f'weights:\\n {weights}\\n')\n",
    "print(df.iloc[::, [0,1]].shape, weights.shape, '\\n')\n",
    "\n",
    "inputs = df.iloc[::, [0,1]]\n",
    "weighted_sum = np.dot(inputs, weights)\n",
    "print(f'weighted_sum:\\n {weighted_sum}\\n')\n",
    "\n",
    "activated_outputs = sigmoid(weighted_sum)\n",
    "print(f'activated_outputs:\\n {activated_outputs}\\n')\n",
    "\n",
    "error = correct_output - activated_outputs\n",
    "print(f'error:\\n {error}\\n')\n",
    "\n",
    "adjustments = error * sigmoid_derivative(activated_outputs)\n",
    "print(f'adjustments:\\n {adjustments}\\n')\n",
    "\n",
    "weights = weights + np.dot(inputs.T, adjustments)\n",
    "print(f'weights:\\n {weights}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Weights after training\n[[ 11.21706391  11.21706801  11.21707498 -10.5981444 ]\n [ 11.21704343  11.21700392  11.21701622 -10.598124  ]]\nOutput after training\n[[5.00000000e-01 5.00000000e-01 5.00000000e-01 5.00000000e-01]\n [9.99986557e-01 9.99986557e-01 9.99986557e-01 2.49629101e-05]\n [9.99986556e-01 9.99986556e-01 9.99986556e-01 2.49634191e-05]\n [1.00000000e+00 1.00000000e+00 1.00000000e+00 6.23190699e-10]]\n"
    }
   ],
   "source": [
    "\n",
    "for iteration in range(10000):\n",
    "    \n",
    "    # Weighted sum of inputs / weights\n",
    "    weighted_sum = np.dot(inputs, weights)\n",
    "    \n",
    "    # Activate!\n",
    "    activated_output = sigmoid(weighted_sum)\n",
    "    \n",
    "    # Cac error\n",
    "    error = correct_output - activated_output\n",
    "    \n",
    "    adjustments = error * sigmoid_derivative(activated_output)\n",
    "    \n",
    "    # Update the Weights\n",
    "    weights += np.dot(inputs.T, adjustments)\n",
    "    \n",
    "print(\"Weights after training\")\n",
    "print(weights)\n",
    "\n",
    "print(\"Output after training\")\n",
    "print(activated_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xf7sdqVs0s4x"
   },
   "source": [
    "## Implement your own Perceptron Class and use it to classify a binary dataset: \n",
    "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
    "\n",
    "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pregnancies</th>\n      <th>Glucose</th>\n      <th>BloodPressure</th>\n      <th>SkinThickness</th>\n      <th>Insulin</th>\n      <th>BMI</th>\n      <th>DiabetesPedigreeFunction</th>\n      <th>Age</th>\n      <th>Outcome</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6</td>\n      <td>148</td>\n      <td>72</td>\n      <td>35</td>\n      <td>0</td>\n      <td>33.60000</td>\n      <td>0.62700</td>\n      <td>50</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>85</td>\n      <td>66</td>\n      <td>29</td>\n      <td>0</td>\n      <td>26.60000</td>\n      <td>0.35100</td>\n      <td>31</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8</td>\n      <td>183</td>\n      <td>64</td>\n      <td>0</td>\n      <td>0</td>\n      <td>23.30000</td>\n      <td>0.67200</td>\n      <td>32</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>89</td>\n      <td>66</td>\n      <td>23</td>\n      <td>94</td>\n      <td>28.10000</td>\n      <td>0.16700</td>\n      <td>21</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>137</td>\n      <td>40</td>\n      <td>35</td>\n      <td>168</td>\n      <td>43.10000</td>\n      <td>2.28800</td>\n      <td>33</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin      BMI  \\\n0            6      148             72             35        0 33.60000   \n1            1       85             66             29        0 26.60000   \n2            8      183             64              0        0 23.30000   \n3            1       89             66             23       94 28.10000   \n4            0      137             40             35      168 43.10000   \n\n   DiabetesPedigreeFunction  Age  Outcome  \n0                   0.62700   50        1  \n1                   0.35100   31        0  \n2                   0.67200   32        1  \n3                   0.16700   21        0  \n4                   2.28800   33        1  "
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv')\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although neural networks can handle non-normalized data, scaling or normalizing your data will improve your neural network's learning speed. Try to apply the sklearn `MinMaxScaler` or `Normalizer` to your diabetes dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "\n",
    "feats = list(diabetes)[:-1]\n",
    "\n",
    "X = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-4-057d68540610>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-057d68540610>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    self.niter = niter\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "##### Update this Class #####\n",
    "\n",
    "class Perceptron(object):\n",
    "    \n",
    "    def __init__(self, niter = 10):\n",
    "    self.niter = niter\n",
    "    \n",
    "    def __sigmoid(self, x):\n",
    "        return None\n",
    "    \n",
    "    def __sigmoid_derivative(self, x):\n",
    "        return None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "    \"\"\"Fit training data\n",
    "    X : Training vectors, X.shape : [#samples, #features]\n",
    "    y : Target values, y.shape : [#samples]\n",
    "    \"\"\"\n",
    "\n",
    "    # Randomly Initialize Weights\n",
    "    weights = ...\n",
    "\n",
    "    for i in range(self.niter):\n",
    "        # Weighted sum of inputs / weights\n",
    "\n",
    "        # Activate!\n",
    "\n",
    "        # Cac error\n",
    "\n",
    "        # Update the Weights\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "    \"\"\"Return class label after unit step\"\"\"\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QR4oAW1xdyu"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
    "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
    "- Try and implement your own backpropagation algorithm.\n",
    "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_431_Intro_to_NN_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python (NN)",
   "language": "python",
   "name": "python-nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}