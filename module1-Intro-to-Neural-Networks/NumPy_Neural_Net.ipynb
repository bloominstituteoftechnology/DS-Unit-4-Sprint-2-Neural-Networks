{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "NumPy-Neural-Net.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqE2t-2MN5ex",
        "colab_type": "text"
      },
      "source": [
        "<h1>Neural Network from Scratch with NumPy</h1>\n",
        "<h2>Jack Ross</h2><br>\n",
        "\n",
        "![Neural Network](https://www.cdn.geeksforgeeks.org/wp-content/uploads/neuralNet.png)\n",
        "\n",
        "<h2>So what is a neural network?</h2>\n",
        "<p>Simply put, a neural network is a mathematical function that maps a given input to a desired output.<br>\n",
        "A neural network consists of the following parts:<br>\n",
        "- Input layer, x<br>\n",
        "- Hidden layers<br>\n",
        "- Output layer yÌ‚<br>\n",
        "- Weights and biases betweeen each later, W and b<br>\n",
        "- Activation function for each hidden layer</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWgd-lhhN5e9",
        "colab_type": "text"
      },
      "source": [
        "<h2>To fully understand neural networks, lets build a few from scratch.<br><br>\n",
        "The first one we'll be building is called a perceptron.</h2>\n",
        "<p>A perceptron is able to classify linearly separable data.\n",
        "    \n",
        "Linearly separable data is the type of data which can be separated by a hyperplane in n-dimensional space.</p>\n",
        "<br><br>\n",
        "<h2>Our goal is to predict if a random person is obese or not.<br>\n",
        "\n",
        "Here is our dataset:<br></h2>\n",
        "\n",
        "![dataset](https://i.gyazo.com/e46cef207f88d157c5d4eddab4b0b077.png)\n",
        "\n",
        "<br>\n",
        "<p>In the above table, we have five columns: Person, Smoking, Obesity, Exercise, and Diabetic. Here 1 refers to true and 0 refers to false. For instance, the first person has values of 0, 1, 0 which means that the person doesn't smoke, is obese, and doesn't exercise. The person is also diabetic.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBBB8N_FN5e_",
        "colab_type": "text"
      },
      "source": [
        "<h1>Feed Forward</h1>\n",
        "<p>Since we have 3 features in our dataset (smoking, obesity, exercise) we will have 3 nodes in the first layer (known as the input layer) in our network. For each input feature, we have 1 weight.</p>\n",
        "\n",
        "<h2>Feed Forward Step 1: Calculate the dot product between input and weights</h2>\n",
        "<p>The nodes in the input layer (smoking, obesity, exercise) are connected with the output layer with 3 weight parameters. In the output layer, the values in the input nodes are multiplied with their corresponding weights and are added together. Finally, the bias term is added to the sum.</p>\n",
        "<h3>Bias</h3>\n",
        "<p>Suppose we have a person in our dataset who doesn't smoke, isn't obese, and doesn't exercise, the sum of the products of the input nodes and weights will be zero everytime. Therefore, to be able to make predictions, even if we do not have any non zero data about he person, we need a bias. In step one of Feed Forward, we perform the following calculations:</p><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woYCsQVUN5fA",
        "colab_type": "text"
      },
      "source": [
        "![steponecalculations](https://i.gyazo.com/b67dcf93fd727bb8eafceac325f5040b.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9DP4IpfN5fd",
        "colab_type": "text"
      },
      "source": [
        "<h2>Feed Forward Step 2: Pass the result from step 1 through an activation function</h2>\n",
        "<p>The result form step one could be any set of values but for out output we need values in the form of 1 and 0 to keep the same format. We need an activation function to squish the input value between 0 and 1. One activation function we'll be looking at is the sigmoid function.</p>\n",
        "\n",
        "<h3>Activation Functions: Sigmoid</h3>\n",
        "<p>The sigmoid function has a characteristic S-shaped or \"sigmoid\" curve and returns 0.5 when the input is 0. It returns a value close to 1 if the input is a large positive number. In case of negative input, the sigmoid function outputs a value close to 0. We can represent the sigmoid activation function mathematically as:</p><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNIkEGIIN5fi",
        "colab_type": "text"
      },
      "source": [
        "![sigmoidmath](https://i.gyazo.com/3957ff564628de0add993705b961ed05.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_wtb1EIN5fj",
        "colab_type": "text"
      },
      "source": [
        "<h2>Let's plot the sigmoid function:</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvAePs32N5fm",
        "colab_type": "code",
        "colab": {},
        "outputId": "e6d2bd54-12d8-42a1-85b1-ce2d1e5d1f9b"
      },
      "source": [
        "import numpy as np\n",
        "input = np.linspace(-10, 10, 100)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "plt.plot(input, sigmoid(input), c=\"r\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f93aea83748>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vfJ0ONsN5gB",
        "colab_type": "text"
      },
      "source": [
        "<h2>Feed Forward Summary</h2>\n",
        "<p>1.) Find the dot product of the input feature matrix and the weight matrix.<br>\n",
        "   2.) Pass the result from the output through an activation function. (Sigmoid)<br>\n",
        "   3.) The result of the activation function is the predicted output for the input features.</p>\n",
        "   \n",
        "![feed forward](https://dzone.com/storage/temp/7913024-general.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGMVSFatN5gC",
        "colab_type": "text"
      },
      "source": [
        "<h1>Back Propagation</h1>\n",
        "<p>Before your neural network makes any predictions, it makes a lot of random ones which are nowhere near correct. We let it make random predictions about the output in the beginning and then we compare the predicted output against the actual output. The we fine-tune our weights and the bias in such a way that our predicted output becomes closer to the actual output. \n",
        "    \n",
        "This is known as training the neural network.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_znNqAgN5gQ",
        "colab_type": "text"
      },
      "source": [
        "<h3>Back Propagation Step 1: Calculating the cost</h3>\n",
        "<p>First we must find the \"cost\" of the predictions. The cost can be calculated by finding the difference between the predicted output and the actual output. The higher the difference, the higher the cost.\n",
        "    \n",
        "<h4>Cost Function: Mean Squared Error</h4>\n",
        "<p>We will use the mean squared error function to find the cost of our predictions. It can be represented mathematically as:</p>\n",
        "\n",
        "![cost function](https://i.gyazo.com/65c4a711b0b94000a4ee5fa7f0eb59d6.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "<h3>Back Propagation Step 2: Minimizing the cost</h3>\n",
        "<p>To minimize the cost, we need to find the bias and weight values that make the cost function as small as possible. The smaller the cost, the more correct the predictions are. Since this is an \"optimization problem\" (problem of finding the best solution from all feasible solutions) where we have to find the \"function minima\" (the smallest value of the function), we can use the gradient decent algorithm. Gradient decent can be mathematically expressed as:</p>\n",
        "\n",
        "![Gradient decent](https://i.gyazo.com/36a8c9fb11f15b27e0072866efd09bde.png)\n",
        "\n",
        "<p>Here in the above equation, J is the cost function. Basically what the above equation says is: find the partial derivative of the cost function with respect to each weight and bias and subtract the result from the existing weight values to get the new weight values.\n",
        "\n",
        "The derivative of a function gives us its slope at any given point. To find if the cost increases or decreases, given the weight value, we can find the derivative of the function at that particular weight value. If the cost increases with the increase in weight, the derivative will return a positive value which will then be subtracted from the existing value.\n",
        "\n",
        "On the other hand, if the cost is decreasing with an increase in weight, a negative value will be returned, which will be added to the existing weight value since negative into negative is positive.\n",
        "\n",
        "In Equation 1, we can see there is an alpha symbol, which is multiplied by the gradient. This is called the learning rate. We need to keep executing Equation 1 until we get values for bias and weights, for which the cost function returns a value close to zero.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peeBoWa8N5gR",
        "colab_type": "text"
      },
      "source": [
        "<h1>Implementation</h1>\n",
        "<br>\n",
        "<h3>First we create our feature set and the corresponding labels.</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PvLOMMsN5gS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_set = np.array([[0,1,0],[0,0,1],[1,0,0],[1,1,0],[1,1,1]])\n",
        "labels = np.array([[1,0,0,1,1]])\n",
        "labels = labels.reshape(5,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHbo7wqDN5g6",
        "colab_type": "text"
      },
      "source": [
        "<h3>Define the hyper parameters</h3>\n",
        "<p>We'll use the random.seed function so that we can get the same random values whenever the script is executed. We initialize the weights with normally distributed random numbers. Since we have 3 input features (smoking, obesity, exercise), we will have a vector of 3 weights. We then initialize the bias value with another random number and then set the learning rate to 0.05.</p></p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXPVVDt8N5hU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(42)\n",
        "weights = np.random.rand(3,1)\n",
        "bias = np.random.rand(1)\n",
        "lr = 0.05"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pwy2vbjLN5hY",
        "colab_type": "text"
      },
      "source": [
        "<h3>Define (sigmoid) activation function</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7kPOFsyN5ha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RALtIJASN5hj",
        "colab_type": "text"
      },
      "source": [
        "<h3>Define the (sigmoid) activation functions derivative</h3>\n",
        "<p>The derivative is simply: sigmoid(x) * sigmoid(1-x)</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnwktevHN5hl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid_der(x):\n",
        "    return sigmoid(x)*(1-sigmoid(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dwAscPjN5hq",
        "colab_type": "text"
      },
      "source": [
        "<h3>Training the neural network</h3>\n",
        "<p>1.) We define the number of epochs. (An epoch is the number of times we want to train the algorithm on our data)<br>\n",
        "   2.) Store the values from the feature_set to the input variable. (This is step 1 of Feed Forward)<br>\n",
        "   3.) Pass the dot product through the sigmoid activation function. (This is step 2 of Feed Forward)<br>\n",
        "   4.) Find the error and print it. (This is step 1 of Back Propagation)<br>\n",
        "       - We need to differentiate this function with respect to each weight.<br>\n",
        "       - Do this using the chain rule of differentiation. Here:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbNSPt-oN5hr",
        "colab_type": "text"
      },
      "source": [
        "![chainrule](https://i.gyazo.com/2850b4b715d853321471ebe2135237c2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aulZutiN5hs",
        "colab_type": "text"
      },
      "source": [
        "<p>4.5) Next we have to find:</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6n8j-7hN5ht",
        "colab_type": "text"
      },
      "source": [
        "![dpred](https://i.gyazo.com/a2fd22bc8a4605edc910c4af08cee1db.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeXR_UiUN5hv",
        "colab_type": "text"
      },
      "source": [
        "<p>Here \"d_pred\" is just the sigmoid function and we have differentiated it with respect to the input dot product \"z\".<br>\n",
        "   \n",
        "   5.) Finally we need to find:</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3m1NOcdN5hw",
        "colab_type": "text"
      },
      "source": [
        "![dzdw](https://i.gyazo.com/7ae7e26c199cf88df53681c6dda5e239.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guvtk8kPN5hx",
        "colab_type": "text"
      },
      "source": [
        "<p>6.) We have the z_delta variable, which contains the product of dcost_dpred and dpred_dz. Instead of looping through each record and multiplying the input with corresponding z_delta, we take the transpose of the input feature matrix and multiply it with the z_delta. Finally, we multiply the learning rate variable lr with the derivative to increase the speed of convergence.</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCEo_o62N5hy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(20000):\n",
        "    inputs = feature_set\n",
        "\n",
        "    # feedforward step1\n",
        "    XW = np.dot(feature_set, weights) + bias\n",
        "\n",
        "    #feedforward step2\n",
        "    z = sigmoid(XW)\n",
        "\n",
        "\n",
        "    # backpropagation step 1\n",
        "    error = z - labels\n",
        "\n",
        "    #print(error.sum())\n",
        "\n",
        "    # backpropagation step 2\n",
        "    dcost_dpred = error\n",
        "    dpred_dz = sigmoid_der(z)\n",
        "\n",
        "    z_delta = dcost_dpred * dpred_dz\n",
        "\n",
        "    inputs = feature_set.T\n",
        "    weights -= lr * np.dot(inputs, z_delta)\n",
        "\n",
        "    for num in z_delta:\n",
        "        bias -= lr * num"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdP6mQixN5h5",
        "colab_type": "text"
      },
      "source": [
        "<h3>Predict the value of a single instance</h3>\n",
        "<p>Suppose we have a record of a patient that comes in who smokes, is not obese, and doesn't exercise. Let's find if he is likely to be diabetic or not. The input feature will look like this: [1,0,0].</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aknj9-emN5h8",
        "colab_type": "code",
        "colab": {},
        "outputId": "997e17ac-4191-43a3-ca4d-80e6eed75cfc"
      },
      "source": [
        "single_point = np.array([1,0,0])\n",
        "result = sigmoid(np.dot(single_point, weights) + bias)\n",
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.00707584]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUk59WPVN5iK",
        "colab_type": "text"
      },
      "source": [
        "<p>You can see that the person is likely not diabetic since the value is much closer to 0 than 1.\n",
        "\n",
        "Now let's test another person who doesn't, smoke, is obese, and doesn't exercises. The input feature vector will be [0,1,0].</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V77-pnlyN5iR",
        "colab_type": "code",
        "colab": {},
        "outputId": "3d230c17-39dc-4c2e-fb60-d8a358a67297"
      },
      "source": [
        "single_point = np.array([0,1,0])\n",
        "result = sigmoid(np.dot(single_point, weights) + bias)\n",
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.99837029]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMhN1CcoN5ie",
        "colab_type": "text"
      },
      "source": [
        "<p>You can see that the value is very close to 1, which is likely due to the person's obesity.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gnsDlReN5if",
        "colab_type": "text"
      },
      "source": [
        "<h1>Conclusion</h1>\n",
        "<p>You did it! We created a very simple neural network with one input and one output layer from scratch in Python. Such a neural network is simply called a perceptron. A perceptron is able to classify linearly separable data like what we used for our input feature vector. Come to my page soon to see more complex models from scratch!</p>"
      ]
    }
  ]
}