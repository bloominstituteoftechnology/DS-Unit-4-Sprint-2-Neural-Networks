{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVfaLrjLvxvQ"
   },
   "source": [
    "# Intro to Neural Networks Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "## Define the Following:\n",
    "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
    "\n",
    "### Input Layer: The layer receives input for the dataset. Might be referred to as the visible layer bcause it's the only part that interacts with the data and is exposed to it.\n",
    "### Hidden Layer(s): The layers that come after the input layer but before the output layer. Hidden layers cannot be accessed except through the input layer and are not interacted with\n",
    "### Output Layer: This is the final layer. The output layer provides a vector of values. Usually it's modified by an activation function to put it into a format that works for our context. \n",
    "### Neuron:  A basic unit of computation in a neural network. Called a node or unit. It receives input from some other nodes or external sources.\n",
    "### Weight: A value based on the relative importance as compared to the other inputs\n",
    "### Activation Function: The activation function introduces non-linearity into the output. Most of our lives are not linear. Therefore, an activation function takes an output value and performs a mathematical operation on it.\n",
    "### Node Map: A pictorial representation of our input, weights, hidden layers, output layers, bias, etc.\n",
    "### Perceptron: A linear classifier (binary) that helps classify the input data. All inputs are mulitplied by their weights. Add all the multiplied values and call them the weighted sum. Apply the weighted sum to the activation function. This is used to classify the data into binary parts.\n",
    "### Bias: Similar to the intercept in a linear equation. It's an additional parameter used to adjust the output along with the weighted sums from the input. Bias is a constant that helps the model find the best fit for the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXuy9WcWzxa4"
   },
   "source": [
    "## Inputs -> Outputs\n",
    "\n",
    "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlSwIJMC0A8F"
   },
   "source": [
    "The flow can happen in one of two ways. The first is when the network is being trained. The second is after being trained. Information is fed into the network by the input units. There is also a bias to help the model find the best fit for the data. Those input units are multiplied by their weights according to importance then activate the hidden units and layers in the network. Each layer of hidden units is receiving input from the layer to the left and those inputs are multiplied by the weights of the connections. The weight of each unit is gradually adjusted as the connection between any two units changes. The activation function produces the output of any node eventually producing a final output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6sWR43PTwhSk"
   },
   "source": [
    "## Write your own perceptron code that can correctly classify a NAND gate. \n",
    "\n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sgh7VFGwnXGH"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs = np.array([[1., 0., 0.],\n",
    "    [1., 1., 0.],\n",
    "    [1., 0., 1.],\n",
    "    [1., 1., 1.]])\n",
    "\n",
    "# Ideal outputs\n",
    "correct_outputs = [[1.],\n",
    "    [1.],\n",
    "    [1.],\n",
    "   [0.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.17785041],\n",
       "       [ 0.09977725],\n",
       "       [-0.06211495]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random weights\n",
    "weights = 2 * np.random.random((3,1))-1\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17785041],\n",
       "       [0.27762766],\n",
       "       [0.11573546],\n",
       "       [0.21551271]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_sum = np.dot(inputs, weights)\n",
    "weighted_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.54434577],\n",
       "       [0.56896452],\n",
       "       [0.52890161],\n",
       "       [0.55367061]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activated_output = sigmoid(weighted_sum)\n",
    "activated_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.45565423],\n",
       "       [ 0.43103548],\n",
       "       [ 0.47109839],\n",
       "       [-0.55367061]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = correct_outputs - activated_output\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.10587496],\n",
       "       [ 0.09948775],\n",
       "       [ 0.10990745],\n",
       "       [-0.12832898]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjustments = error * sigmoid_derivative(activated_output)  # Gradient Descent\n",
    "adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.36479158],\n",
       "       [ 0.07093602],\n",
       "       [-0.08053648]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights += np.dot(inputs.T, adjustments)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized weights after training: \n",
      "[[ 19.89513933]\n",
      " [-13.2310983 ]\n",
      " [-13.2310983 ]]\n",
      "Output After Training:\n",
      "[[1.        ]\n",
      " [0.99872558]\n",
      " [0.99872558]\n",
      " [0.00140403]]\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(10000):\n",
    "  \n",
    "  # Weighted sum of inputs and weights\n",
    "  weighted_sum = np.dot(inputs, weights)\n",
    "  \n",
    "  # Activate with sigmoid function\n",
    "  activated_output = sigmoid(weighted_sum)\n",
    "  \n",
    "  # Calculate Error\n",
    "  error = correct_outputs - activated_output\n",
    "  \n",
    "  # Calculate weight adjustments with sigmoid_derivative\n",
    "  adjustments = error * sigmoid_derivative(activated_output)\n",
    "  \n",
    "  # Update weights\n",
    "  weights += np.dot(inputs.T, adjustments)\n",
    "  \n",
    "print('Optimized weights after training: ')\n",
    "print(weights)\n",
    "\n",
    "print(\"Output After Training:\")\n",
    "print(activated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xf7sdqVs0s4x"
   },
   "source": [
    "## Implement your own Perceptron Class and use it to classify a binary dataset like: \n",
    "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
    "- [Titanic](https://raw.githubusercontent.com/ryanleeallred/datasets/master/titanic.csv)\n",
    "- [A two-class version of the Iris dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/Iris.csv)\n",
    "\n",
    "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-W0tiX1F1hh2"
   },
   "outputs": [],
   "source": [
    "##### Your Code Here #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QR4oAW1xdyu"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
    "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
    "- Try and implement your own backpropagation algorithm.\n",
    "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Intro_to_NN_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
