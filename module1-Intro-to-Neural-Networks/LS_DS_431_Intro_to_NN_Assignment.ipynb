{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_431_Intro_to_NN_Assignment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShreyasJothish/DS-Unit-4-Sprint-3-Neural-Networks/blob/master/module1-Intro-to-Neural-Networks/LS_DS_431_Intro_to_NN_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "dVfaLrjLvxvQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Intro to Neural Networks Assignment"
      ]
    },
    {
      "metadata": {
        "id": "wxtoY12mwmih",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define the Following:\n",
        "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
        "\n",
        "### Input Layer:\n",
        "\n",
        "* The Input Layer is the 1st neuron layer which receives input from our dataset. \n",
        "* This is also called the visible layer because it's the only part that is exposed to our data and that our data interacts with directly. \n",
        "* Typically node maps within the input layer corresponds with one input node for each of the different inputs/features/columns of our dataset that will be passed to the neural network.\n",
        "\n",
        "### Hidden Layer:\n",
        "\n",
        "* The neuron layers between Input layer and Output layer are called Hidden layers since it cannot be accessed directly.\n",
        "* Weighted sum of inputs and bias from previous layers are passed onto hidden layer for processing.\n",
        "* There can be multiple hidden layers and usually refers to Deep Learning.\n",
        "\n",
        "### Output Layer:\n",
        "\n",
        "* The final layer of Neural Network is called Output layer.\n",
        "* The purpose of the output layer is to output a vector of values that is in a format that is suitable for the type of problem that we're trying to address.\n",
        "* This is handled using activation functions based on the problem.\n",
        "1. Regression - Single output node for providing continous value.\n",
        "2. Binary Classification - Sigmoid activation function in order to squishify values down to represent a probability.\n",
        "3. Multi-class classification - Softmax function might have multiple output nodes in the output layer, one for each class that we're trying to predict.\n",
        "\n",
        "### Neuron:\n",
        "An artificial neuron is a mathematical function are elementary units in an artificial neural network. The artificial neuron receives one or more inputs and sums them to produce an output. Usually each input is separately weighted, and the sum is passed through a non-linear function known as an **activation  or transfer function**. The transfer functions can be sigmoid, rectified linear unit ReLU or step functions...\n",
        "\n",
        "### Weight:\n",
        "In computer science, synaptic weight refers to the strength or amplitude of a connection between two nodes. \n",
        "\n",
        "In a computational neural network, a vector or set of inputs ${\\displaystyle {\\textbf {x}}}$ and outputs ${\\displaystyle {\\textbf {y}}}$, or pre- and post-synaptic neurons respectively, are interconnected with synaptic weights represented by the matrix ${\\displaystyle w}$, where for a linear neuron\n",
        "\n",
        "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/ed6616f818670502533e9f6181934324a77aafc7)\n",
        "\n",
        "### Activation Function:\n",
        "\n",
        "In artificial neural networks, the activation function of a node defines the output of that node, or \"neuron,\" given an input or set of inputs. This output is then used as input for the next node and so on until a desired solution to the original problem is found.\n",
        "\n",
        "It maps the resulting values into the desired range such as between 0 to 1 or -1 to 1 etc. (depending upon the choice of activation function). For example, the use of the logistic activation function would map all inputs in the real number domain into the range of 0 to 1.\n",
        "\n",
        "### Node Map:\n",
        "\n",
        "\"Node Map\" it's a visual diagram of the architecture or \"topology\" of our neural network. It's kind of like a flow chart in that it shows the path from inputs to outputs.\n",
        "\n",
        "### Perceptron:\n",
        "\n",
        "The first and simplest kind of neural network that we could talk about is the perceptron. A perceptron is just a single node or neuron of a neural network with nothing else. It can take any number of inputs and gives out an output. \n",
        "\n",
        "### Bias:\n",
        "\n",
        "Bias is a constant which helps the model in a way that it can fit best for the given data.\n",
        "\n",
        "In other words, Bias is a constant which gives freedom to perform best.\n",
        "\n",
        "A bias value allows you to shift the activation function curve up or down.\n"
      ]
    },
    {
      "metadata": {
        "id": "NXuy9WcWzxa4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Inputs -> Outputs\n",
        "\n",
        "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"
      ]
    },
    {
      "metadata": {
        "id": "PlSwIJMC0A8F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Neuron is the basic building block of Neural Network. What a neuron does is it takes each of the input values, multplies each with corresponding weight, sums all of these products up, adds bias and then passes the sum through what is called an \"activation function\" the result of which is the final value.\n",
        "\n",
        "There can be several layers of neurons and each layer can have one or more neurons. "
      ]
    },
    {
      "metadata": {
        "id": "6sWR43PTwhSk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Write your own perceptron code that can correctly classify a NAND gate. \n",
        "\n",
        "| x1 | x2 | y |\n",
        "|----|----|---|\n",
        "| 0  | 0  | 1 |\n",
        "| 1  | 0  | 1 |\n",
        "| 0  | 1  | 1 |\n",
        "| 1  | 1  | 0 |"
      ]
    },
    {
      "metadata": {
        "id": "Sgh7VFGwnXGH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.seed(1)\n",
        "\n",
        "inputs = np.array([[0,0,1],\n",
        "                   [1,1,1],\n",
        "                   [1,0,1],\n",
        "                   [0,1,1]])\n",
        "# Expected output for NAND (!AND)\n",
        "correct_outputs = [[1],\n",
        "                  [1],\n",
        "                  [1],\n",
        "                  [0]]\n",
        "\n",
        "# Initial weights\n",
        "weights = 2 * np.random.random((3,1)) - 1\n",
        "# weights = np.random.random((3,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AcTlc8Fw1SCH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "  return sigmoid(x) * (1 - sigmoid(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uRVou2k71cdC",
        "colab_type": "code",
        "outputId": "e5bc52a8-d96c-4ea6-c146-ea4aa02b64e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "cell_type": "code",
      "source": [
        "for iteration in range(10000):\n",
        "  \n",
        "  # Weighted sum of inputs and weights\n",
        "  weighted_sum = np.dot(inputs, weights)\n",
        "  \n",
        "  # Activate with sigmoid function\n",
        "  activated_output = sigmoid(weighted_sum)\n",
        "  \n",
        "  # Calculate Error\n",
        "  error = correct_outputs - activated_output\n",
        "  \n",
        "  # Calculate weight adjustments with sigmoid_derivative\n",
        "  adjustments = error * sigmoid_derivative(activated_output)\n",
        "  \n",
        "  # Update weights\n",
        "  weights += np.dot(inputs.T, adjustments)\n",
        "  \n",
        "print('optimized weights after training: ')\n",
        "print(weights)\n",
        "\n",
        "print(\"Output After Training:\")\n",
        "print(activated_output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "optimized weights after training: \n",
            "[[ 13.31887341]\n",
            " [-12.91309088]\n",
            " [  6.48068717]]\n",
            "Output After Training:\n",
            "[[0.99846944]\n",
            " [0.99897943]\n",
            " [1.        ]\n",
            " [0.00160616]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Xf7sdqVs0s4x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Implement your own Perceptron Class and use it to classify a binary dataset like: \n",
        "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
        "- [Titanic](https://raw.githubusercontent.com/ryanleeallred/datasets/master/titanic.csv)\n",
        "- [A two-class version of the Iris dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/Iris.csv)\n",
        "\n",
        "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
      ]
    },
    {
      "metadata": {
        "id": "-W0tiX1F1hh2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "81bda476-598d-482a-9e79-926030d3a2d5"
      },
      "cell_type": "code",
      "source": [
        "##### Your Code Here #####\n",
        "# https://machinelearningmastery.com/implement-perceptron-algorithm-scratch-python/\n",
        "\n",
        "from random import seed\n",
        "from random import randrange\n",
        "from csv import reader\n",
        " \n",
        "# Load a CSV file\n",
        "def load_csv(filename):\n",
        "\tdataset = list()\n",
        "\twith open(filename, 'r') as file:\n",
        "\t\tcsv_reader = reader(file)\n",
        "\t\tfor row in csv_reader:\n",
        "\t\t\tif not row:\n",
        "\t\t\t\tcontinue\n",
        "\t\t\tdataset.append(row)\n",
        "\treturn dataset\n",
        " \n",
        "# Convert string column to float\n",
        "def str_column_to_float(dataset, column):\n",
        "\tfor row in dataset:\n",
        "\t\trow[column] = float(row[column].strip())\n",
        " \n",
        "# Convert string column to integer\n",
        "def str_column_to_int(dataset, column):\n",
        "\tclass_values = [row[column] for row in dataset]\n",
        "\tunique = set(class_values)\n",
        "\tlookup = dict()\n",
        "\tfor i, value in enumerate(unique):\n",
        "\t\tlookup[value] = i\n",
        "\tfor row in dataset:\n",
        "\t\trow[column] = lookup[row[column]]\n",
        "\treturn lookup\n",
        " \n",
        "# Split a dataset into k folds\n",
        "def cross_validation_split(dataset, n_folds):\n",
        "\tdataset_split = list()\n",
        "\tdataset_copy = list(dataset)\n",
        "\tfold_size = int(len(dataset) / n_folds)\n",
        "\tfor i in range(n_folds):\n",
        "\t\tfold = list()\n",
        "\t\twhile len(fold) < fold_size:\n",
        "\t\t\tindex = randrange(len(dataset_copy))\n",
        "\t\t\tfold.append(dataset_copy.pop(index))\n",
        "\t\tdataset_split.append(fold)\n",
        "\treturn dataset_split\n",
        " \n",
        "# Calculate accuracy percentage\n",
        "def accuracy_metric(actual, predicted):\n",
        "\tcorrect = 0\n",
        "\tfor i in range(len(actual)):\n",
        "\t\tif actual[i] == predicted[i]:\n",
        "\t\t\tcorrect += 1\n",
        "\treturn correct / float(len(actual)) * 100.0\n",
        " \n",
        "# Evaluate an algorithm using a cross validation split\n",
        "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
        "\tfolds = cross_validation_split(dataset, n_folds)\n",
        "\tscores = list()\n",
        "\tfor fold in folds:\n",
        "\t\ttrain_set = list(folds)\n",
        "\t\ttrain_set.remove(fold)\n",
        "\t\ttrain_set = sum(train_set, [])\n",
        "\t\ttest_set = list()\n",
        "\t\tfor row in fold:\n",
        "\t\t\trow_copy = list(row)\n",
        "\t\t\ttest_set.append(row_copy)\n",
        "\t\t\trow_copy[-1] = None\n",
        "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
        "\t\tactual = [row[-1] for row in fold]\n",
        "\t\taccuracy = accuracy_metric(actual, predicted)\n",
        "\t\tscores.append(accuracy)\n",
        "\treturn scores\n",
        " \n",
        "# Make a prediction with weights\n",
        "def predict(row, weights):\n",
        "\tactivation = weights[0]\n",
        "\tfor i in range(len(row)-1):\n",
        "\t\tactivation += weights[i + 1] * row[i]\n",
        "\treturn 1.0 if activation >= 0.0 else 0.0\n",
        " \n",
        "# Estimate Perceptron weights using stochastic gradient descent\n",
        "def train_weights(train, l_rate, n_epoch):\n",
        "\tweights = [0.0 for i in range(len(train[0]))]\n",
        "\tfor epoch in range(n_epoch):\n",
        "\t\tfor row in train:\n",
        "\t\t\tprediction = predict(row, weights)\n",
        "\t\t\terror = row[-1] - prediction\n",
        "\t\t\tweights[0] = weights[0] + l_rate * error\n",
        "\t\t\tfor i in range(len(row)-1):\n",
        "\t\t\t\tweights[i + 1] = weights[i + 1] + l_rate * error * row[i]\n",
        "\treturn weights\n",
        " \n",
        "# Perceptron Algorithm With Stochastic Gradient Descent\n",
        "def perceptron(train, test, l_rate, n_epoch):\n",
        "\tpredictions = list()\n",
        "\tweights = train_weights(train, l_rate, n_epoch)\n",
        "\tfor row in test:\n",
        "\t\tprediction = predict(row, weights)\n",
        "\t\tpredictions.append(prediction)\n",
        "\treturn(predictions)\n",
        " \n",
        "# Test the Perceptron algorithm on the sonar dataset\n",
        "seed(1)\n",
        "# load and prepare data\n",
        "filename = 'diabetes.csv'\n",
        "dataset = load_csv(filename)\n",
        "for i in range(len(dataset[0])-1):\n",
        "\tstr_column_to_float(dataset, i)\n",
        "# convert string class to integers\n",
        "str_column_to_int(dataset, len(dataset[0])-1)\n",
        "# evaluate algorithm\n",
        "n_folds = 5\n",
        "l_rate = 0.01\n",
        "n_epoch = 500\n",
        "scores = evaluate_algorithm(dataset, perceptron, n_folds, l_rate, n_epoch)\n",
        "print('Scores: %s' % scores)\n",
        "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Scores: [66.01307189542483, 64.70588235294117, 64.05228758169935, 64.70588235294117, 51.633986928104584]\n",
            "Mean Accuracy: 62.222%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6QR4oAW1xdyu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Stretch Goals:\n",
        "\n",
        "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
        "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
        "- Try and implement your own backpropagation algorithm.\n",
        "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
      ]
    }
  ]
}