{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVfaLrjLvxvQ"
   },
   "source": [
    "\n",
    "# <u>Intro to Neural Networks</u>\n",
    "<img src=\"https://static1.squarespace.com/static/5800c6211b631b49b4d63657/t/5a6caf4753450a17187dd1d3/1517075821859/fullyconnected_525.gif\" alt=\"gif\" title=\"gif\"/>\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*V1mNUbnpA7thNIUCHJNpuA.png\" alt=\"abstraction\" title=\"abstraction\"/>\n",
    "\n",
    "\n",
    "___\n",
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>Neuron</u>:\n",
    "### The basic unit of computation in a neural network is the neuron, often called a node or unit. It receives input from some other nodes, or from an external source and computes an output. Each input has an associated weight (w), which is assigned on the basis of its relative importance to other inputs. The node applies a function f (defined below) to the weighted sum of its inputs as shown in the Figure below:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://ujwlkarn.files.wordpress.com/2016/08/screen-shot-2016-08-09-at-3-42-21-am.png?w=568&h=303\" alt=\"Image of Neuron\" title=\"An Image of a Neuron\"/>\n",
    "\n",
    "\n",
    "\n",
    "> ### **Operations at one neuron of a neural network**:\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*H2GFATdntBwfKcR4Kuc5mw.png\" alt=\"op\" title=\"ops\"/>\n",
    "\n",
    "\n",
    "> ### **Connections:**\n",
    "> ### Information flows through the network across connections. A connection always has a weight value associated with it. Goal of the training is to update this weight value to decrease the loss(error). \n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1200/1*5zQUUvTbSTJyGQH_qdB2hA.png\" alt=\"conn\" title=\"conns\"/>\n",
    "\n",
    "<p style=\"page-break-after:always;\"></p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### <u>Weights:</u>\n",
    "> ### A weight represents the strength of the connection between units. If the weight from node 1 to node 2 has greater magnitude, it means that neuron 1 has greater influence over neuron 2. A weight brings down the importance of the input value. Weights near zero means changing this input will not change the output. Negative weights mean increasing this input will decrease the output. A weight decides how much influence the input will have on the output.\n",
    "> ### <u>Bias(Offset):</u>\n",
    "> ### A bias unit is an \"extra\" neuron added to each pre-output layer that stores the value of a constant (typically 1 or -1). Bias units aren't connected to any previous layer and in this sense don't represent a true \"activity\".\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1200/1*0NKtEk20-qnaLkwOa8DlnA.png\" alt=\"conn\" title=\"conns\"/>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "___\n",
    "\n",
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>Activation Function</u>: \n",
    "### An activation function decides, whether a neuron should be activated or not by calculating weighted sum and further adding bias with it. The purpose of the activation function is to introduce non-linearity into the output of a neuron.\n",
    "\n",
    "<img src=\"https://qph.fs.quoracdn.net/main-qimg-d131b1b1ffb1ae9d842e135f05635f1c\" alt=\"act\" title=\"acts\" height=\"899.2\" width=\"1199.2\"/>\n",
    "\n",
    "___\n",
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>Layers</u>: \n",
    "### A node layer is a row of neurons that turn on or off as the input is fed through the net. Each layer’s output is simultaneously the subsequent layer’s input, starting from an initial input layer receiving your data.\n",
    "\n",
    "\n",
    "> ### <u>Input Layer</u>: First layer - this is where information from the outside world enters the neural network. Nodes in this layer pass information to the hidden layer.\n",
    "> ### <u>Hidden Layer</u>: Nodes in the hidden layer have no direct connection to the outside world - they perform computations and transfer information from the input nodes to the output nodes.\n",
    "> ### <u>Output Layer</u>: This is the layer where information exits the neural network (ideally structured that it is interpretable by humans)\n",
    "\n",
    "\n",
    "<img src=\"https://www.jeremyjordan.me/content/images/2017/07/Screen-Shot-2017-07-26-at-1.44.58-PM.png\" alt=\"A diagram of a Neural Network\" title=\"Neural Network Diagram\" height=\"408.24\" width=\"715.05\"  />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "___\n",
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "\n",
    "\n",
    "## <u>Perceptron</u>: \n",
    "### The Perceptron is the precursor to modern neural networks. The Perceptron algorithm was invented in 1957 at the Cornell Aeronautical Laboratory by Frank Rosenblatt, funded by the United States Office of Naval Research. \n",
    "> ### In a 1958 press conference organized by the US Navy, Rosenblatt made statements about the perceptron that caused a heated controversy among the fledgling AI community; based on Rosenblatt's statements, The New York Times reported the perceptron to be \"the embryo of an electronic computer that [the Navy] expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence.\"\n",
    "\n",
    "### Although the perceptron initially seemed promising, it was quickly proved that perceptrons could not be trained to recognise many classes of patterns. This caused the field of neural network research to stagnate for many years, before it was recognised that a feedforward neural network with two or more layers (also called a multilayer perceptron) had far greater processing power than perceptrons with one layer\n",
    "\n",
    "### **Although the perceptron was invented decades ago, researchers lacked the computational power to make effective use of it. This algorithm only proved useful after rapid advances in computer hardware.**\n",
    "\n",
    "<img src=\"http://www.ryanleeallred.com/wp-content/uploads/2019/04/Screen-Shot-2019-04-01-at-2.34.58-AM.png\" alt=\"perc\" title=\"perc\" height=\"326\" width=\"572\"  />\n",
    "\n",
    "____\n",
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Performing Logic Operations with a Single Perceptron</u>: \n",
    "\n",
    "<img src=\"http://slideplayer.com/slide/5070403/16/images/1/Logic%3A+Connectives+AND+OR+NOT+P+Q+%28P+%5E+Q%29+T+F+P+Q+%28P+v+Q%29+T+F+P+~P+T+F.jpg\" alt=\"logcon\" title=\"logconns\"/>\n",
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Propositional Calculus\n",
    ">  Propositional calculus is a branch of logic. It is also called propositional logic, statement logic, sentential calculus, sentential logic, or sometimes zeroth-order logic. It deals with propositions (which can be true or false) and argument flow. \n",
    " Compound propositions are formed by connecting propositions by logical connectives.\n",
    " Logical connectives are found in natural languages. In English for example, some examples are \"and\" (conjunction), \"or\" (disjunction), \"not” (negation) and \"if\" (but only when used to denote material conditional).\n",
    "\n",
    "**The following is an example of a very simple inference within the scope of propositional logic:**\n",
    "\n",
    ">*  Premise 1: If it's raining then it's cloudy.\n",
    "*   Premise 2: It's raining.\n",
    "*  Conclusion: It's cloudy.\n",
    "\n",
    "**Both premises and the conclusion are propositions. The premises are taken for granted and then with the application of modus ponens (an inference rule) the conclusion follows.**\n",
    "\n",
    ">As propositional logic is not concerned with the structure of propositions beyond the point where they can't be decomposed any more by logical connectives, this inference can be restated replacing those atomic statements with statement letters, which are interpreted as variables representing statements:\n",
    "\n",
    ">*  Premise 1: ${\\displaystyle P\\to Q}$ \n",
    "*  Premise 2: ${\\displaystyle P} $\n",
    "*  Conclusion: ${\\displaystyle Q} $\n",
    "\n",
    "**The same can be stated succinctly in the following way:**\n",
    "\n",
    "> ${\\displaystyle P\\to Q,P\\vdash Q}$ \n",
    "\n",
    "**When P is interpreted as “It's raining” and Q as “it's cloudy” the above symbolic expressions can be seen to exactly correspond with the original expression in natural language. Not only that, but they will also correspond with any other inference of this form, which will be valid on the same basis that this inference is.**\n",
    "\n",
    ">#### Propositional logic may be studied through a formal system in which formulas of a formal language may be interpreted to represent propositions. A system of inference rules and axioms allows certain formulas to be derived. These derived formulas are called theorems and may be interpreted to be true propositions. A constructed sequence of such formulas is known as a derivation or proof and the last formula of the sequence is the theorem. The derivation may be interpreted as proof of the proposition represented by the theorem.\n",
    "\n",
    "\n",
    "___\n",
    "<p style=\"page-break-after:always;\"></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/800/1*zq3cbyx-xd_SRq8EwzER0w.jpeg\" alt=\"logic\" title=\"loggates\"/>\n",
    "\n",
    ">#### Logic gates are the basic building blocks of any digital system. It is an electronic circuit having one or more than one input and only one output. The relationship between the input and the output is based on a certain logic. Based on this, logic gates are named as AND gate, OR gate, NOT gate etc.\n",
    "\n",
    "_____\n",
    "\n",
    "## The NAND gate (NOT-AND)\n",
    ">#### The NAND or “Not AND” function is a combination of the two separate logical functions, the AND function and the NOT function in series.\n",
    "\n",
    "<img src=\"https://bjc.edc.org/bjc-r/img/6-computers/LogicGates_img/TruthTables/NAND_TruthTableTF.png\" alt=\"NAND\" title=\"NANDG\"/>\n",
    "\n",
    "**The Logic NAND Function only produces an output when “ANY” of its inputs are not present and in Boolean Algebra terms the output will be TRUE only when any of its inputs are FALSE.**\n",
    "____\n",
    "\n",
    "<div style=\"page-break-after:always;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a NAND gate with a single perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Establish training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random State for reproducibility\n",
    "\n",
    "np.random.seed(43)\n",
    "\n",
    "\n",
    "#The first two columns represent T/F, the third column is a constant bias\n",
    "inputs = np.array([[0,0,1],\n",
    "                   [0,1,1],\n",
    "                   [1,0,1],\n",
    "                   [1,1,1]])\n",
    "\n",
    "#Correct Labels\n",
    "correct_outputs = [[1],\n",
    "                  [1],\n",
    "                  [1],\n",
    "                  [0]]\n",
    "\n",
    "# Initialize weights, all in the range (-1, 1)\n",
    "weights = 2 * np.random.random((3,1)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after:always;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.stack.imgur.com/inMoa.png\" alt=\"sigmoid\" title=\"sig\"/>\n",
    "<p style=\"page-break-after:always;\"></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "  return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Testing the logic gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sgh7VFGwnXGH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized weights after training: \n",
      "[[-11.83921936]\n",
      " [-11.83921936]\n",
      " [ 17.80769383]]\n",
      "Output After Training:\n",
      "[[0.99999998]\n",
      " [0.99744813]\n",
      " [0.99744813]\n",
      " [0.00281312]]\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(10000):\n",
    "  \n",
    "  # Weighted sum of inputs and weights\n",
    "  weighted_sum = np.dot(inputs, weights)\n",
    "  \n",
    "  # Activate with sigmoid function\n",
    "  activated_output = sigmoid(weighted_sum)\n",
    "  \n",
    "  # Calculate Error\n",
    "  error = correct_outputs - activated_output\n",
    "  \n",
    "  # Calculate weight adjustments with sigmoid_derivative\n",
    "  adjustments = error * sigmoid_derivative(activated_output)\n",
    "  \n",
    "  # Update weights\n",
    "  weights += np.dot(inputs.T, adjustments)\n",
    "  \n",
    "print('optimized weights after training: ')\n",
    "print(weights)\n",
    "\n",
    "print(\"Output After Training:\")\n",
    "print(activated_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model converged on the correct values: \n",
    "\n",
    "\n",
    "| Output | Actual |\n",
    "|----|----|\n",
    "| 0.99  | 1 |\n",
    "| 0.99  | 1  |\n",
    "| 0.99  | 1  |\n",
    "| 0.00  | 0  |\n",
    "\n",
    "\n",
    "___\n",
    "<p style=\"page-break-before:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xf7sdqVs0s4x"
   },
   "source": [
    "\n",
    "# Classification of Diabetic Patients with a Perceptron\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context\n",
    "\n",
    "#### **This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.**\n",
    "\n",
    ">#### The dataset consists of several medical predictor variables and one target variable, **Outcome**. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>116</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.201</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>88</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.248</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.3</td>\n",
       "      <td>0.134</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>197</td>\n",
       "      <td>70</td>\n",
       "      <td>45</td>\n",
       "      <td>543</td>\n",
       "      <td>30.5</td>\n",
       "      <td>0.158</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>125</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "5            5      116             74              0        0  25.6   \n",
       "6            3       78             50             32       88  31.0   \n",
       "7           10      115              0              0        0  35.3   \n",
       "8            2      197             70             45      543  30.5   \n",
       "9            8      125             96              0        0   0.0   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  \n",
       "5                     0.201   30        0  \n",
       "6                     0.248   26        1  \n",
       "7                     0.134   29        0  \n",
       "8                     0.158   53        1  \n",
       "9                     0.232   54        1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 9)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are 768 patient records\n",
    "<p style=\"page-break-after:always;\"></p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Creating a Perceptron Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Perceptron(object):\n",
    "\n",
    "    def __init__(self, no_of_inputs, epochs=10000, learning_rate=0.01):\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = np.zeros(no_of_inputs + 1)\n",
    "           \n",
    "    def predict(self, inputs):\n",
    "        summation = np.dot(inputs, self.weights[1:]) + self.weights[0]\n",
    "        if summation > 0:\n",
    "          activation = 1\n",
    "        else:\n",
    "          activation = 0            \n",
    "        return activation\n",
    "\n",
    "    def train(self, training_inputs, labels):\n",
    "        for _ in range(self.epochs):\n",
    "            for inputs, label in zip(training_inputs, labels):\n",
    "                prediction = self.predict(inputs)\n",
    "                self.weights[1:] += self.learning_rate * (label - prediction) * inputs\n",
    "                self.weights[0] += self.learning_rate * (label - prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a test for the model's predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df.iloc[:,0:8].values\n",
    "y = df.iloc[:,8:9].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Perceptron(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.train(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for i in range(len(X_test)):\n",
    "    preds.append(classifier.predict(X_test[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The network classified 77% of patients correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7716535433070866\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(preds, y_test)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_431_Intro_to_NN_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
