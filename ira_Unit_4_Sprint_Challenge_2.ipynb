{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "nteract": {
      "version": "0.15.0"
    },
    "colab": {
      "name": "ira_Unit_4_Sprint_Challenge_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7k-TvWgnBJJc",
        "colab_type": "text"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "## *Data Science Unit 4 Sprint 2*\n",
        "\n",
        "# Sprint Challenge - Neural Network Foundations\n",
        "\n",
        "Table of Problems\n",
        "\n",
        "1. [Defining Neural Networks](#Q1)\n",
        "2. [Chocolate Gummy Bears](#Q2)\n",
        "    - Perceptron\n",
        "    - Multilayer Perceptron\n",
        "4. [Keras MMP](#Q3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsgCflzSBJJl",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"Q1\"></a>\n",
        "## 1. Define the following terms:\n",
        "\n",
        "- **Neuron:** They make up the neural network.  These nodes are the simple, but highly interconnected elements which are organized in layers which process information using dynamic state responses to external inputs. (artificial neural networks are modeled after how our own biological brains & networks)\n",
        "\n",
        "- **Input Layer:** These present the patterns to the network.  Which would communicate to one or more hidden layers.  These nodes are passive (they don't change the data); receiving a single value on their input and duplicate the value to their many outputs.  \n",
        "\n",
        "   Starting from the input layer, it duplicates each value and sends it to all the hidden nodes.\n",
        "\n",
        "- **Hidden Layer:** layers of mathematical functions each designed to produce an output specific to an intended result.  They allow for the function of a neural network to be broken down into specific transformations of the data.  \n",
        "\n",
        "   They are found between the input and output of the algorithm.  The function applies weights to the inputs and directs them through an activation function as the input.\n",
        "\n",
        "- **Output Layer:** This is where hidden layers link to. Output layers receive connections from hidden layers. It returns an output value that corresponds to the prediction of the response variable.  \n",
        "\n",
        "   In classification problems, only one output mode. In multiclass classification, could be more.  The activate nodes of the output layer combine and change the data to produce output values.\n",
        "\n",
        "- **Activation:** Each node has one, and the activation function defines the output of that node given an input (or set of inputs).\n",
        "\n",
        "   There are a number of activation functions available. The ones we used for this week's sprint were sigmoid and RelU.  \n",
        "\n",
        "- **Backpropagation:** an algorithm for supervised learning of artificial neural networks using gradient descent.  \n",
        "\n",
        "   The method calculates the gradient of the error function with respect to the neural network's weights. \n",
        "\n",
        "   The calculation of the gradient proceeds \"backwards\" through the network.  With the gradient of the final layer of weights being calculated first and the gradient of the first layer of weights being calculated last.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "532_ApnuHuD5",
        "colab_type": "text"
      },
      "source": [
        "# Referenced links:\n",
        "(for when I refer to this sprint in the future)\n",
        "*   https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6\n",
        "*   https://en.wikipedia.org/wiki/Activation_function\n",
        "*   https://deepai.org/machine-learning-glossary-and-terms/hidden-layer-machine-learning\n",
        "*   https://brilliant.org/wiki/backpropagation/\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2XzaC1lBJJn",
        "colab_type": "text"
      },
      "source": [
        "## 2. Chocolate Gummy Bears <a id=\"Q2\"></a>\n",
        "\n",
        "Right now, you're probably thinking, \"yuck, who the hell would eat that?\". Great question. Your candy company wants to know too. And you thought I was kidding about the [Chocolate Gummy Bears](https://nuts.com/chocolatessweets/gummies/gummy-bears/milk-gummy-bears.html?utm_source=google&utm_medium=cpc&adpos=1o1&gclid=Cj0KCQjwrfvsBRD7ARIsAKuDvMOZrysDku3jGuWaDqf9TrV3x5JLXt1eqnVhN0KM6fMcbA1nod3h8AwaAvWwEALw_wcB). \n",
        "\n",
        "Let's assume that a candy company has gone out and collected information on the types of Halloween candy kids ate. Our candy company wants to predict the eating behavior of witches, warlocks, and ghosts -- aka costumed kids. They shared a sample dataset with us. Each row represents a piece of candy that a costumed child was presented with during \"trick\" or \"treat\". We know if the candy was `chocolate` (or not chocolate) or `gummy` (or not gummy). Your goal is to predict if the costumed kid `ate` the piece of candy. \n",
        "\n",
        "If both chocolate and gummy equal one, you've got a chocolate gummy bear on your hands!?!?!\n",
        "![Chocolate Gummy Bear](https://ed910ae2d60f0d25bcb8-80550f96b5feb12604f4f720bfefb46d.ssl.cf1.rackcdn.com/3fb630c04435b7b5-2leZuM7_-zoom.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "outputHidden": false,
        "id": "LYNLk64fBJJo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "candy = pd.read_csv('chocolate_gummy_bears.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "outputHidden": false,
        "id": "kX6nf84yBJJy",
        "colab_type": "code",
        "outputId": "08d565a3-0954-426f-dc5d-a32eb4209b79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "candy.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>chocolate</th>\n",
              "      <th>gummy</th>\n",
              "      <th>ate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   chocolate  gummy  ate\n",
              "0          0      1    1\n",
              "1          1      0    1\n",
              "2          0      1    1\n",
              "3          0      0    0\n",
              "4          1      1    0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nZKnxbuCAN9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The necessities\n",
        "import numpy as np\n",
        "\n",
        "# Sklearn packages\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWhXFXSeBJJ9",
        "colab_type": "text"
      },
      "source": [
        "### Perceptron\n",
        "\n",
        "To make predictions on the `candy` dataframe. Build and train a Perceptron using numpy. Your target column is `ate` and your features: `chocolate` and `gummy`. Do not do any feature engineering. :P\n",
        "\n",
        "Once you've trained your model, report your accuracy. You will not be able to achieve more than ~50% with the simple perceptron. Explain why you could not achieve a higher accuracy with the *simple perceptron* architecture, because it's possible to achieve ~95% accuracy on this dataset. Provide your answer in markdown (and *optional* data anlysis code) after your perceptron implementation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqe-qEWvCR3a",
        "colab_type": "code",
        "outputId": "22a99433-5e8a-4594-f590-9dba0c2cbb46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "print(candy.shape)\n",
        "candy.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>chocolate</th>\n",
              "      <th>gummy</th>\n",
              "      <th>ate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   chocolate  gummy  ate\n",
              "0          0      1    1\n",
              "1          1      0    1\n",
              "2          0      1    1\n",
              "3          0      0    0\n",
              "4          1      1    0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "outputHidden": false,
        "id": "xiSSLXi5BJJ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Start your candy perceptron here\n",
        "\n",
        "X = candy[['chocolate', 'gummy']].values\n",
        "y = candy['ate'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmkOjjOzCk6S",
        "colab_type": "code",
        "outputId": "f2e693cf-93cf-4e12-eb3d-8b254fa06753",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X.shape, y.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((10000, 2), (10000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScgIzRkrCr27",
        "colab_type": "code",
        "outputId": "cd4fc0f7-fdff-4fd1-ef29-98a08665428e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=85)\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((8000, 2), (2000, 2), (8000,), (2000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63SnCogVCr5x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Perceptron(object):\n",
        "  def __init__(self, rate=0.1, niter=1000):\n",
        "    self.rate = rate\n",
        "    self.niter = niter\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    \"\"\" fit training data\n",
        "    X : training vectors, X.shape : [#samples, #features]\n",
        "    y : target values, y.shape : [#samples]\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize weights\n",
        "    self.weight = np.zeros(1 + X.shape[1])\n",
        "\n",
        "    # Number of misclassifications\n",
        "    self.errors = [] # number of misclassifications\n",
        "\n",
        "    for i in range(self.niter):\n",
        "      err = 0\n",
        "      for xi, target in zip(X, y):\n",
        "        delta_w = self.rate * (target - self.predict(xi))\n",
        "        self.weight[1:] += delta_w * xi\n",
        "        self.weight[0] += delta_w\n",
        "        err += int(delta_w != 0.0)\n",
        "      self.errors.append(err)\n",
        "    return self\n",
        "\n",
        "  def net_input(self, X):\n",
        "    return np.dot(X, self.weight[1:]) + self.weight[0]\n",
        "\n",
        "  def predict(self, X):\n",
        "    \"\"\"Return class label after unit step\"\"\"\n",
        "    return np.where(self.net_input(X) >= 0.0, 1, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeY18gedCr8e",
        "colab_type": "code",
        "outputId": "5e71d393-eb2c-4b5d-f2c8-abc75059fcec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "pn = Perceptron()\n",
        "pn.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Perceptron at 0x7f84f49788d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEP5aJdqDvWy",
        "colab_type": "code",
        "outputId": "53b6f9a3-2fad-4b47-bf2a-c9640cce0c64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "predictions = [pn.predict(X_test[i]) for i in range(len(X_test))]\n",
        "print(\"accuracy:\", accuracy_score(y_test, predictions))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.507\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySeqxVs_DvTp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The accuracy provided is right in line with \n",
        "# where the instructions for this part said they would be, ~50%."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhjoR0iDBJKD",
        "colab_type": "text"
      },
      "source": [
        "### Multilayer Perceptron <a id=\"Q3\"></a>\n",
        "\n",
        "Using the sample candy dataset, implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. Your Multilayer Perceptron should be implemented in Numpy. \n",
        "Your network must have one hidden layer.\n",
        "\n",
        "Once you've trained your model, report your accuracy. Explain why your MLP's performance is considerably better than your simple perceptron's on the candy dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "outputHidden": false,
        "id": "ln0mEsQ0BJKE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuralNetwork:\n",
        "  def __init__(self):\n",
        "    # Set up the architecture of the neural network\n",
        "    self.inputs = 2\n",
        "    self.hiddenNodes = 3\n",
        "    self.outputNodes = 1\n",
        "\n",
        "    # initial weights: 2 x 3 matrix array for the 1st layer \n",
        "    self.weights1 = np.random.rand(self.inputs, self.hiddenNodes)\n",
        "\n",
        "    # 3 x 1 matrix array for the hidden to output pathway\n",
        "    self.weights2 = np.random.rand(self.hiddenNodes, self.outputNodes)\n",
        "\n",
        "  def sigmoid(self, s):\n",
        "    return 1 / (1+np.exp(-s))\n",
        "\n",
        "  def sigmoidPrime(self, s):\n",
        "    return s * (1 - s)\n",
        "\n",
        "  def feed_forward(self, X):\n",
        "    \"\"\"\n",
        "    Calculate the NN inference using feed forward. aka \"predict\"\n",
        "    \"\"\"\n",
        "\n",
        "    # weighted sum of inputs => hidden layer\n",
        "    self.hidden_sum = np.dot(X, self.weights1)\n",
        "\n",
        "    # activations of weighted sum\n",
        "    self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
        "\n",
        "    # weighted sum between hidden and output\n",
        "    self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
        "\n",
        "    # final activation of output\n",
        "    self.activated_output = self.sigmoid(self.output_sum)\n",
        "\n",
        "    return self.activated_output\n",
        "\n",
        "  def backward(self, X, y, o):\n",
        "    \"\"\"\n",
        "    Backward propagate through the network\n",
        "    \"\"\"\n",
        "\n",
        "    # error in output\n",
        "    self.o_error = y - o\n",
        "\n",
        "    # apply derivative of sigmoid to error\n",
        "    # how far off are we in relation to the sigmoid f(x) of the output\n",
        "    # ^- aka hidden => output\n",
        "    self.o_delta = self.o_error * self.sigmoidPrime(o)\n",
        "\n",
        "    # z2 error\n",
        "    self.z2_error = self.o_delta.dot(self.weights2.T)\n",
        "\n",
        "    # how much of that \"far off\" can be explained by the input => hidden\n",
        "    self.z2_delta = self.z2_error * self.sigmoidPrime(self.activated_hidden)\n",
        "\n",
        "    # adjustment to first set of weights (input => hdiden)\n",
        "    self.weights1 += X.T.dot(self.z2_delta)\n",
        "\n",
        "    # adjustment to second set of weights (hidden => output)\n",
        "    self.weights2 += self.activated_hidden.T.dot(self.o_delta)\n",
        "\n",
        "  def train(self, X, y):\n",
        "    o = self.feed_forward(X)\n",
        "    self.backward(X, y, o)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HquB-1EWBJKI",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "P.S. Don't try candy gummy bears. They're disgusting. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0_D3PbbOPdQ",
        "colab_type": "text"
      },
      "source": [
        "### the three cells below reference Lecture & Assignment from module 2\n",
        "(reference for self)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLlpTDNcLhzQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QT1WLkWeLhvu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reshaping y_train and y_test\n",
        "y_train = y_train.reshape(-1, 1)\n",
        "y_test = y_test.reshape(-1, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYIcYOH0LqQ8",
        "colab_type": "code",
        "outputId": "235246b5-11f2-4eed-eab2-0b5c835c3dee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "nn = NeuralNetwork()\n",
        "\n",
        "cost = []\n",
        "for i in range(1000):\n",
        "    cost.append(np.mean(np.square(y - nn.feed_forward(X))))\n",
        "    nn.train(X_train, y_train)\n",
        "\n",
        "\n",
        "print(\"Predicted Output: \\n\" + str(nn.feed_forward(X))) \n",
        "print(\"Loss: \\n\" + str(np.mean(np.square(y - nn.feed_forward(X)))))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted Output: \n",
            "[[2.49337724e-94]\n",
            " [2.49333457e-94]\n",
            " [2.49337724e-94]\n",
            " ...\n",
            " [2.49337724e-94]\n",
            " [2.49337724e-94]\n",
            " [2.49333457e-94]]\n",
            "Loss: \n",
            "0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyHiKf5zOaFl",
        "colab_type": "text"
      },
      "source": [
        "# Explanation: \n",
        "*(answer to the question)*\n",
        "  - The added layers, and the ability to \"backpropagate\" account for why the score is so dramatically improved. \n",
        "\n",
        " A perceptron is limited to a feed-forward process, so it can't go back to implement backpropagation (i.e. making adjustments on the error and the weights).  \n",
        "\n",
        " The multilayer has better performance because it can \"learn\" from this error, which allows us to get these high accuracy scores in our predictions.  \n",
        "\n",
        "*reference*: https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBpH8_I-BJKK",
        "colab_type": "text"
      },
      "source": [
        "## 3. Keras MMP <a id=\"Q3\"></a>\n",
        "\n",
        "Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy.\n",
        "Use the Heart Disease Dataset (binary classification)\n",
        "Use an appropriate loss function for a binary classification task\n",
        "Use an appropriate activation function on the final layer of your network.\n",
        "Train your model using verbose output for ease of grading.\n",
        "Use GridSearchCV or RandomSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
        "When hyperparameter tuning, show you work by adding code cells for each new experiment.\n",
        "Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
        "You must hyperparameter tune at least 3 parameters in order to get a 3 on this section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "outputHidden": false,
        "id": "64jJO-N_BJKL",
        "colab_type": "code",
        "outputId": "bddf0aa1-1a7d-40c8-b2f6-1191b080bb84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        }
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
        "df = df.sample(frac=1)\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(303, 14)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>51</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>125</td>\n",
              "      <td>213</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>125</td>\n",
              "      <td>1</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>293</th>\n",
              "      <td>67</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>152</td>\n",
              "      <td>212</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>43</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>130</td>\n",
              "      <td>315</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>162</td>\n",
              "      <td>0</td>\n",
              "      <td>1.9</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>139</th>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>128</td>\n",
              "      <td>263</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>105</td>\n",
              "      <td>1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>44</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>108</td>\n",
              "      <td>141</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>175</td>\n",
              "      <td>0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     age  sex  cp  trestbps  chol  fbs  ...  exang  oldpeak  slope  ca  thal  target\n",
              "34    51    1   3       125   213    0  ...      1      1.4      2   1     2       1\n",
              "293   67    1   2       152   212    0  ...      0      0.8      1   0     3       0\n",
              "98    43    1   2       130   315    0  ...      0      1.9      2   1     2       1\n",
              "139   64    1   0       128   263    0  ...      1      0.2      1   1     3       1\n",
              "53    44    0   2       108   141    0  ...      0      0.6      1   0     2       1\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_xXLhPASB85",
        "colab_type": "text"
      },
      "source": [
        "### Step 1: importing packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iig9nsh7RCm2",
        "colab_type": "code",
        "outputId": "d7802e68-45e9-47dc-c596-8eb008002ca3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "from sklearn.preprocessing import MinMaxScaler, Normalizer, OrdinalEncoder\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDvaHBB8SJTn",
        "colab_type": "text"
      },
      "source": [
        "### Step 2: Train, test, split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tz7KT2Y4RCqa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler = MinMaxScaler()\n",
        "df_transform = scaler.fit_transform(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jsLLabKRiSb",
        "colab_type": "code",
        "outputId": "a4319bb5-e369-42a4-ed03-3ac627c88d34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Split the values into X and y components:\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_transform[:, :-1], df_transform[:, -1], \n",
        "test_size=0.20, random_state=85)\n",
        "\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(242, 13) (61, 13) (242,) (61,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNOKUVGDSNh_",
        "colab_type": "text"
      },
      "source": [
        "### Step 3: Arriving at our baseline model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoiKv0hyRiWV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fixing random seed for reproducibility\n",
        "seed = 85\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Function to create model, required for KerasClassifier\n",
        "def create_model():\n",
        "  # create model\n",
        "  model = Sequential()\n",
        "  model.add(Dense(13, input_dim=13, activation='relu'))\n",
        "  #model.add(Dropout(0.2))  # <-- added these later to see effect on score\n",
        "  model.add(Dense(12, activation='sigmoid'))\n",
        "  #model.add(Dropout(0.2))  # <-- added these later to see effect on score, omitted from baseline.\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  # compile model\n",
        "  adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "  print(model.summary())\n",
        "  return model\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YB3KV5X7RC2t",
        "colab_type": "code",
        "outputId": "04d7eba0-4703-4ba3-fbe5-abb41a79b675",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(X_train, y_train,\n",
        "          validation_data=(X_test,y_test),\n",
        "          epochs=50,\n",
        "          batch_size=20,\n",
        "          verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_54 (Dense)             (None, 13)                182       \n",
            "_________________________________________________________________\n",
            "dense_55 (Dense)             (None, 12)                168       \n",
            "_________________________________________________________________\n",
            "dense_56 (Dense)             (None, 1)                 13        \n",
            "=================================================================\n",
            "Total params: 363\n",
            "Trainable params: 363\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 242 samples, validate on 61 samples\n",
            "Epoch 1/50\n",
            "242/242 [==============================] - 1s 2ms/sample - loss: 0.6806 - acc: 0.5702 - val_loss: 0.6902 - val_acc: 0.4426\n",
            "Epoch 2/50\n",
            "242/242 [==============================] - 0s 195us/sample - loss: 0.6567 - acc: 0.5702 - val_loss: 0.6499 - val_acc: 0.4426\n",
            "Epoch 3/50\n",
            "242/242 [==============================] - 0s 154us/sample - loss: 0.6073 - acc: 0.7231 - val_loss: 0.5515 - val_acc: 0.8689\n",
            "Epoch 4/50\n",
            "242/242 [==============================] - 0s 154us/sample - loss: 0.5366 - acc: 0.7893 - val_loss: 0.4789 - val_acc: 0.8361\n",
            "Epoch 5/50\n",
            "242/242 [==============================] - 0s 131us/sample - loss: 0.4764 - acc: 0.7975 - val_loss: 0.3836 - val_acc: 0.9016\n",
            "Epoch 6/50\n",
            "242/242 [==============================] - 0s 135us/sample - loss: 0.4340 - acc: 0.8099 - val_loss: 0.3517 - val_acc: 0.9180\n",
            "Epoch 7/50\n",
            "242/242 [==============================] - 0s 138us/sample - loss: 0.4139 - acc: 0.8017 - val_loss: 0.3225 - val_acc: 0.8852\n",
            "Epoch 8/50\n",
            "242/242 [==============================] - 0s 150us/sample - loss: 0.3992 - acc: 0.8182 - val_loss: 0.3231 - val_acc: 0.8852\n",
            "Epoch 9/50\n",
            "242/242 [==============================] - 0s 124us/sample - loss: 0.3960 - acc: 0.8099 - val_loss: 0.3142 - val_acc: 0.8852\n",
            "Epoch 10/50\n",
            "242/242 [==============================] - 0s 135us/sample - loss: 0.3865 - acc: 0.8264 - val_loss: 0.3211 - val_acc: 0.8852\n",
            "Epoch 11/50\n",
            "242/242 [==============================] - 0s 156us/sample - loss: 0.4025 - acc: 0.8140 - val_loss: 0.3059 - val_acc: 0.8852\n",
            "Epoch 12/50\n",
            "242/242 [==============================] - 0s 142us/sample - loss: 0.4589 - acc: 0.7851 - val_loss: 0.3334 - val_acc: 0.8852\n",
            "Epoch 13/50\n",
            "242/242 [==============================] - 0s 146us/sample - loss: 0.4224 - acc: 0.8223 - val_loss: 0.2916 - val_acc: 0.9180\n",
            "Epoch 14/50\n",
            "242/242 [==============================] - 0s 141us/sample - loss: 0.4006 - acc: 0.8058 - val_loss: 0.3477 - val_acc: 0.8689\n",
            "Epoch 15/50\n",
            "242/242 [==============================] - 0s 158us/sample - loss: 0.3877 - acc: 0.8388 - val_loss: 0.2910 - val_acc: 0.9180\n",
            "Epoch 16/50\n",
            "242/242 [==============================] - 0s 140us/sample - loss: 0.3939 - acc: 0.8264 - val_loss: 0.3095 - val_acc: 0.8852\n",
            "Epoch 17/50\n",
            "242/242 [==============================] - 0s 123us/sample - loss: 0.3660 - acc: 0.8306 - val_loss: 0.2864 - val_acc: 0.9016\n",
            "Epoch 18/50\n",
            "242/242 [==============================] - 0s 138us/sample - loss: 0.3639 - acc: 0.8554 - val_loss: 0.2937 - val_acc: 0.9016\n",
            "Epoch 19/50\n",
            "242/242 [==============================] - 0s 127us/sample - loss: 0.3652 - acc: 0.8554 - val_loss: 0.2771 - val_acc: 0.9180\n",
            "Epoch 20/50\n",
            "242/242 [==============================] - 0s 152us/sample - loss: 0.3574 - acc: 0.8595 - val_loss: 0.2776 - val_acc: 0.9180\n",
            "Epoch 21/50\n",
            "242/242 [==============================] - 0s 132us/sample - loss: 0.3562 - acc: 0.8595 - val_loss: 0.2859 - val_acc: 0.9016\n",
            "Epoch 22/50\n",
            "242/242 [==============================] - 0s 124us/sample - loss: 0.3535 - acc: 0.8430 - val_loss: 0.2748 - val_acc: 0.9344\n",
            "Epoch 23/50\n",
            "242/242 [==============================] - 0s 128us/sample - loss: 0.3603 - acc: 0.8430 - val_loss: 0.2866 - val_acc: 0.9016\n",
            "Epoch 24/50\n",
            "242/242 [==============================] - 0s 142us/sample - loss: 0.3518 - acc: 0.8388 - val_loss: 0.2780 - val_acc: 0.9180\n",
            "Epoch 25/50\n",
            "242/242 [==============================] - 0s 126us/sample - loss: 0.3393 - acc: 0.8512 - val_loss: 0.2860 - val_acc: 0.9016\n",
            "Epoch 26/50\n",
            "242/242 [==============================] - 0s 130us/sample - loss: 0.3417 - acc: 0.8430 - val_loss: 0.2679 - val_acc: 0.9344\n",
            "Epoch 27/50\n",
            "242/242 [==============================] - 0s 126us/sample - loss: 0.3366 - acc: 0.8554 - val_loss: 0.2663 - val_acc: 0.9508\n",
            "Epoch 28/50\n",
            "242/242 [==============================] - 0s 148us/sample - loss: 0.3437 - acc: 0.8471 - val_loss: 0.2617 - val_acc: 0.9508\n",
            "Epoch 29/50\n",
            "242/242 [==============================] - 0s 148us/sample - loss: 0.3362 - acc: 0.8554 - val_loss: 0.2673 - val_acc: 0.9180\n",
            "Epoch 30/50\n",
            "242/242 [==============================] - 0s 147us/sample - loss: 0.3318 - acc: 0.8512 - val_loss: 0.2623 - val_acc: 0.9344\n",
            "Epoch 31/50\n",
            "242/242 [==============================] - 0s 131us/sample - loss: 0.3309 - acc: 0.8595 - val_loss: 0.2857 - val_acc: 0.9016\n",
            "Epoch 32/50\n",
            "242/242 [==============================] - 0s 151us/sample - loss: 0.3288 - acc: 0.8512 - val_loss: 0.2662 - val_acc: 0.9344\n",
            "Epoch 33/50\n",
            "242/242 [==============================] - 0s 144us/sample - loss: 0.3289 - acc: 0.8636 - val_loss: 0.2690 - val_acc: 0.9508\n",
            "Epoch 34/50\n",
            "242/242 [==============================] - 0s 127us/sample - loss: 0.3213 - acc: 0.8719 - val_loss: 0.2680 - val_acc: 0.9180\n",
            "Epoch 35/50\n",
            "242/242 [==============================] - 0s 154us/sample - loss: 0.3186 - acc: 0.8678 - val_loss: 0.2654 - val_acc: 0.9180\n",
            "Epoch 36/50\n",
            "242/242 [==============================] - 0s 151us/sample - loss: 0.3297 - acc: 0.8512 - val_loss: 0.2952 - val_acc: 0.9180\n",
            "Epoch 37/50\n",
            "242/242 [==============================] - 0s 137us/sample - loss: 0.3250 - acc: 0.8636 - val_loss: 0.2826 - val_acc: 0.9180\n",
            "Epoch 38/50\n",
            "242/242 [==============================] - 0s 129us/sample - loss: 0.3472 - acc: 0.8306 - val_loss: 0.2902 - val_acc: 0.9180\n",
            "Epoch 39/50\n",
            "242/242 [==============================] - 0s 130us/sample - loss: 0.3203 - acc: 0.8636 - val_loss: 0.2815 - val_acc: 0.9180\n",
            "Epoch 40/50\n",
            "242/242 [==============================] - 0s 176us/sample - loss: 0.3103 - acc: 0.8636 - val_loss: 0.2737 - val_acc: 0.9180\n",
            "Epoch 41/50\n",
            "242/242 [==============================] - 0s 129us/sample - loss: 0.3057 - acc: 0.8636 - val_loss: 0.2742 - val_acc: 0.9180\n",
            "Epoch 42/50\n",
            "242/242 [==============================] - 0s 139us/sample - loss: 0.2974 - acc: 0.8719 - val_loss: 0.2756 - val_acc: 0.9508\n",
            "Epoch 43/50\n",
            "242/242 [==============================] - 0s 132us/sample - loss: 0.3600 - acc: 0.8347 - val_loss: 0.2711 - val_acc: 0.9344\n",
            "Epoch 44/50\n",
            "242/242 [==============================] - 0s 139us/sample - loss: 0.2982 - acc: 0.8636 - val_loss: 0.2731 - val_acc: 0.9508\n",
            "Epoch 45/50\n",
            "242/242 [==============================] - 0s 148us/sample - loss: 0.3114 - acc: 0.8719 - val_loss: 0.2722 - val_acc: 0.9508\n",
            "Epoch 46/50\n",
            "242/242 [==============================] - 0s 134us/sample - loss: 0.2924 - acc: 0.8802 - val_loss: 0.2724 - val_acc: 0.9344\n",
            "Epoch 47/50\n",
            "242/242 [==============================] - 0s 131us/sample - loss: 0.2863 - acc: 0.8843 - val_loss: 0.2700 - val_acc: 0.9344\n",
            "Epoch 48/50\n",
            "242/242 [==============================] - 0s 124us/sample - loss: 0.2881 - acc: 0.8719 - val_loss: 0.2745 - val_acc: 0.9180\n",
            "Epoch 49/50\n",
            "242/242 [==============================] - 0s 134us/sample - loss: 0.3002 - acc: 0.8719 - val_loss: 0.2805 - val_acc: 0.9344\n",
            "Epoch 50/50\n",
            "242/242 [==============================] - 0s 134us/sample - loss: 0.2857 - acc: 0.8802 - val_loss: 0.2759 - val_acc: 0.9344\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f84adc53940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YW3yKrJWTedz",
        "colab_type": "code",
        "outputId": "7e9301b5-b9d9-4032-e97b-78dd554b5148",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(f\"Accuracy: {accuracy_score(np.round(model.predict(X_test)),y_test)}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9344262295081968\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJgbVYDyfpEk",
        "colab_type": "text"
      },
      "source": [
        "###Results, with tuning (Keras):\n",
        "- *Version 1.0*: *no dropout rate*, learning rate: 0.01, epochs = 20, batch size = 10. ***0.7868852459016393***\n",
        "- *Version 1.1*: *with dropout*, learning rate: 0.01, epochs = 50, batch size = 10. ***0.8852459016393442***\n",
        "\n",
        "- *Version 1.2*: with dropout.  same LR, epoch, and *batch sizes: 20* ***Accuracy: 0.9344262295081968***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQIr4unkTnhK",
        "colab_type": "text"
      },
      "source": [
        "### Step 4: Using Grid Search to find the best parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlVDb02WTsCE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to create model, required for KerasClassifier:\n",
        "def gridsearch_create_model():\n",
        "  # create model\n",
        "  model = Sequential()\n",
        "  model.add(Dense(13, input_dim=13, activation='relu'))\n",
        "  model.add(Dense(12, activation='sigmoid'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  # compile model\n",
        "  adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "  # create model\n",
        "model2 = KerasClassifier(build_fn=gridsearch_create_model, verbose=0)\n",
        "\n",
        "  # define the grid search parameters\n",
        "param_grid = {'batch_size': [32, 64, 128, 256, 512],\n",
        "                'epochs': [100]}\n",
        "\n",
        "                # see \"Results, with tuning:\" cell below for version changes\n",
        "                # note: original batch_size: [10, 20, 40, 60, 80, 100],\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "842-KQc3TsG9",
        "colab_type": "code",
        "outputId": "736021cd-6983-4299-af83-f6480790bf02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "# Create grid search\n",
        "grid = GridSearchCV(estimator=model2, param_grid=param_grid)\n",
        "grid_result = grid.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Dgjgij1TsKk",
        "colab_type": "code",
        "outputId": "5d314030-fe9b-4394-a9c8-84457983fff0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "  print(f\"Means: {mean}, Stdev: {stdev} with: {params}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.8057851306170472 using {'batch_size': 128, 'epochs': 100}\n",
            "Means: 0.8057851259373436, Stdev: 0.01634438450914547 with: [{'batch_size': 32, 'epochs': 100}, {'batch_size': 64, 'epochs': 100}, {'batch_size': 128, 'epochs': 100}, {'batch_size': 256, 'epochs': 100}, {'batch_size': 512, 'epochs': 100}]\n",
            "Means: 0.7933884391114732, Stdev: 0.006529184827057151 with: [{'batch_size': 32, 'epochs': 100}, {'batch_size': 64, 'epochs': 100}, {'batch_size': 128, 'epochs': 100}, {'batch_size': 256, 'epochs': 100}, {'batch_size': 512, 'epochs': 100}]\n",
            "Means: 0.8057851306170472, Stdev: 0.042027961140042376 with: [{'batch_size': 32, 'epochs': 100}, {'batch_size': 64, 'epochs': 100}, {'batch_size': 128, 'epochs': 100}, {'batch_size': 256, 'epochs': 100}, {'batch_size': 512, 'epochs': 100}]\n",
            "Means: 0.7644628104099558, Stdev: 0.06307058614582445 with: [{'batch_size': 32, 'epochs': 100}, {'batch_size': 64, 'epochs': 100}, {'batch_size': 128, 'epochs': 100}, {'batch_size': 256, 'epochs': 100}, {'batch_size': 512, 'epochs': 100}]\n",
            "Means: 0.7190082568275041, Stdev: 0.07312253093226838 with: [{'batch_size': 32, 'epochs': 100}, {'batch_size': 64, 'epochs': 100}, {'batch_size': 128, 'epochs': 100}, {'batch_size': 256, 'epochs': 100}, {'batch_size': 512, 'epochs': 100}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrwQRIT3YCYf",
        "colab_type": "text"
      },
      "source": [
        "###Results, with tuning (Grid Search):\n",
        "*(adjustments made on: learning rate, batch size, epochs)*\n",
        "- *Version 1.0:*\n",
        "reduced learning rate to: 0.001, kept batch_size: 10, epochs: 20 = **Best: 0.8057851259373436**\n",
        "- *Version 1.1:* Increased epochs to 40. **Best: 0.8016528969953868 using {'batch_size': 10, 'epochs': 40}**\n",
        "- *Version 1.2:* Increased epochs to 50. **Best: 0.8181818282801258 using {'batch_size': 10, 'epochs': 50}**\n",
        "- *Version 1.3:* Increased batch size to 32, 64, 128, etc. ***Best: 0.7520661083134738 using {'batch_size': 32, 'epochs': 50}***\n",
        "- *Version 1.4:* increased epochs to 100. ***Best: 0.8057851306170472 using {'batch_size': 64, 'epochs': 100}***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVK9Gw7iTsFG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}