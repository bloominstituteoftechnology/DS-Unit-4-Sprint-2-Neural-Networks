{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2*\n",
    "\n",
    "# Sprint Challenge - Neural Network Foundations\n",
    "\n",
    "Table of Problems\n",
    "\n",
    "1. [Defining Neural Networks](#Q1)\n",
    "2. [Perceptron on XOR Gates](#Q2)\n",
    "3. [Multilayer Perceptron](#Q3)\n",
    "4. [Keras MMP](#Q4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Q1\"></a>\n",
    "## 1. Define the following terms:\n",
    "\n",
    "- **Neuron:** An individual node in a layer of a Neural Network. Each neuron sends the weighted sum of its inputs through an activation function. The output of a neuron is the value of the activation function.\n",
    "\n",
    "- **Input Layer:** The first layer of a Neural Network which takes the values of features as inputs.\n",
    "\n",
    "- **Hidden Layer:** Any layer between the input and output layer. These layers add complexity.\n",
    "\n",
    "- **Output Layer:** The final layer of a Neural Network which returns the output we're looking for.\n",
    "\n",
    "- **Activation:** The function that determines the final output of a Neuron. This is analogous to \"how much a Neuron fires\" in the brain.\n",
    "\n",
    "- **Backpropagation:** The process by which weights in the Neural Network are adjusted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Perceptron on XOR Gates <a id=\"Q3=2\"></a>\n",
    "\n",
    "Create a perceptron class that can model the behavior of an AND gate. You can use the following table as your training data:\n",
    "\n",
    "|x1\t|x2|x3|\ty|\n",
    "|---|---|---|---|\n",
    "1|\t1|\t1|\t1|\n",
    "1|\t0|\t1|\t0|\n",
    "0|\t1|\t1|\t0|\n",
    "0|\t0|\t1|\t0|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set the input and output\n",
    "inputs = np.array([\n",
    "    [1, 1, 1],\n",
    "    [1, 0, 1],\n",
    "    [0, 1, 1],\n",
    "    [0, 0, 1]\n",
    "])\n",
    "\n",
    "correct_outputs = [[1], [0], [0], [0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the activation function & derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivative of sigmoid\n",
    "def sigmoid_derivative(x):\n",
    "    sx = sigmoid(x)\n",
    "    return sx * (1-sx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.25091976],\n",
       "       [ 0.90142861],\n",
       "       [ 0.46398788]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep weights bounded [-1, 1]\n",
    "weights = 2 * np.random.random((3, 1)) - 1\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output after Training:\n",
      "[[9.96429406e-01]\n",
      " [2.00908516e-03]\n",
      " [2.00908516e-03]\n",
      " [1.45223694e-08]]\n"
     ]
    }
   ],
   "source": [
    "# trian for 10,000 epochs\n",
    "for iteration in range(10000):\n",
    "    # weighted sum\n",
    "    weighted_sum = np.dot(inputs, weights)\n",
    "    \n",
    "    # Activate using Sigmoid\n",
    "    activated_output = sigmoid(weighted_sum)\n",
    "    \n",
    "    # Errors & Adjustments\n",
    "    error = correct_outputs - activated_output\n",
    "    adjustments = error * sigmoid_derivative(activated_output)\n",
    "    \n",
    "    # Update weights\n",
    "    weights += np.dot(inputs.T, adjustments)\n",
    "    \n",
    "print('Output after Training:')\n",
    "print(activated_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multilayer Perceptron <a id=\"Q3\"></a>\n",
    "\n",
    "Implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights.\n",
    "Your network must have one hidden layer.\n",
    "You do not have to update weights via gradient descent. You can use something like the derivative of the sigmoid function to update weights.\n",
    "Train your model on the Heart Disease dataset from UCI:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>276</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>170</td>\n",
       "      <td>288</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>126</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>134</td>\n",
       "      <td>409</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>110</td>\n",
       "      <td>265</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "179   57    1   0       150   276    0        0      112      1      0.6   \n",
       "228   59    1   3       170   288    0        0      159      0      0.2   \n",
       "111   57    1   2       150   126    1        1      173      0      0.2   \n",
       "246   56    0   0       134   409    0        0      150      1      1.9   \n",
       "60    71    0   2       110   265    1        0      130      0      0.0   \n",
       "\n",
       "     slope  ca  thal  target  \n",
       "179      1   1     1       0  \n",
       "228      1   0     3       0  \n",
       "111      2   1     3       1  \n",
       "246      1   2     3       0  \n",
       "60       2   1     2       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
    "df = df.sample(frac=1)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "# Select Features & train_test split\n",
    "features = df.columns.tolist()[0:13]\n",
    "target = df.columns.tolist()[13]\n",
    "\n",
    "# Split by features & target\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "# Instantiate scaler\n",
    "scaler = StandardScaler()\n",
    "# Normalize X\n",
    "X = pd.DataFrame(scaler.fit_transform(X), columns = features)\n",
    "\n",
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(242, 13)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(61, 13)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self):\n",
    "        # Set up Architecture\n",
    "        self.inputs = 13\n",
    "        self.hiddenNodes = 4\n",
    "        self.output = 1\n",
    "        \n",
    "        # Initialize Weights\n",
    "        self.weights1 = np.random.rand(self.inputs, self.hiddenNodes)\n",
    "        self.weights2 = np.random.rand(self.hiddenNodes, self.output)\n",
    "        \n",
    "    # Define the activation function & derivative\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # Derivative of sigmoid\n",
    "    def sigmoid_derivative(self, x):\n",
    "        sx = self.sigmoid(x)\n",
    "        return sx * (1-sx)\n",
    "    \n",
    "    # Forward Propogation\n",
    "    def feed_forward(self, X):\n",
    "        # Activation of Weighted Sum\n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "        \n",
    "        # Next Layer\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "        \n",
    "        return self.activated_output\n",
    "    \n",
    "    # Backward Propagation\n",
    "    def feed_backward(self, X, y, y_pred):\n",
    "        # Output --> Hidden\n",
    "        self.output_error = y.values - y_pred\n",
    "        self.output_delta = self.output_error.dot(self.sigmoid_derivative(y_pred))\n",
    "        \n",
    "        # Hidden --> Input\n",
    "        self.input_error = self.output_delta.dot(self.weights2.T)\n",
    "        self.input_delta = self.input_error * self.sigmoid_derivative(self.activated_hidden)\n",
    "        \n",
    "        # Update Weights\n",
    "        self.weights1 += X.T.dot(self.input_delta)\n",
    "        self.weights2 += self.activated_hidden.T.dot(self.output_delta)\n",
    "        \n",
    "    # Train MLP\n",
    "    def train(self, X, y):\n",
    "        y_pred = self.feed_forward(X)\n",
    "        self.feed_backward(X, y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate MLP\n",
    "mlp = MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------EPOCH 1---------+\n",
      "Loss: \n",
      " 0.45544554455445546\n",
      "+---------EPOCH 2---------+\n",
      "Loss: \n",
      " 0.5445544554455446\n",
      "+---------EPOCH 3---------+\n",
      "Loss: \n",
      " 0.45544554455445546\n",
      "+---------EPOCH 4---------+\n",
      "Loss: \n",
      " 0.45544554455445546\n",
      "+---------EPOCH 5---------+\n",
      "Loss: \n",
      " 0.5445544554455446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------EPOCH 10000---------+\n",
      "Loss: \n",
      " 0.45544554455445546\n",
      "+---------EPOCH 20000---------+\n",
      "Loss: \n",
      " 0.45544554455445546\n",
      "+---------EPOCH 30000---------+\n",
      "Loss: \n",
      " 0.45544554455445546\n",
      "+---------EPOCH 40000---------+\n",
      "Loss: \n",
      " 0.5445544554455446\n",
      "+---------EPOCH 50000---------+\n",
      "Loss: \n",
      " 0.5445544554455446\n",
      "+---------EPOCH 60000---------+\n",
      "Loss: \n",
      " 0.5445544554455446\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-0a7a208029cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'+'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'---'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mf'EPOCH {i+1}'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'---'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'+'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss: \\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-54-3001c7bcc9e7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-54-3001c7bcc9e7>\u001b[0m in \u001b[0;36mfeed_backward\u001b[0;34m(self, X, y, y_pred)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Output --> Hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_delta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_error\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Hidden --> Input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set number of Epochs\n",
    "for i in range(100000):\n",
    "    if (i+1 in [1,2,3,4,5]) or ((i+1) % 10000 ==0):\n",
    "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
    "        print(\"Loss: \\n\", str(np.mean(np.square(y.values - mlp.feed_forward(X)))))\n",
    "    mlp.train(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Keras MMP <a id=\"Q4\"></a>\n",
    "\n",
    "Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy.\n",
    "Use the Heart Disease Dataset (binary classification)\n",
    "Use an appropriate loss function for a binary classification task\n",
    "Use an appropriate activation function on the final layer of your network.\n",
    "Train your model using verbose output for ease of grading.\n",
    "Use GridSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
    "When hyperparameter tuning, show you work by adding code cells for each new experiment.\n",
    "Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
    "You must hyperparameter tune at least 5 parameters in order to get a 3 on this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "      <td>201</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>110</td>\n",
       "      <td>175</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>234</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>160</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>138</td>\n",
       "      <td>220</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>152</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "      <td>303</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>181</td>\n",
       "      <td>0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "138   57    1   0       110   201    0        1      126      1      1.5   \n",
       "27    51    1   2       110   175    0        1      123      0      0.6   \n",
       "49    53    0   0       138   234    0        0      160      0      0.0   \n",
       "154   39    0   2       138   220    0        1      152      0      0.0   \n",
       "141   43    1   0       115   303    0        1      181      0      1.2   \n",
       "\n",
       "     slope  ca  thal  target  \n",
       "138      1   0     1       1  \n",
       "27       2   0     2       1  \n",
       "49       2   0     2       1  \n",
       "154      1   0     2       1  \n",
       "141      1   0     2       1  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
    "df = df.sample(frac=1)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "# Select Features & train_test split\n",
    "features = df.columns.tolist()[0:13]\n",
    "target = df.columns.tolist()[13]\n",
    "\n",
    "# Split by features & target\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "# Instantiate scaler\n",
    "scaler = StandardScaler()\n",
    "# Normalize X\n",
    "X = pd.DataFrame(scaler.fit_transform(X), columns = features)\n",
    "\n",
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(812)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model function for Keras Classifier\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=13, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "242/242 [==============================] - 0s 517us/sample - loss: 0.7311 - acc: 0.5207\n",
      "Epoch 2/10\n",
      "242/242 [==============================] - 0s 93us/sample - loss: 0.7029 - acc: 0.5455\n",
      "Epoch 3/10\n",
      "242/242 [==============================] - 0s 125us/sample - loss: 0.6763 - acc: 0.5785\n",
      "Epoch 4/10\n",
      "242/242 [==============================] - 0s 125us/sample - loss: 0.6536 - acc: 0.6116\n",
      "Epoch 5/10\n",
      "242/242 [==============================] - 0s 167us/sample - loss: 0.6314 - acc: 0.6405\n",
      "Epoch 6/10\n",
      "242/242 [==============================] - 0s 84us/sample - loss: 0.6103 - acc: 0.6694\n",
      "Epoch 7/10\n",
      "242/242 [==============================] - 0s 56us/sample - loss: 0.5914 - acc: 0.6818\n",
      "Epoch 8/10\n",
      "242/242 [==============================] - 0s 178us/sample - loss: 0.5740 - acc: 0.6983\n",
      "Epoch 9/10\n",
      "242/242 [==============================] - 0s 61us/sample - loss: 0.5567 - acc: 0.7066\n",
      "Epoch 10/10\n",
      "242/242 [==============================] - 0s 47us/sample - loss: 0.5412 - acc: 0.7314\n",
      "61/61 [==============================] - 0s 610us/sample - loss: 0.5605 - acc: 0.6721\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.560492181387104, 0.6721311]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check baseline model stats\n",
    "model = create_model()\n",
    "\n",
    "# Fit\n",
    "model.fit(X_train, y_train, epochs=10)\n",
    "\n",
    "# Evaluate\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseline Accuracy: 67.21%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define search params\n",
    "param_grid = {'batch_size': [10, 20, 40, 60, 80, 100],\n",
    "              'epochs': [10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "202/202 [==============================] - 0s 883us/sample - loss: 0.7092 - acc: 0.5891\n",
      "Epoch 2/10\n",
      "202/202 [==============================] - 0s 254us/sample - loss: 0.6597 - acc: 0.6436\n",
      "Epoch 3/10\n",
      "202/202 [==============================] - 0s 138us/sample - loss: 0.6157 - acc: 0.6832\n",
      "Epoch 4/10\n",
      "202/202 [==============================] - 0s 261us/sample - loss: 0.5806 - acc: 0.7079\n",
      "Epoch 5/10\n",
      "202/202 [==============================] - 0s 253us/sample - loss: 0.5486 - acc: 0.7376\n",
      "Epoch 6/10\n",
      "202/202 [==============================] - 0s 330us/sample - loss: 0.5221 - acc: 0.7772\n",
      "Epoch 7/10\n",
      "202/202 [==============================] - 0s 257us/sample - loss: 0.4990 - acc: 0.7970\n",
      "Epoch 8/10\n",
      "202/202 [==============================] - 0s 280us/sample - loss: 0.4786 - acc: 0.8069\n",
      "Epoch 9/10\n",
      "202/202 [==============================] - 0s 217us/sample - loss: 0.4601 - acc: 0.8168\n",
      "Epoch 10/10\n",
      "202/202 [==============================] - 0s 307us/sample - loss: 0.4455 - acc: 0.8218\n",
      "101/101 [==============================] - 0s 522us/sample - loss: 0.4585 - acc: 0.8119\n",
      "202/202 [==============================] - 0s 182us/sample - loss: 0.4379 - acc: 0.8317\n",
      "Epoch 1/10\n",
      "202/202 [==============================] - 0s 728us/sample - loss: 0.6564 - acc: 0.6238\n",
      "Epoch 2/10\n",
      "202/202 [==============================] - 0s 241us/sample - loss: 0.6017 - acc: 0.6782\n",
      "Epoch 3/10\n",
      "202/202 [==============================] - 0s 263us/sample - loss: 0.5609 - acc: 0.7376\n",
      "Epoch 4/10\n",
      "202/202 [==============================] - 0s 250us/sample - loss: 0.5269 - acc: 0.7723\n",
      "Epoch 5/10\n",
      "202/202 [==============================] - 0s 250us/sample - loss: 0.4985 - acc: 0.7970\n",
      "Epoch 6/10\n",
      "202/202 [==============================] - 0s 180us/sample - loss: 0.4738 - acc: 0.7970\n",
      "Epoch 7/10\n",
      "202/202 [==============================] - 0s 241us/sample - loss: 0.4543 - acc: 0.8119\n",
      "Epoch 8/10\n",
      "202/202 [==============================] - 0s 228us/sample - loss: 0.4388 - acc: 0.8218\n",
      "Epoch 9/10\n",
      "202/202 [==============================] - 0s 237us/sample - loss: 0.4238 - acc: 0.8267\n",
      "Epoch 10/10\n",
      "202/202 [==============================] - 0s 392us/sample - loss: 0.4135 - acc: 0.8366\n",
      "101/101 [==============================] - 0s 512us/sample - loss: 0.4136 - acc: 0.8416\n",
      "202/202 [==============================] - 0s 158us/sample - loss: 0.4060 - acc: 0.8366\n",
      "Epoch 1/10\n",
      "202/202 [==============================] - 0s 957us/sample - loss: 0.9502 - acc: 0.3713\n",
      "Epoch 2/10\n",
      "202/202 [==============================] - 0s 248us/sample - loss: 0.8690 - acc: 0.4109\n",
      "Epoch 3/10\n",
      "202/202 [==============================] - 0s 285us/sample - loss: 0.7997 - acc: 0.4703\n",
      "Epoch 4/10\n",
      "202/202 [==============================] - 0s 211us/sample - loss: 0.7401 - acc: 0.5396\n",
      "Epoch 5/10\n",
      "202/202 [==============================] - 0s 206us/sample - loss: 0.6878 - acc: 0.5644\n",
      "Epoch 6/10\n",
      "202/202 [==============================] - 0s 156us/sample - loss: 0.6412 - acc: 0.6485\n",
      "Epoch 7/10\n",
      "202/202 [==============================] - 0s 308us/sample - loss: 0.6011 - acc: 0.6683\n",
      "Epoch 8/10\n",
      "202/202 [==============================] - 0s 270us/sample - loss: 0.5677 - acc: 0.6980\n",
      "Epoch 9/10\n",
      "202/202 [==============================] - 0s 193us/sample - loss: 0.5389 - acc: 0.7228\n",
      "Epoch 10/10\n",
      "202/202 [==============================] - 0s 244us/sample - loss: 0.5115 - acc: 0.7624\n",
      "101/101 [==============================] - 0s 549us/sample - loss: 0.5354 - acc: 0.7624\n",
      "202/202 [==============================] - 0s 100us/sample - loss: 0.4972 - acc: 0.7723\n",
      "Epoch 1/10\n",
      "202/202 [==============================] - 0s 657us/sample - loss: 0.8463 - acc: 0.4604\n",
      "Epoch 2/10\n",
      "202/202 [==============================] - 0s 167us/sample - loss: 0.7910 - acc: 0.5050\n",
      "Epoch 3/10\n",
      "202/202 [==============================] - 0s 161us/sample - loss: 0.7444 - acc: 0.5396\n",
      "Epoch 4/10\n",
      "202/202 [==============================] - 0s 224us/sample - loss: 0.7023 - acc: 0.5842\n",
      "Epoch 5/10\n",
      "202/202 [==============================] - 0s 180us/sample - loss: 0.6668 - acc: 0.6188\n",
      "Epoch 6/10\n",
      "202/202 [==============================] - 0s 170us/sample - loss: 0.6386 - acc: 0.6485\n",
      "Epoch 7/10\n",
      "202/202 [==============================] - 0s 173us/sample - loss: 0.6131 - acc: 0.6931\n",
      "Epoch 8/10\n",
      "202/202 [==============================] - 0s 202us/sample - loss: 0.5921 - acc: 0.7030\n",
      "Epoch 9/10\n",
      "202/202 [==============================] - 0s 188us/sample - loss: 0.5732 - acc: 0.7228\n",
      "Epoch 10/10\n",
      "202/202 [==============================] - 0s 170us/sample - loss: 0.5556 - acc: 0.7574\n",
      "101/101 [==============================] - 0s 552us/sample - loss: 0.5429 - acc: 0.7525\n",
      "202/202 [==============================] - 0s 43us/sample - loss: 0.5462 - acc: 0.7723\n",
      "Epoch 1/10\n",
      "202/202 [==============================] - 0s 874us/sample - loss: 0.7334 - acc: 0.5842\n",
      "Epoch 2/10\n",
      "202/202 [==============================] - 0s 211us/sample - loss: 0.6986 - acc: 0.5941\n",
      "Epoch 3/10\n",
      "202/202 [==============================] - 0s 171us/sample - loss: 0.6685 - acc: 0.6188\n",
      "Epoch 4/10\n",
      "202/202 [==============================] - 0s 168us/sample - loss: 0.6424 - acc: 0.6188\n",
      "Epoch 5/10\n",
      "202/202 [==============================] - 0s 225us/sample - loss: 0.6173 - acc: 0.6535\n",
      "Epoch 6/10\n",
      "202/202 [==============================] - 0s 218us/sample - loss: 0.5957 - acc: 0.6832\n",
      "Epoch 7/10\n",
      "202/202 [==============================] - 0s 187us/sample - loss: 0.5761 - acc: 0.7079\n",
      "Epoch 8/10\n",
      "202/202 [==============================] - 0s 165us/sample - loss: 0.5579 - acc: 0.7178\n",
      "Epoch 9/10\n",
      "202/202 [==============================] - 0s 148us/sample - loss: 0.5428 - acc: 0.7228\n",
      "Epoch 10/10\n",
      "202/202 [==============================] - 0s 190us/sample - loss: 0.5277 - acc: 0.7327\n",
      "101/101 [==============================] - 0s 599us/sample - loss: 0.4834 - acc: 0.8416\n",
      "202/202 [==============================] - 0s 42us/sample - loss: 0.5184 - acc: 0.7426\n",
      "Epoch 1/10\n",
      "202/202 [==============================] - 0s 763us/sample - loss: 0.9468 - acc: 0.3069\n",
      "Epoch 2/10\n",
      "202/202 [==============================] - 0s 170us/sample - loss: 0.8987 - acc: 0.3317\n",
      "Epoch 3/10\n",
      "202/202 [==============================] - 0s 163us/sample - loss: 0.8580 - acc: 0.3564\n",
      "Epoch 4/10\n",
      "202/202 [==============================] - 0s 136us/sample - loss: 0.8198 - acc: 0.4307\n",
      "Epoch 5/10\n",
      "202/202 [==============================] - 0s 259us/sample - loss: 0.7883 - acc: 0.4653\n",
      "Epoch 6/10\n",
      "202/202 [==============================] - 0s 81us/sample - loss: 0.7592 - acc: 0.5248\n",
      "Epoch 7/10\n",
      "202/202 [==============================] - 0s 173us/sample - loss: 0.7320 - acc: 0.5644\n",
      "Epoch 8/10\n",
      "202/202 [==============================] - 0s 101us/sample - loss: 0.7072 - acc: 0.5990\n",
      "Epoch 9/10\n",
      "202/202 [==============================] - 0s 219us/sample - loss: 0.6836 - acc: 0.6089\n",
      "Epoch 10/10\n",
      "202/202 [==============================] - 0s 90us/sample - loss: 0.6607 - acc: 0.6287\n",
      "101/101 [==============================] - 0s 642us/sample - loss: 0.6717 - acc: 0.6040\n",
      "202/202 [==============================] - 0s 45us/sample - loss: 0.6490 - acc: 0.6436\n",
      "Epoch 1/10\n",
      "202/202 [==============================] - 0s 977us/sample - loss: 0.8810 - acc: 0.3267\n",
      "Epoch 2/10\n",
      "202/202 [==============================] - 0s 202us/sample - loss: 0.8522 - acc: 0.3861\n",
      "Epoch 3/10\n",
      "202/202 [==============================] - 0s 95us/sample - loss: 0.8278 - acc: 0.4109\n",
      "Epoch 4/10\n",
      "202/202 [==============================] - 0s 47us/sample - loss: 0.8052 - acc: 0.4208\n",
      "Epoch 5/10\n",
      "202/202 [==============================] - 0s 170us/sample - loss: 0.7864 - acc: 0.4455\n",
      "Epoch 6/10\n",
      "202/202 [==============================] - 0s 147us/sample - loss: 0.7673 - acc: 0.4851\n",
      "Epoch 7/10\n",
      "202/202 [==============================] - 0s 126us/sample - loss: 0.7497 - acc: 0.5000\n",
      "Epoch 8/10\n",
      "202/202 [==============================] - 0s 172us/sample - loss: 0.7346 - acc: 0.5198\n",
      "Epoch 9/10\n",
      "202/202 [==============================] - 0s 97us/sample - loss: 0.7204 - acc: 0.5297\n",
      "Epoch 10/10\n",
      "202/202 [==============================] - 0s 176us/sample - loss: 0.7067 - acc: 0.5743\n",
      "101/101 [==============================] - 0s 1ms/sample - loss: 0.7057 - acc: 0.5545\n",
      "202/202 [==============================] - 0s 116us/sample - loss: 0.6973 - acc: 0.5990\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202/202 [==============================] - 0s 661us/sample - loss: 1.1438 - acc: 0.4356\n",
      "Epoch 2/10\n",
      "202/202 [==============================] - 0s 52us/sample - loss: 1.0950 - acc: 0.4406\n",
      "Epoch 3/10\n",
      "202/202 [==============================] - 0s 73us/sample - loss: 1.0548 - acc: 0.4455\n",
      "Epoch 4/10\n",
      "202/202 [==============================] - 0s 109us/sample - loss: 1.0175 - acc: 0.4554\n",
      "Epoch 5/10\n",
      "202/202 [==============================] - 0s 120us/sample - loss: 0.9853 - acc: 0.4703\n",
      "Epoch 6/10\n",
      "202/202 [==============================] - 0s 100us/sample - loss: 0.9528 - acc: 0.4752\n",
      "Epoch 7/10\n",
      "202/202 [==============================] - 0s 108us/sample - loss: 0.9192 - acc: 0.4802\n",
      "Epoch 8/10\n",
      "202/202 [==============================] - 0s 93us/sample - loss: 0.8912 - acc: 0.4851\n",
      "Epoch 9/10\n",
      "202/202 [==============================] - 0s 117us/sample - loss: 0.8647 - acc: 0.5000\n",
      "Epoch 10/10\n",
      "202/202 [==============================] - 0s 116us/sample - loss: 0.8408 - acc: 0.5099\n",
      "101/101 [==============================] - 0s 1ms/sample - loss: 0.9040 - acc: 0.4455\n",
      "202/202 [==============================] - 0s 180us/sample - loss: 0.8253 - acc: 0.5099\n",
      "Epoch 1/10\n",
      "202/202 [==============================] - 0s 744us/sample - loss: 0.6887 - acc: 0.5693\n",
      "Epoch 2/10\n",
      "202/202 [==============================] - 0s 47us/sample - loss: 0.6742 - acc: 0.5842\n",
      "Epoch 3/10\n",
      "202/202 [==============================] - 0s 96us/sample - loss: 0.6622 - acc: 0.5941\n",
      "Epoch 4/10\n",
      "202/202 [==============================] - 0s 56us/sample - loss: 0.6512 - acc: 0.6089\n",
      "Epoch 5/10\n",
      "202/202 [==============================] - 0s 107us/sample - loss: 0.6395 - acc: 0.6337\n",
      "Epoch 6/10\n",
      "202/202 [==============================] - 0s 141us/sample - loss: 0.6284 - acc: 0.6683\n",
      "Epoch 7/10\n",
      "202/202 [==============================] - 0s 89us/sample - loss: 0.6179 - acc: 0.6980\n",
      "Epoch 8/10\n",
      "202/202 [==============================] - 0s 104us/sample - loss: 0.6072 - acc: 0.7178\n",
      "Epoch 9/10\n",
      "202/202 [==============================] - 0s 89us/sample - loss: 0.5975 - acc: 0.7376\n",
      "Epoch 10/10\n",
      "202/202 [==============================] - 0s 101us/sample - loss: 0.5879 - acc: 0.7525\n",
      "101/101 [==============================] - 0s 939us/sample - loss: 0.5744 - acc: 0.7129\n",
      "202/202 [==============================] - 0s 30us/sample - loss: 0.5820 - acc: 0.7574\n",
      "Epoch 1/10\n",
      "202/202 [==============================] - 0s 690us/sample - loss: 0.6464 - acc: 0.6782\n",
      "Epoch 2/10\n",
      "202/202 [==============================] - 0s 46us/sample - loss: 0.6321 - acc: 0.6881\n",
      "Epoch 3/10\n",
      "202/202 [==============================] - 0s 77us/sample - loss: 0.6178 - acc: 0.6931\n",
      "Epoch 4/10\n",
      "202/202 [==============================] - 0s 93us/sample - loss: 0.6044 - acc: 0.6980\n",
      "Epoch 5/10\n",
      "202/202 [==============================] - 0s 94us/sample - loss: 0.5916 - acc: 0.7030\n",
      "Epoch 6/10\n",
      "202/202 [==============================] - 0s 93us/sample - loss: 0.5801 - acc: 0.7030\n",
      "Epoch 7/10\n",
      "202/202 [==============================] - 0s 125us/sample - loss: 0.5675 - acc: 0.7129\n",
      "Epoch 8/10\n",
      "202/202 [==============================] - 0s 119us/sample - loss: 0.5563 - acc: 0.7129\n",
      "Epoch 9/10\n",
      "202/202 [==============================] - 0s 111us/sample - loss: 0.5460 - acc: 0.7228\n",
      "Epoch 10/10\n",
      "202/202 [==============================] - 0s 121us/sample - loss: 0.5352 - acc: 0.7426\n",
      "101/101 [==============================] - 0s 1ms/sample - loss: 0.5480 - acc: 0.7327\n",
      "202/202 [==============================] - 0s 23us/sample - loss: 0.5284 - acc: 0.7574\n",
      "Epoch 1/10\n",
      "202/202 [==============================] - 0s 713us/sample - loss: 0.9071 - acc: 0.3762\n",
      "Epoch 2/10\n",
      "202/202 [==============================] - 0s 43us/sample - loss: 0.8858 - acc: 0.3960\n",
      "Epoch 3/10\n",
      "202/202 [==============================] - 0s 58us/sample - loss: 0.8640 - acc: 0.3960\n",
      "Epoch 4/10\n",
      "202/202 [==============================] - 0s 88us/sample - loss: 0.8456 - acc: 0.4010\n",
      "Epoch 5/10\n",
      "202/202 [==============================] - 0s 111us/sample - loss: 0.8256 - acc: 0.4109\n",
      "Epoch 6/10\n",
      "202/202 [==============================] - 0s 75us/sample - loss: 0.8089 - acc: 0.4505\n",
      "Epoch 7/10\n",
      "202/202 [==============================] - 0s 84us/sample - loss: 0.7916 - acc: 0.4851\n",
      "Epoch 8/10\n",
      "202/202 [==============================] - 0s 98us/sample - loss: 0.7760 - acc: 0.4901\n",
      "Epoch 9/10\n",
      "202/202 [==============================] - 0s 73us/sample - loss: 0.7607 - acc: 0.5099\n",
      "Epoch 10/10\n",
      "202/202 [==============================] - 0s 92us/sample - loss: 0.7465 - acc: 0.5149\n",
      "101/101 [==============================] - 0s 789us/sample - loss: 0.7803 - acc: 0.5743\n",
      "202/202 [==============================] - 0s 21us/sample - loss: 0.7360 - acc: 0.5198\n",
      "Epoch 1/10\n",
      "202/202 [==============================] - 0s 718us/sample - loss: 0.6136 - acc: 0.6584\n",
      "Epoch 2/10\n",
      "202/202 [==============================] - 0s 46us/sample - loss: 0.5992 - acc: 0.6634\n",
      "Epoch 3/10\n",
      "202/202 [==============================] - 0s 36us/sample - loss: 0.5863 - acc: 0.6832\n",
      "Epoch 4/10\n",
      "202/202 [==============================] - 0s 82us/sample - loss: 0.5743 - acc: 0.6980\n",
      "Epoch 5/10\n",
      "202/202 [==============================] - 0s 89us/sample - loss: 0.5627 - acc: 0.7129\n",
      "Epoch 6/10\n",
      "202/202 [==============================] - 0s 93us/sample - loss: 0.5519 - acc: 0.7277\n",
      "Epoch 7/10\n",
      "202/202 [==============================] - 0s 74us/sample - loss: 0.5416 - acc: 0.7327\n",
      "Epoch 8/10\n",
      "202/202 [==============================] - 0s 87us/sample - loss: 0.5318 - acc: 0.7327\n",
      "Epoch 9/10\n",
      "202/202 [==============================] - 0s 79us/sample - loss: 0.5228 - acc: 0.7475\n",
      "Epoch 10/10\n",
      "202/202 [==============================] - 0s 92us/sample - loss: 0.5142 - acc: 0.7574\n",
      "101/101 [==============================] - 0s 818us/sample - loss: 0.5959 - acc: 0.6931\n",
      "202/202 [==============================] - 0s 27us/sample - loss: 0.5083 - acc: 0.7723\n",
      "Epoch 1/10\n",
      "202/202 [==============================] - 0s 729us/sample - loss: 0.8611 - acc: 0.5495\n",
      "Epoch 2/10\n",
      "202/202 [==============================] - 0s 28us/sample - loss: 0.8457 - acc: 0.5495\n",
      "Epoch 3/10\n",
      "202/202 [==============================] - 0s 31us/sample - loss: 0.8309 - acc: 0.5594\n",
      "Epoch 4/10\n",
      "202/202 [==============================] - 0s 31us/sample - loss: 0.8172 - acc: 0.5644\n",
      "Epoch 5/10\n",
      "202/202 [==============================] - 0s 78us/sample - loss: 0.8029 - acc: 0.5693\n",
      "Epoch 6/10\n",
      "202/202 [==============================] - 0s 36us/sample - loss: 0.7896 - acc: 0.5743\n",
      "Epoch 7/10\n",
      "202/202 [==============================] - 0s 85us/sample - loss: 0.7765 - acc: 0.5743\n",
      "Epoch 8/10\n",
      "202/202 [==============================] - 0s 74us/sample - loss: 0.7647 - acc: 0.5792\n",
      "Epoch 9/10\n",
      "202/202 [==============================] - 0s 57us/sample - loss: 0.7521 - acc: 0.5842\n",
      "Epoch 10/10\n",
      "202/202 [==============================] - 0s 71us/sample - loss: 0.7404 - acc: 0.5842\n",
      "101/101 [==============================] - 0s 883us/sample - loss: 0.8066 - acc: 0.4752\n",
      "202/202 [==============================] - 0s 39us/sample - loss: 0.7324 - acc: 0.5891\n",
      "Epoch 1/10\n",
      "202/202 [==============================] - 0s 967us/sample - loss: 1.0310 - acc: 0.3911\n",
      "Epoch 2/10\n",
      "202/202 [==============================] - 0s 29us/sample - loss: 1.0102 - acc: 0.3911\n",
      "Epoch 3/10\n",
      "202/202 [==============================] - 0s 47us/sample - loss: 0.9905 - acc: 0.3911\n",
      "Epoch 4/10\n",
      "202/202 [==============================] - 0s 29us/sample - loss: 0.9711 - acc: 0.3960\n",
      "Epoch 5/10\n",
      "202/202 [==============================] - 0s 94us/sample - loss: 0.9522 - acc: 0.3960\n",
      "Epoch 6/10\n",
      "202/202 [==============================] - 0s 68us/sample - loss: 0.9341 - acc: 0.4059\n",
      "Epoch 7/10\n",
      "202/202 [==============================] - 0s 57us/sample - loss: 0.9163 - acc: 0.4109\n",
      "Epoch 8/10\n",
      "202/202 [==============================] - 0s 65us/sample - loss: 0.8996 - acc: 0.4059\n",
      "Epoch 9/10\n",
      "202/202 [==============================] - 0s 75us/sample - loss: 0.8823 - acc: 0.4109\n",
      "Epoch 10/10\n",
      "202/202 [==============================] - 0s 75us/sample - loss: 0.8660 - acc: 0.4208\n",
      "101/101 [==============================] - 0s 949us/sample - loss: 0.8585 - acc: 0.4158\n",
      "202/202 [==============================] - 0s 19us/sample - loss: 0.8549 - acc: 0.4257\n",
      "Epoch 1/10\n",
      "202/202 [==============================] - 0s 776us/sample - loss: 0.9562 - acc: 0.4505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "202/202 [==============================] - 0s 33us/sample - loss: 0.9338 - acc: 0.4505\n",
      "Epoch 3/10\n",
      "202/202 [==============================] - 0s 30us/sample - loss: 0.9123 - acc: 0.4554\n",
      "Epoch 4/10\n",
      "202/202 [==============================] - 0s 34us/sample - loss: 0.8907 - acc: 0.4554\n",
      "Epoch 5/10\n",
      "202/202 [==============================] - 0s 72us/sample - loss: 0.8716 - acc: 0.4653\n",
      "Epoch 6/10\n",
      "202/202 [==============================] - 0s 46us/sample - loss: 0.8522 - acc: 0.4752\n",
      "Epoch 7/10\n",
      "202/202 [==============================] - 0s 70us/sample - loss: 0.8327 - acc: 0.4950\n",
      "Epoch 8/10\n",
      "202/202 [==============================] - 0s 103us/sample - loss: 0.8144 - acc: 0.5000\n",
      "Epoch 9/10\n",
      "202/202 [==============================] - 0s 122us/sample - loss: 0.7971 - acc: 0.5050\n",
      "Epoch 10/10\n",
      "202/202 [==============================] - 0s 76us/sample - loss: 0.7804 - acc: 0.5050\n",
      "101/101 [==============================] - 0s 990us/sample - loss: 0.8112 - acc: 0.5446\n",
      "202/202 [==============================] - 0s 28us/sample - loss: 0.7688 - acc: 0.5099\n",
      "Epoch 1/10\n",
      "202/202 [==============================] - 0s 834us/sample - loss: 0.6935 - acc: 0.5594\n",
      "Epoch 2/10\n",
      "202/202 [==============================] - 0s 28us/sample - loss: 0.6852 - acc: 0.5693\n",
      "Epoch 3/10\n",
      "202/202 [==============================] - 0s 30us/sample - loss: 0.6780 - acc: 0.5743\n",
      "Epoch 4/10\n",
      "202/202 [==============================] - 0s 31us/sample - loss: 0.6712 - acc: 0.5941\n",
      "Epoch 5/10\n",
      "202/202 [==============================] - 0s 83us/sample - loss: 0.6646 - acc: 0.5891\n",
      "Epoch 6/10\n",
      "202/202 [==============================] - 0s 29us/sample - loss: 0.6587 - acc: 0.5941\n",
      "Epoch 7/10\n",
      "202/202 [==============================] - 0s 34us/sample - loss: 0.6533 - acc: 0.6089\n",
      "Epoch 8/10\n",
      "202/202 [==============================] - 0s 31us/sample - loss: 0.6483 - acc: 0.6188\n",
      "Epoch 9/10\n",
      "202/202 [==============================] - 0s 32us/sample - loss: 0.6433 - acc: 0.6188\n",
      "Epoch 10/10\n",
      "202/202 [==============================] - 0s 34us/sample - loss: 0.6385 - acc: 0.6287\n",
      "101/101 [==============================] - 0s 1ms/sample - loss: 0.5879 - acc: 0.6535\n",
      "202/202 [==============================] - 0s 17us/sample - loss: 0.6344 - acc: 0.6337\n",
      "Epoch 1/10\n",
      "202/202 [==============================] - 0s 816us/sample - loss: 0.9901 - acc: 0.5050\n",
      "Epoch 2/10\n",
      "202/202 [==============================] - 0s 31us/sample - loss: 0.9743 - acc: 0.5149\n",
      "Epoch 3/10\n",
      "202/202 [==============================] - 0s 29us/sample - loss: 0.9620 - acc: 0.5248\n",
      "Epoch 4/10\n",
      "202/202 [==============================] - 0s 72us/sample - loss: 0.9510 - acc: 0.5248\n",
      "Epoch 5/10\n",
      "202/202 [==============================] - 0s 35us/sample - loss: 0.9402 - acc: 0.5248\n",
      "Epoch 6/10\n",
      "202/202 [==============================] - 0s 62us/sample - loss: 0.9293 - acc: 0.5248\n",
      "Epoch 7/10\n",
      "202/202 [==============================] - 0s 78us/sample - loss: 0.9190 - acc: 0.5248\n",
      "Epoch 8/10\n",
      "202/202 [==============================] - 0s 75us/sample - loss: 0.9094 - acc: 0.5297\n",
      "Epoch 9/10\n",
      "202/202 [==============================] - 0s 73us/sample - loss: 0.9003 - acc: 0.5347\n",
      "Epoch 10/10\n",
      "202/202 [==============================] - 0s 75us/sample - loss: 0.8913 - acc: 0.5347\n",
      "101/101 [==============================] - 0s 1ms/sample - loss: 0.8949 - acc: 0.5545\n",
      "202/202 [==============================] - 0s 61us/sample - loss: 0.8840 - acc: 0.5347\n",
      "Epoch 1/10\n",
      "202/202 [==============================] - 0s 850us/sample - loss: 0.8486 - acc: 0.5198\n",
      "Epoch 2/10\n",
      "202/202 [==============================] - 0s 31us/sample - loss: 0.8328 - acc: 0.5198\n",
      "Epoch 3/10\n",
      "202/202 [==============================] - 0s 30us/sample - loss: 0.8195 - acc: 0.5248\n",
      "Epoch 4/10\n",
      "202/202 [==============================] - 0s 31us/sample - loss: 0.8073 - acc: 0.5297\n",
      "Epoch 5/10\n",
      "202/202 [==============================] - 0s 95us/sample - loss: 0.7964 - acc: 0.5297\n",
      "Epoch 6/10\n",
      "202/202 [==============================] - 0s 32us/sample - loss: 0.7857 - acc: 0.5347\n",
      "Epoch 7/10\n",
      "202/202 [==============================] - 0s 71us/sample - loss: 0.7759 - acc: 0.5347\n",
      "Epoch 8/10\n",
      "202/202 [==============================] - 0s 61us/sample - loss: 0.7670 - acc: 0.5347\n",
      "Epoch 9/10\n",
      "202/202 [==============================] - 0s 54us/sample - loss: 0.7582 - acc: 0.5396\n",
      "Epoch 10/10\n",
      "202/202 [==============================] - 0s 100us/sample - loss: 0.7497 - acc: 0.5396\n",
      "101/101 [==============================] - 0s 1ms/sample - loss: 0.8788 - acc: 0.4653\n",
      "202/202 [==============================] - 0s 19us/sample - loss: 0.7433 - acc: 0.5446\n",
      "Epoch 1/10\n",
      "303/303 [==============================] - 0s 721us/sample - loss: 0.7237 - acc: 0.5248\n",
      "Epoch 2/10\n",
      "303/303 [==============================] - 0s 172us/sample - loss: 0.6499 - acc: 0.6304\n",
      "Epoch 3/10\n",
      "303/303 [==============================] - 0s 167us/sample - loss: 0.5948 - acc: 0.6799\n",
      "Epoch 4/10\n",
      "303/303 [==============================] - 0s 203us/sample - loss: 0.5508 - acc: 0.7162\n",
      "Epoch 5/10\n",
      "303/303 [==============================] - 0s 165us/sample - loss: 0.5160 - acc: 0.7525\n",
      "Epoch 6/10\n",
      "303/303 [==============================] - 0s 163us/sample - loss: 0.4894 - acc: 0.7756\n",
      "Epoch 7/10\n",
      "303/303 [==============================] - 0s 204us/sample - loss: 0.4666 - acc: 0.7756\n",
      "Epoch 8/10\n",
      "303/303 [==============================] - 0s 164us/sample - loss: 0.4479 - acc: 0.7954\n",
      "Epoch 9/10\n",
      "303/303 [==============================] - 0s 198us/sample - loss: 0.4332 - acc: 0.8119\n",
      "Epoch 10/10\n",
      "303/303 [==============================] - 0s 164us/sample - loss: 0.4210 - acc: 0.8185\n"
     ]
    }
   ],
   "source": [
    "# New model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, cv=3)\n",
    "grid_result = grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.8052805264790853 using {'batch_size': 10, 'epochs': 10}\n"
     ]
    }
   ],
   "source": [
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define search params\n",
    "param_grid = {'batch_size': [10],\n",
    "              'epochs': [5, 10, 20, 50, 100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "202/202 [==============================] - 0s 1ms/sample - loss: 0.6224 - acc: 0.7228\n",
      "Epoch 2/5\n",
      "202/202 [==============================] - 0s 280us/sample - loss: 0.5771 - acc: 0.7426\n",
      "Epoch 3/5\n",
      "202/202 [==============================] - 0s 222us/sample - loss: 0.5418 - acc: 0.7723\n",
      "Epoch 4/5\n",
      "202/202 [==============================] - 0s 223us/sample - loss: 0.5089 - acc: 0.8020\n",
      "Epoch 5/5\n",
      "202/202 [==============================] - 0s 212us/sample - loss: 0.4830 - acc: 0.8218\n",
      "101/101 [==============================] - 0s 1ms/sample - loss: 0.5409 - acc: 0.7327\n",
      "202/202 [==============================] - 0s 191us/sample - loss: 0.4693 - acc: 0.8317\n",
      "Epoch 1/5\n",
      "202/202 [==============================] - 0s 1ms/sample - loss: 0.8373 - acc: 0.3366\n",
      "Epoch 2/5\n",
      "202/202 [==============================] - 0s 424us/sample - loss: 0.7852 - acc: 0.3911\n",
      "Epoch 3/5\n",
      "202/202 [==============================] - 0s 549us/sample - loss: 0.7411 - acc: 0.4505\n",
      "Epoch 4/5\n",
      "202/202 [==============================] - 0s 469us/sample - loss: 0.7014 - acc: 0.5198\n",
      "Epoch 5/5\n",
      "202/202 [==============================] - 0s 544us/sample - loss: 0.6666 - acc: 0.5990\n",
      "101/101 [==============================] - 0s 1ms/sample - loss: 0.6609 - acc: 0.6139\n",
      "202/202 [==============================] - 0s 110us/sample - loss: 0.6496 - acc: 0.6337\n",
      "Epoch 1/5\n",
      "202/202 [==============================] - 0s 1ms/sample - loss: 0.6688 - acc: 0.5198\n",
      "Epoch 2/5\n",
      "202/202 [==============================] - 0s 249us/sample - loss: 0.6314 - acc: 0.5644\n",
      "Epoch 3/5\n",
      "202/202 [==============================] - 0s 339us/sample - loss: 0.6022 - acc: 0.6238\n",
      "Epoch 4/5\n",
      "202/202 [==============================] - 0s 272us/sample - loss: 0.5776 - acc: 0.6683\n",
      "Epoch 5/5\n",
      "202/202 [==============================] - 0s 250us/sample - loss: 0.5573 - acc: 0.7079\n",
      "101/101 [==============================] - 0s 1ms/sample - loss: 0.6328 - acc: 0.5644\n",
      "202/202 [==============================] - 0s 271us/sample - loss: 0.5460 - acc: 0.7327\n",
      "Epoch 1/10\n",
      "202/202 [==============================] - 0s 1ms/sample - loss: 0.8172 - acc: 0.5693\n",
      "Epoch 2/10\n",
      "202/202 [==============================] - 0s 709us/sample - loss: 0.7555 - acc: 0.5941\n",
      "Epoch 3/10\n",
      "202/202 [==============================] - 0s 664us/sample - loss: 0.7047 - acc: 0.6238\n",
      "Epoch 4/10\n",
      "202/202 [==============================] - 0s 747us/sample - loss: 0.6634 - acc: 0.6535\n",
      "Epoch 5/10\n",
      "202/202 [==============================] - 0s 722us/sample - loss: 0.6238 - acc: 0.6881\n",
      "Epoch 6/10\n",
      "202/202 [==============================] - 0s 905us/sample - loss: 0.5905 - acc: 0.7376\n",
      "Epoch 7/10\n",
      "202/202 [==============================] - 0s 662us/sample - loss: 0.5612 - acc: 0.7376\n",
      "Epoch 8/10\n",
      "202/202 [==============================] - 0s 401us/sample - loss: 0.5347 - acc: 0.7574\n",
      "Epoch 9/10\n",
      "202/202 [==============================] - 0s 1ms/sample - loss: 0.5125 - acc: 0.7624\n",
      "Epoch 10/10\n",
      "202/202 [==============================] - 0s 455us/sample - loss: 0.4922 - acc: 0.7970\n",
      "101/101 [==============================] - 0s 4ms/sample - loss: 0.5626 - acc: 0.7129\n",
      "202/202 [==============================] - 0s 469us/sample - loss: 0.4799 - acc: 0.7921\n",
      "Epoch 1/10\n",
      "202/202 [==============================] - 0s 1ms/sample - loss: 0.8443 - acc: 0.4703\n",
      "Epoch 2/10\n",
      "202/202 [==============================] - 0s 256us/sample - loss: 0.7746 - acc: 0.5446\n",
      "Epoch 3/10\n",
      "202/202 [==============================] - 0s 251us/sample - loss: 0.7214 - acc: 0.5693\n",
      "Epoch 4/10\n",
      "202/202 [==============================] - 0s 255us/sample - loss: 0.6729 - acc: 0.6139\n",
      "Epoch 5/10\n",
      "202/202 [==============================] - 0s 353us/sample - loss: 0.6321 - acc: 0.6436\n",
      "Epoch 6/10\n",
      "202/202 [==============================] - 0s 337us/sample - loss: 0.5964 - acc: 0.6733\n",
      "Epoch 7/10\n",
      "202/202 [==============================] - 0s 433us/sample - loss: 0.5650 - acc: 0.7327\n",
      "Epoch 8/10\n",
      "202/202 [==============================] - 0s 247us/sample - loss: 0.5383 - acc: 0.7624\n",
      "Epoch 9/10\n",
      "202/202 [==============================] - 0s 293us/sample - loss: 0.5154 - acc: 0.7871\n",
      "Epoch 10/10\n",
      "202/202 [==============================] - 0s 280us/sample - loss: 0.4957 - acc: 0.8020\n",
      "101/101 [==============================] - 0s 1ms/sample - loss: 0.5285 - acc: 0.7228\n",
      "202/202 [==============================] - 0s 210us/sample - loss: 0.4839 - acc: 0.8218\n",
      "Epoch 1/10\n",
      "202/202 [==============================] - 0s 2ms/sample - loss: 0.7657 - acc: 0.5000\n",
      "Epoch 2/10\n",
      "202/202 [==============================] - 0s 754us/sample - loss: 0.7256 - acc: 0.5495\n",
      "Epoch 3/10\n",
      "202/202 [==============================] - 0s 482us/sample - loss: 0.6896 - acc: 0.6188\n",
      "Epoch 4/10\n",
      "202/202 [==============================] - 0s 828us/sample - loss: 0.6582 - acc: 0.6337\n",
      "Epoch 5/10\n",
      "202/202 [==============================] - 0s 780us/sample - loss: 0.6293 - acc: 0.6832\n",
      "Epoch 6/10\n",
      "202/202 [==============================] - 0s 457us/sample - loss: 0.6003 - acc: 0.7178\n",
      "Epoch 7/10\n",
      "202/202 [==============================] - 0s 303us/sample - loss: 0.5755 - acc: 0.7376\n",
      "Epoch 8/10\n",
      "202/202 [==============================] - 0s 253us/sample - loss: 0.5511 - acc: 0.7574\n",
      "Epoch 9/10\n",
      "202/202 [==============================] - 0s 351us/sample - loss: 0.5285 - acc: 0.7673\n",
      "Epoch 10/10\n",
      "202/202 [==============================] - 0s 631us/sample - loss: 0.5075 - acc: 0.7921\n",
      "101/101 [==============================] - 0s 2ms/sample - loss: 0.5276 - acc: 0.7426\n",
      "202/202 [==============================] - 0s 301us/sample - loss: 0.4973 - acc: 0.7921\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 0s 1ms/sample - loss: 0.7541 - acc: 0.5545\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 262us/sample - loss: 0.7098 - acc: 0.6040\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 342us/sample - loss: 0.6723 - acc: 0.6485\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 235us/sample - loss: 0.6374 - acc: 0.6634\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 240us/sample - loss: 0.6056 - acc: 0.6931\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 228us/sample - loss: 0.5782 - acc: 0.7228\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 225us/sample - loss: 0.5515 - acc: 0.7277\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 235us/sample - loss: 0.5261 - acc: 0.7475\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 235us/sample - loss: 0.5043 - acc: 0.7871\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 235us/sample - loss: 0.4839 - acc: 0.7970\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 234us/sample - loss: 0.4656 - acc: 0.7970\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 223us/sample - loss: 0.4488 - acc: 0.8168\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 217us/sample - loss: 0.4343 - acc: 0.8168\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 195us/sample - loss: 0.4202 - acc: 0.8218\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 219us/sample - loss: 0.4086 - acc: 0.8317\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 331us/sample - loss: 0.3982 - acc: 0.8317\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 257us/sample - loss: 0.3885 - acc: 0.8317\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 169us/sample - loss: 0.3800 - acc: 0.8366\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 246us/sample - loss: 0.3715 - acc: 0.8366\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 272us/sample - loss: 0.3644 - acc: 0.8366\n",
      "101/101 [==============================] - 0s 2ms/sample - loss: 0.3949 - acc: 0.8317\n",
      "202/202 [==============================] - 0s 236us/sample - loss: 0.3597 - acc: 0.8366\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 0s 1ms/sample - loss: 0.9650 - acc: 0.3317\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 258us/sample - loss: 0.8845 - acc: 0.4059\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 342us/sample - loss: 0.8169 - acc: 0.4109\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 261us/sample - loss: 0.7588 - acc: 0.4752\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 287us/sample - loss: 0.7090 - acc: 0.5297\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202/202 [==============================] - 0s 211us/sample - loss: 0.6630 - acc: 0.5891\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 224us/sample - loss: 0.6232 - acc: 0.6535\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 158us/sample - loss: 0.5880 - acc: 0.6931\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 217us/sample - loss: 0.5583 - acc: 0.7426\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 182us/sample - loss: 0.5288 - acc: 0.7574\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 245us/sample - loss: 0.5034 - acc: 0.7822\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 161us/sample - loss: 0.4803 - acc: 0.7970\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 242us/sample - loss: 0.4618 - acc: 0.8119\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 189us/sample - loss: 0.4449 - acc: 0.8168\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 258us/sample - loss: 0.4310 - acc: 0.8267\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 161us/sample - loss: 0.4186 - acc: 0.8267\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 256us/sample - loss: 0.4070 - acc: 0.8416\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 206us/sample - loss: 0.3967 - acc: 0.8465\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 167us/sample - loss: 0.3880 - acc: 0.8465\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 232us/sample - loss: 0.3804 - acc: 0.8515\n",
      "101/101 [==============================] - 0s 2ms/sample - loss: 0.4088 - acc: 0.8218\n",
      "202/202 [==============================] - 0s 141us/sample - loss: 0.3749 - acc: 0.8564\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 0s 1ms/sample - loss: 0.7076 - acc: 0.6485\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 238us/sample - loss: 0.6496 - acc: 0.6881\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 201us/sample - loss: 0.6046 - acc: 0.7129\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 271us/sample - loss: 0.5686 - acc: 0.7376\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 202us/sample - loss: 0.5393 - acc: 0.7574\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 255us/sample - loss: 0.5128 - acc: 0.7673\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 193us/sample - loss: 0.4903 - acc: 0.7723\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 262us/sample - loss: 0.4725 - acc: 0.7673\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 210us/sample - loss: 0.4556 - acc: 0.7772\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 251us/sample - loss: 0.4424 - acc: 0.7970\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 191us/sample - loss: 0.4295 - acc: 0.8069\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 250us/sample - loss: 0.4193 - acc: 0.8119\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 251us/sample - loss: 0.4092 - acc: 0.8069\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 201us/sample - loss: 0.4007 - acc: 0.8119\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 285us/sample - loss: 0.3939 - acc: 0.8168\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 210us/sample - loss: 0.3876 - acc: 0.8267\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 192us/sample - loss: 0.3827 - acc: 0.8267\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 265us/sample - loss: 0.3768 - acc: 0.8317\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 192us/sample - loss: 0.3723 - acc: 0.8267\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 236us/sample - loss: 0.3675 - acc: 0.8366\n",
      "101/101 [==============================] - 0s 2ms/sample - loss: 0.4377 - acc: 0.8317\n",
      "202/202 [==============================] - 0s 164us/sample - loss: 0.3644 - acc: 0.8366\n",
      "Epoch 1/50\n",
      "202/202 [==============================] - 0s 1ms/sample - loss: 0.7406 - acc: 0.4901\n",
      "Epoch 2/50\n",
      "202/202 [==============================] - 0s 282us/sample - loss: 0.6919 - acc: 0.5941\n",
      "Epoch 3/50\n",
      "202/202 [==============================] - 0s 217us/sample - loss: 0.6532 - acc: 0.6436\n",
      "Epoch 4/50\n",
      "202/202 [==============================] - 0s 218us/sample - loss: 0.6171 - acc: 0.7079\n",
      "Epoch 5/50\n",
      "202/202 [==============================] - 0s 199us/sample - loss: 0.5852 - acc: 0.7327\n",
      "Epoch 6/50\n",
      "202/202 [==============================] - 0s 300us/sample - loss: 0.5552 - acc: 0.7525\n",
      "Epoch 7/50\n",
      "202/202 [==============================] - 0s 225us/sample - loss: 0.5272 - acc: 0.7624\n",
      "Epoch 8/50\n",
      "202/202 [==============================] - 0s 230us/sample - loss: 0.5014 - acc: 0.7772\n",
      "Epoch 9/50\n",
      "202/202 [==============================] - 0s 211us/sample - loss: 0.4798 - acc: 0.7921\n",
      "Epoch 10/50\n",
      "202/202 [==============================] - 0s 171us/sample - loss: 0.4588 - acc: 0.8069\n",
      "Epoch 11/50\n",
      "202/202 [==============================] - 0s 269us/sample - loss: 0.4409 - acc: 0.8218\n",
      "Epoch 12/50\n",
      "202/202 [==============================] - 0s 216us/sample - loss: 0.4253 - acc: 0.8168\n",
      "Epoch 13/50\n",
      "202/202 [==============================] - 0s 162us/sample - loss: 0.4113 - acc: 0.8267\n",
      "Epoch 14/50\n",
      "202/202 [==============================] - 0s 221us/sample - loss: 0.4001 - acc: 0.8267\n",
      "Epoch 15/50\n",
      "202/202 [==============================] - 0s 206us/sample - loss: 0.3899 - acc: 0.8317\n",
      "Epoch 16/50\n",
      "202/202 [==============================] - 0s 252us/sample - loss: 0.3804 - acc: 0.8317\n",
      "Epoch 17/50\n",
      "202/202 [==============================] - 0s 223us/sample - loss: 0.3724 - acc: 0.8416\n",
      "Epoch 18/50\n",
      "202/202 [==============================] - 0s 312us/sample - loss: 0.3641 - acc: 0.8465\n",
      "Epoch 19/50\n",
      "202/202 [==============================] - 0s 414us/sample - loss: 0.3580 - acc: 0.8465\n",
      "Epoch 20/50\n",
      "202/202 [==============================] - 0s 442us/sample - loss: 0.3520 - acc: 0.8515\n",
      "Epoch 21/50\n",
      "202/202 [==============================] - 0s 321us/sample - loss: 0.3463 - acc: 0.8614\n",
      "Epoch 22/50\n",
      "202/202 [==============================] - 0s 322us/sample - loss: 0.3420 - acc: 0.8614\n",
      "Epoch 23/50\n",
      "202/202 [==============================] - 0s 346us/sample - loss: 0.3368 - acc: 0.8614\n",
      "Epoch 24/50\n",
      "202/202 [==============================] - 0s 335us/sample - loss: 0.3331 - acc: 0.8663\n",
      "Epoch 25/50\n",
      "202/202 [==============================] - 0s 337us/sample - loss: 0.3297 - acc: 0.8614\n",
      "Epoch 26/50\n",
      "202/202 [==============================] - 0s 376us/sample - loss: 0.3261 - acc: 0.8614\n",
      "Epoch 27/50\n",
      "202/202 [==============================] - 0s 311us/sample - loss: 0.3230 - acc: 0.8614\n",
      "Epoch 28/50\n",
      "202/202 [==============================] - 0s 288us/sample - loss: 0.3199 - acc: 0.8663\n",
      "Epoch 29/50\n",
      "202/202 [==============================] - 0s 207us/sample - loss: 0.3169 - acc: 0.8663\n",
      "Epoch 30/50\n",
      "202/202 [==============================] - 0s 172us/sample - loss: 0.3144 - acc: 0.8762\n",
      "Epoch 31/50\n",
      "202/202 [==============================] - 0s 262us/sample - loss: 0.3121 - acc: 0.8762\n",
      "Epoch 32/50\n",
      "202/202 [==============================] - 0s 207us/sample - loss: 0.3099 - acc: 0.8762\n",
      "Epoch 33/50\n",
      "202/202 [==============================] - 0s 259us/sample - loss: 0.3076 - acc: 0.8762\n",
      "Epoch 34/50\n",
      "202/202 [==============================] - 0s 198us/sample - loss: 0.3054 - acc: 0.8812\n",
      "Epoch 35/50\n",
      "202/202 [==============================] - 0s 319us/sample - loss: 0.3030 - acc: 0.8861\n",
      "Epoch 36/50\n",
      "202/202 [==============================] - 0s 447us/sample - loss: 0.3007 - acc: 0.8861\n",
      "Epoch 37/50\n",
      "202/202 [==============================] - 0s 410us/sample - loss: 0.2987 - acc: 0.8861\n",
      "Epoch 38/50\n",
      "202/202 [==============================] - 0s 348us/sample - loss: 0.2967 - acc: 0.8861\n",
      "Epoch 39/50\n",
      "202/202 [==============================] - 0s 242us/sample - loss: 0.2952 - acc: 0.8861\n",
      "Epoch 40/50\n",
      "202/202 [==============================] - 0s 200us/sample - loss: 0.2930 - acc: 0.8861\n",
      "Epoch 41/50\n",
      "202/202 [==============================] - 0s 194us/sample - loss: 0.2918 - acc: 0.8861\n",
      "Epoch 42/50\n",
      "202/202 [==============================] - 0s 245us/sample - loss: 0.2899 - acc: 0.8861\n",
      "Epoch 43/50\n",
      "202/202 [==============================] - 0s 237us/sample - loss: 0.2882 - acc: 0.8861\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202/202 [==============================] - 0s 245us/sample - loss: 0.2865 - acc: 0.8812\n",
      "Epoch 45/50\n",
      "202/202 [==============================] - 0s 252us/sample - loss: 0.2855 - acc: 0.8812\n",
      "Epoch 46/50\n",
      "202/202 [==============================] - 0s 192us/sample - loss: 0.2837 - acc: 0.8812\n",
      "Epoch 47/50\n",
      "202/202 [==============================] - 0s 253us/sample - loss: 0.2822 - acc: 0.8812\n",
      "Epoch 48/50\n",
      "202/202 [==============================] - 0s 185us/sample - loss: 0.2810 - acc: 0.8812\n",
      "Epoch 49/50\n",
      "202/202 [==============================] - 0s 255us/sample - loss: 0.2794 - acc: 0.8812\n",
      "Epoch 50/50\n",
      "202/202 [==============================] - 0s 244us/sample - loss: 0.2781 - acc: 0.8861\n",
      "101/101 [==============================] - 0s 2ms/sample - loss: 0.4504 - acc: 0.8317\n",
      "202/202 [==============================] - 0s 118us/sample - loss: 0.2761 - acc: 0.8861\n",
      "Epoch 1/50\n",
      "202/202 [==============================] - 0s 1ms/sample - loss: 0.9217 - acc: 0.4901\n",
      "Epoch 2/50\n",
      "202/202 [==============================] - 0s 244us/sample - loss: 0.8410 - acc: 0.5446\n",
      "Epoch 3/50\n",
      "202/202 [==============================] - 0s 203us/sample - loss: 0.7761 - acc: 0.5792\n",
      "Epoch 4/50\n",
      "202/202 [==============================] - 0s 304us/sample - loss: 0.7196 - acc: 0.5891\n",
      "Epoch 5/50\n",
      "202/202 [==============================] - 0s 399us/sample - loss: 0.6716 - acc: 0.6139\n",
      "Epoch 6/50\n",
      "202/202 [==============================] - 0s 394us/sample - loss: 0.6296 - acc: 0.6634\n",
      "Epoch 7/50\n",
      "202/202 [==============================] - 0s 347us/sample - loss: 0.5918 - acc: 0.6782\n",
      "Epoch 8/50\n",
      "202/202 [==============================] - 0s 331us/sample - loss: 0.5619 - acc: 0.7376\n",
      "Epoch 9/50\n",
      "202/202 [==============================] - 0s 330us/sample - loss: 0.5356 - acc: 0.7574\n",
      "Epoch 10/50\n",
      "202/202 [==============================] - 0s 313us/sample - loss: 0.5135 - acc: 0.7723\n",
      "Epoch 11/50\n",
      "202/202 [==============================] - 0s 204us/sample - loss: 0.4939 - acc: 0.7921\n",
      "Epoch 12/50\n",
      "202/202 [==============================] - 0s 256us/sample - loss: 0.4766 - acc: 0.8020\n",
      "Epoch 13/50\n",
      "202/202 [==============================] - 0s 210us/sample - loss: 0.4634 - acc: 0.8119\n",
      "Epoch 14/50\n",
      "202/202 [==============================] - 0s 204us/sample - loss: 0.4508 - acc: 0.8168\n",
      "Epoch 15/50\n",
      "202/202 [==============================] - 0s 271us/sample - loss: 0.4397 - acc: 0.8168\n",
      "Epoch 16/50\n",
      "202/202 [==============================] - 0s 210us/sample - loss: 0.4306 - acc: 0.8218\n",
      "Epoch 17/50\n",
      "202/202 [==============================] - 0s 191us/sample - loss: 0.4223 - acc: 0.8168\n",
      "Epoch 18/50\n",
      "202/202 [==============================] - 0s 266us/sample - loss: 0.4144 - acc: 0.8218\n",
      "Epoch 19/50\n",
      "202/202 [==============================] - 0s 176us/sample - loss: 0.4068 - acc: 0.8267\n",
      "Epoch 20/50\n",
      "202/202 [==============================] - 0s 213us/sample - loss: 0.4005 - acc: 0.8267\n",
      "Epoch 21/50\n",
      "202/202 [==============================] - 0s 194us/sample - loss: 0.3941 - acc: 0.8366\n",
      "Epoch 22/50\n",
      "202/202 [==============================] - 0s 230us/sample - loss: 0.3880 - acc: 0.8416\n",
      "Epoch 23/50\n",
      "202/202 [==============================] - 0s 221us/sample - loss: 0.3825 - acc: 0.8366\n",
      "Epoch 24/50\n",
      "202/202 [==============================] - 0s 245us/sample - loss: 0.3784 - acc: 0.8366\n",
      "Epoch 25/50\n",
      "202/202 [==============================] - 0s 242us/sample - loss: 0.3740 - acc: 0.8366\n",
      "Epoch 26/50\n",
      "202/202 [==============================] - 0s 163us/sample - loss: 0.3702 - acc: 0.8366\n",
      "Epoch 27/50\n",
      "202/202 [==============================] - 0s 215us/sample - loss: 0.3668 - acc: 0.8366\n",
      "Epoch 28/50\n",
      "202/202 [==============================] - 0s 184us/sample - loss: 0.3629 - acc: 0.8416\n",
      "Epoch 29/50\n",
      "202/202 [==============================] - 0s 251us/sample - loss: 0.3600 - acc: 0.8416\n",
      "Epoch 30/50\n",
      "202/202 [==============================] - 0s 241us/sample - loss: 0.3562 - acc: 0.8416\n",
      "Epoch 31/50\n",
      "202/202 [==============================] - 0s 260us/sample - loss: 0.3539 - acc: 0.8366\n",
      "Epoch 32/50\n",
      "202/202 [==============================] - 0s 212us/sample - loss: 0.3507 - acc: 0.8366\n",
      "Epoch 33/50\n",
      "202/202 [==============================] - 0s 260us/sample - loss: 0.3483 - acc: 0.8416\n",
      "Epoch 34/50\n",
      "202/202 [==============================] - 0s 230us/sample - loss: 0.3457 - acc: 0.8465\n",
      "Epoch 35/50\n",
      "202/202 [==============================] - 0s 209us/sample - loss: 0.3433 - acc: 0.8465\n",
      "Epoch 36/50\n",
      "202/202 [==============================] - 0s 250us/sample - loss: 0.3406 - acc: 0.8515\n",
      "Epoch 37/50\n",
      "202/202 [==============================] - 0s 180us/sample - loss: 0.3385 - acc: 0.8515\n",
      "Epoch 38/50\n",
      "202/202 [==============================] - 0s 246us/sample - loss: 0.3362 - acc: 0.8515\n",
      "Epoch 39/50\n",
      "202/202 [==============================] - 0s 235us/sample - loss: 0.3331 - acc: 0.8515\n",
      "Epoch 40/50\n",
      "202/202 [==============================] - 0s 280us/sample - loss: 0.3317 - acc: 0.8465\n",
      "Epoch 41/50\n",
      "202/202 [==============================] - 0s 211us/sample - loss: 0.3296 - acc: 0.8465\n",
      "Epoch 42/50\n",
      "202/202 [==============================] - 0s 199us/sample - loss: 0.3276 - acc: 0.8465\n",
      "Epoch 43/50\n",
      "202/202 [==============================] - 0s 252us/sample - loss: 0.3259 - acc: 0.8465\n",
      "Epoch 44/50\n",
      "202/202 [==============================] - 0s 192us/sample - loss: 0.3235 - acc: 0.8515\n",
      "Epoch 45/50\n",
      "202/202 [==============================] - 0s 255us/sample - loss: 0.3221 - acc: 0.8515\n",
      "Epoch 46/50\n",
      "202/202 [==============================] - 0s 244us/sample - loss: 0.3200 - acc: 0.8515\n",
      "Epoch 47/50\n",
      "202/202 [==============================] - 0s 190us/sample - loss: 0.3178 - acc: 0.8515\n",
      "Epoch 48/50\n",
      "202/202 [==============================] - 0s 241us/sample - loss: 0.3160 - acc: 0.8515\n",
      "Epoch 49/50\n",
      "202/202 [==============================] - 0s 258us/sample - loss: 0.3145 - acc: 0.8515\n",
      "Epoch 50/50\n",
      "202/202 [==============================] - 0s 243us/sample - loss: 0.3128 - acc: 0.8515\n",
      "101/101 [==============================] - 0s 2ms/sample - loss: 0.3991 - acc: 0.7921\n",
      "202/202 [==============================] - 0s 215us/sample - loss: 0.3107 - acc: 0.8515\n",
      "Epoch 1/50\n",
      "202/202 [==============================] - 0s 1ms/sample - loss: 0.6160 - acc: 0.6634\n",
      "Epoch 2/50\n",
      "202/202 [==============================] - 0s 204us/sample - loss: 0.5769 - acc: 0.7228\n",
      "Epoch 3/50\n",
      "202/202 [==============================] - 0s 196us/sample - loss: 0.5452 - acc: 0.7475\n",
      "Epoch 4/50\n",
      "202/202 [==============================] - 0s 243us/sample - loss: 0.5193 - acc: 0.7723\n",
      "Epoch 5/50\n",
      "202/202 [==============================] - 0s 230us/sample - loss: 0.4959 - acc: 0.7822\n",
      "Epoch 6/50\n",
      "202/202 [==============================] - 0s 164us/sample - loss: 0.4773 - acc: 0.7871\n",
      "Epoch 7/50\n",
      "202/202 [==============================] - 0s 232us/sample - loss: 0.4607 - acc: 0.8119\n",
      "Epoch 8/50\n",
      "202/202 [==============================] - 0s 186us/sample - loss: 0.4459 - acc: 0.8218\n",
      "Epoch 9/50\n",
      "202/202 [==============================] - 0s 253us/sample - loss: 0.4340 - acc: 0.8267\n",
      "Epoch 10/50\n",
      "202/202 [==============================] - 0s 235us/sample - loss: 0.4231 - acc: 0.8267\n",
      "Epoch 11/50\n",
      "202/202 [==============================] - 0s 162us/sample - loss: 0.4133 - acc: 0.8218\n",
      "Epoch 12/50\n",
      "202/202 [==============================] - 0s 223us/sample - loss: 0.4050 - acc: 0.8366\n",
      "Epoch 13/50\n",
      "202/202 [==============================] - 0s 175us/sample - loss: 0.3972 - acc: 0.8366\n",
      "Epoch 14/50\n",
      "202/202 [==============================] - 0s 236us/sample - loss: 0.3909 - acc: 0.8416\n",
      "Epoch 15/50\n",
      "202/202 [==============================] - 0s 226us/sample - loss: 0.3838 - acc: 0.8416\n",
      "Epoch 16/50\n",
      "202/202 [==============================] - 0s 226us/sample - loss: 0.3780 - acc: 0.8465\n",
      "Epoch 17/50\n",
      "202/202 [==============================] - 0s 228us/sample - loss: 0.3729 - acc: 0.8515\n",
      "Epoch 18/50\n",
      "202/202 [==============================] - 0s 254us/sample - loss: 0.3672 - acc: 0.8564\n",
      "Epoch 19/50\n",
      "202/202 [==============================] - 0s 158us/sample - loss: 0.3628 - acc: 0.8564\n",
      "Epoch 20/50\n",
      "202/202 [==============================] - 0s 224us/sample - loss: 0.3582 - acc: 0.8614\n",
      "Epoch 21/50\n",
      "202/202 [==============================] - 0s 167us/sample - loss: 0.3546 - acc: 0.8614\n",
      "Epoch 22/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202/202 [==============================] - 0s 245us/sample - loss: 0.3501 - acc: 0.8713\n",
      "Epoch 23/50\n",
      "202/202 [==============================] - 0s 237us/sample - loss: 0.3466 - acc: 0.8713\n",
      "Epoch 24/50\n",
      "202/202 [==============================] - 0s 230us/sample - loss: 0.3434 - acc: 0.8713\n",
      "Epoch 25/50\n",
      "202/202 [==============================] - 0s 242us/sample - loss: 0.3399 - acc: 0.8713\n",
      "Epoch 26/50\n",
      "202/202 [==============================] - 0s 162us/sample - loss: 0.3371 - acc: 0.8713\n",
      "Epoch 27/50\n",
      "202/202 [==============================] - 0s 224us/sample - loss: 0.3339 - acc: 0.8713\n",
      "Epoch 28/50\n",
      "202/202 [==============================] - 0s 188us/sample - loss: 0.3310 - acc: 0.8713\n",
      "Epoch 29/50\n",
      "202/202 [==============================] - 0s 250us/sample - loss: 0.3279 - acc: 0.8762\n",
      "Epoch 30/50\n",
      "202/202 [==============================] - 0s 157us/sample - loss: 0.3255 - acc: 0.8762\n",
      "Epoch 31/50\n",
      "202/202 [==============================] - 0s 232us/sample - loss: 0.3238 - acc: 0.8812\n",
      "Epoch 32/50\n",
      "202/202 [==============================] - 0s 183us/sample - loss: 0.3203 - acc: 0.8861\n",
      "Epoch 33/50\n",
      "202/202 [==============================] - 0s 252us/sample - loss: 0.3177 - acc: 0.8861\n",
      "Epoch 34/50\n",
      "202/202 [==============================] - 0s 161us/sample - loss: 0.3150 - acc: 0.8861\n",
      "Epoch 35/50\n",
      "202/202 [==============================] - 0s 216us/sample - loss: 0.3127 - acc: 0.8861\n",
      "Epoch 36/50\n",
      "202/202 [==============================] - 0s 193us/sample - loss: 0.3102 - acc: 0.8861\n",
      "Epoch 37/50\n",
      "202/202 [==============================] - 0s 225us/sample - loss: 0.3083 - acc: 0.8861\n",
      "Epoch 38/50\n",
      "202/202 [==============================] - 0s 250us/sample - loss: 0.3065 - acc: 0.8911\n",
      "Epoch 39/50\n",
      "202/202 [==============================] - 0s 237us/sample - loss: 0.3038 - acc: 0.8911\n",
      "Epoch 40/50\n",
      "202/202 [==============================] - 0s 195us/sample - loss: 0.3018 - acc: 0.8911\n",
      "Epoch 41/50\n",
      "202/202 [==============================] - 0s 224us/sample - loss: 0.2997 - acc: 0.8960\n",
      "Epoch 42/50\n",
      "202/202 [==============================] - 0s 225us/sample - loss: 0.2983 - acc: 0.8960\n",
      "Epoch 43/50\n",
      "202/202 [==============================] - 0s 233us/sample - loss: 0.2962 - acc: 0.9010\n",
      "Epoch 44/50\n",
      "202/202 [==============================] - 0s 236us/sample - loss: 0.2946 - acc: 0.9010\n",
      "Epoch 45/50\n",
      "202/202 [==============================] - 0s 236us/sample - loss: 0.2923 - acc: 0.9010\n",
      "Epoch 46/50\n",
      "202/202 [==============================] - 0s 240us/sample - loss: 0.2905 - acc: 0.9010\n",
      "Epoch 47/50\n",
      "202/202 [==============================] - 0s 232us/sample - loss: 0.2885 - acc: 0.9059\n",
      "Epoch 48/50\n",
      "202/202 [==============================] - 0s 298us/sample - loss: 0.2866 - acc: 0.9059\n",
      "Epoch 49/50\n",
      "202/202 [==============================] - 0s 166us/sample - loss: 0.2852 - acc: 0.9010\n",
      "Epoch 50/50\n",
      "202/202 [==============================] - 0s 359us/sample - loss: 0.2834 - acc: 0.9059\n",
      "101/101 [==============================] - 0s 2ms/sample - loss: 0.4160 - acc: 0.8218\n",
      "202/202 [==============================] - 0s 176us/sample - loss: 0.2815 - acc: 0.9059\n",
      "Epoch 1/100\n",
      "202/202 [==============================] - 0s 2ms/sample - loss: 0.7965 - acc: 0.4802\n",
      "Epoch 2/100\n",
      "202/202 [==============================] - 0s 366us/sample - loss: 0.7299 - acc: 0.5050\n",
      "Epoch 3/100\n",
      "202/202 [==============================] - 0s 342us/sample - loss: 0.6759 - acc: 0.5644\n",
      "Epoch 4/100\n",
      "202/202 [==============================] - 0s 302us/sample - loss: 0.6275 - acc: 0.6386\n",
      "Epoch 5/100\n",
      "202/202 [==============================] - 0s 581us/sample - loss: 0.5929 - acc: 0.6584\n",
      "Epoch 6/100\n",
      "202/202 [==============================] - 0s 363us/sample - loss: 0.5611 - acc: 0.7079\n",
      "Epoch 7/100\n",
      "202/202 [==============================] - 0s 594us/sample - loss: 0.5342 - acc: 0.7574\n",
      "Epoch 8/100\n",
      "202/202 [==============================] - 0s 365us/sample - loss: 0.5099 - acc: 0.7921\n",
      "Epoch 9/100\n",
      "202/202 [==============================] - 0s 290us/sample - loss: 0.4881 - acc: 0.7970\n",
      "Epoch 10/100\n",
      "202/202 [==============================] - 0s 462us/sample - loss: 0.4687 - acc: 0.8119\n",
      "Epoch 11/100\n",
      "202/202 [==============================] - 0s 442us/sample - loss: 0.4511 - acc: 0.8267\n",
      "Epoch 12/100\n",
      "202/202 [==============================] - 0s 515us/sample - loss: 0.4365 - acc: 0.8317\n",
      "Epoch 13/100\n",
      "202/202 [==============================] - 0s 246us/sample - loss: 0.4240 - acc: 0.8416\n",
      "Epoch 14/100\n",
      "202/202 [==============================] - 0s 438us/sample - loss: 0.4117 - acc: 0.8465\n",
      "Epoch 15/100\n",
      "202/202 [==============================] - 0s 281us/sample - loss: 0.4013 - acc: 0.8515\n",
      "Epoch 16/100\n",
      "202/202 [==============================] - 0s 251us/sample - loss: 0.3915 - acc: 0.8515\n",
      "Epoch 17/100\n",
      "202/202 [==============================] - 0s 207us/sample - loss: 0.3827 - acc: 0.8515\n",
      "Epoch 18/100\n",
      "202/202 [==============================] - 0s 191us/sample - loss: 0.3744 - acc: 0.8614\n",
      "Epoch 19/100\n",
      "202/202 [==============================] - 0s 231us/sample - loss: 0.3672 - acc: 0.8614\n",
      "Epoch 20/100\n",
      "202/202 [==============================] - 0s 200us/sample - loss: 0.3602 - acc: 0.8663\n",
      "Epoch 21/100\n",
      "202/202 [==============================] - 0s 297us/sample - loss: 0.3543 - acc: 0.8663\n",
      "Epoch 22/100\n",
      "202/202 [==============================] - 0s 309us/sample - loss: 0.3477 - acc: 0.8713\n",
      "Epoch 23/100\n",
      "202/202 [==============================] - 0s 263us/sample - loss: 0.3427 - acc: 0.8713\n",
      "Epoch 24/100\n",
      "202/202 [==============================] - 0s 227us/sample - loss: 0.3376 - acc: 0.8762\n",
      "Epoch 25/100\n",
      "202/202 [==============================] - 0s 240us/sample - loss: 0.3329 - acc: 0.8762\n",
      "Epoch 26/100\n",
      "202/202 [==============================] - 0s 239us/sample - loss: 0.3287 - acc: 0.8762\n",
      "Epoch 27/100\n",
      "202/202 [==============================] - 0s 232us/sample - loss: 0.3244 - acc: 0.8762\n",
      "Epoch 28/100\n",
      "202/202 [==============================] - 0s 239us/sample - loss: 0.3209 - acc: 0.8762\n",
      "Epoch 29/100\n",
      "202/202 [==============================] - 0s 243us/sample - loss: 0.3171 - acc: 0.8762\n",
      "Epoch 30/100\n",
      "202/202 [==============================] - 0s 163us/sample - loss: 0.3136 - acc: 0.8762\n",
      "Epoch 31/100\n",
      "202/202 [==============================] - 0s 215us/sample - loss: 0.3099 - acc: 0.8812\n",
      "Epoch 32/100\n",
      "202/202 [==============================] - 0s 176us/sample - loss: 0.3068 - acc: 0.8812\n",
      "Epoch 33/100\n",
      "202/202 [==============================] - 0s 233us/sample - loss: 0.3035 - acc: 0.8861\n",
      "Epoch 34/100\n",
      "202/202 [==============================] - 0s 226us/sample - loss: 0.3009 - acc: 0.8861\n",
      "Epoch 35/100\n",
      "202/202 [==============================] - 0s 202us/sample - loss: 0.2980 - acc: 0.8861\n",
      "Epoch 36/100\n",
      "202/202 [==============================] - 0s 179us/sample - loss: 0.2953 - acc: 0.8911\n",
      "Epoch 37/100\n",
      "202/202 [==============================] - 0s 202us/sample - loss: 0.2929 - acc: 0.8960\n",
      "Epoch 38/100\n",
      "202/202 [==============================] - 0s 200us/sample - loss: 0.2910 - acc: 0.8960\n",
      "Epoch 39/100\n",
      "202/202 [==============================] - 0s 244us/sample - loss: 0.2883 - acc: 0.8960\n",
      "Epoch 40/100\n",
      "202/202 [==============================] - 0s 166us/sample - loss: 0.2862 - acc: 0.8960\n",
      "Epoch 41/100\n",
      "202/202 [==============================] - 0s 272us/sample - loss: 0.2842 - acc: 0.8960\n",
      "Epoch 42/100\n",
      "202/202 [==============================] - 0s 215us/sample - loss: 0.2817 - acc: 0.8960\n",
      "Epoch 43/100\n",
      "202/202 [==============================] - 0s 220us/sample - loss: 0.2800 - acc: 0.8960\n",
      "Epoch 44/100\n",
      "202/202 [==============================] - 0s 210us/sample - loss: 0.2781 - acc: 0.8960\n",
      "Epoch 45/100\n",
      "202/202 [==============================] - 0s 247us/sample - loss: 0.2760 - acc: 0.8911\n",
      "Epoch 46/100\n",
      "202/202 [==============================] - 0s 198us/sample - loss: 0.2744 - acc: 0.8911\n",
      "Epoch 47/100\n",
      "202/202 [==============================] - 0s 248us/sample - loss: 0.2723 - acc: 0.8911\n",
      "Epoch 48/100\n",
      "202/202 [==============================] - 0s 156us/sample - loss: 0.2711 - acc: 0.9010\n",
      "Epoch 49/100\n",
      "202/202 [==============================] - 0s 215us/sample - loss: 0.2683 - acc: 0.9010\n",
      "Epoch 50/100\n",
      "202/202 [==============================] - 0s 206us/sample - loss: 0.2674 - acc: 0.9059\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202/202 [==============================] - 0s 264us/sample - loss: 0.2660 - acc: 0.9010\n",
      "Epoch 52/100\n",
      "202/202 [==============================] - 0s 277us/sample - loss: 0.2639 - acc: 0.9059\n",
      "Epoch 53/100\n",
      "202/202 [==============================] - 0s 375us/sample - loss: 0.2621 - acc: 0.9010\n",
      "Epoch 54/100\n",
      "202/202 [==============================] - 0s 315us/sample - loss: 0.2607 - acc: 0.9010\n",
      "Epoch 55/100\n",
      "202/202 [==============================] - 0s 325us/sample - loss: 0.2590 - acc: 0.9010\n",
      "Epoch 56/100\n",
      "202/202 [==============================] - 0s 326us/sample - loss: 0.2576 - acc: 0.9010\n",
      "Epoch 57/100\n",
      "202/202 [==============================] - 0s 363us/sample - loss: 0.2561 - acc: 0.9059\n",
      "Epoch 58/100\n",
      "202/202 [==============================] - 0s 318us/sample - loss: 0.2556 - acc: 0.9010\n",
      "Epoch 59/100\n",
      "202/202 [==============================] - 0s 250us/sample - loss: 0.2542 - acc: 0.8960\n",
      "Epoch 60/100\n",
      "202/202 [==============================] - 0s 202us/sample - loss: 0.2526 - acc: 0.8960\n",
      "Epoch 61/100\n",
      "202/202 [==============================] - 0s 262us/sample - loss: 0.2512 - acc: 0.8960\n",
      "Epoch 62/100\n",
      "202/202 [==============================] - 0s 187us/sample - loss: 0.2500 - acc: 0.9010\n",
      "Epoch 63/100\n",
      "202/202 [==============================] - 0s 257us/sample - loss: 0.2487 - acc: 0.9010\n",
      "Epoch 64/100\n",
      "202/202 [==============================] - 0s 205us/sample - loss: 0.2472 - acc: 0.9010\n",
      "Epoch 65/100\n",
      "202/202 [==============================] - 0s 252us/sample - loss: 0.2461 - acc: 0.9010\n",
      "Epoch 66/100\n",
      "202/202 [==============================] - 0s 189us/sample - loss: 0.2447 - acc: 0.9059\n",
      "Epoch 67/100\n",
      "202/202 [==============================] - 0s 267us/sample - loss: 0.2433 - acc: 0.9010\n",
      "Epoch 68/100\n",
      "202/202 [==============================] - 0s 161us/sample - loss: 0.2422 - acc: 0.9059\n",
      "Epoch 69/100\n",
      "202/202 [==============================] - 0s 251us/sample - loss: 0.2411 - acc: 0.9059\n",
      "Epoch 70/100\n",
      "202/202 [==============================] - 0s 239us/sample - loss: 0.2397 - acc: 0.9109\n",
      "Epoch 71/100\n",
      "202/202 [==============================] - 0s 242us/sample - loss: 0.2389 - acc: 0.9109\n",
      "Epoch 72/100\n",
      "202/202 [==============================] - 0s 170us/sample - loss: 0.2378 - acc: 0.9158\n",
      "Epoch 73/100\n",
      "202/202 [==============================] - 0s 270us/sample - loss: 0.2368 - acc: 0.9158\n",
      "Epoch 74/100\n",
      "202/202 [==============================] - 0s 164us/sample - loss: 0.2357 - acc: 0.9158\n",
      "Epoch 75/100\n",
      "202/202 [==============================] - 0s 218us/sample - loss: 0.2340 - acc: 0.9158\n",
      "Epoch 76/100\n",
      "202/202 [==============================] - 0s 182us/sample - loss: 0.2331 - acc: 0.9158\n",
      "Epoch 77/100\n",
      "202/202 [==============================] - 0s 258us/sample - loss: 0.2322 - acc: 0.9158\n",
      "Epoch 78/100\n",
      "202/202 [==============================] - 0s 167us/sample - loss: 0.2312 - acc: 0.9158\n",
      "Epoch 79/100\n",
      "202/202 [==============================] - 0s 265us/sample - loss: 0.2302 - acc: 0.9158\n",
      "Epoch 80/100\n",
      "202/202 [==============================] - 0s 250us/sample - loss: 0.2288 - acc: 0.9208\n",
      "Epoch 81/100\n",
      "202/202 [==============================] - 0s 174us/sample - loss: 0.2277 - acc: 0.9208\n",
      "Epoch 82/100\n",
      "202/202 [==============================] - 0s 214us/sample - loss: 0.2266 - acc: 0.9257\n",
      "Epoch 83/100\n",
      "202/202 [==============================] - 0s 188us/sample - loss: 0.2260 - acc: 0.9208\n",
      "Epoch 84/100\n",
      "202/202 [==============================] - 0s 251us/sample - loss: 0.2250 - acc: 0.9208\n",
      "Epoch 85/100\n",
      "202/202 [==============================] - 0s 165us/sample - loss: 0.2237 - acc: 0.9257\n",
      "Epoch 86/100\n",
      "202/202 [==============================] - 0s 245us/sample - loss: 0.2228 - acc: 0.9257\n",
      "Epoch 87/100\n",
      "202/202 [==============================] - 0s 236us/sample - loss: 0.2219 - acc: 0.9257\n",
      "Epoch 88/100\n",
      "202/202 [==============================] - 0s 265us/sample - loss: 0.2209 - acc: 0.9257\n",
      "Epoch 89/100\n",
      "202/202 [==============================] - 0s 188us/sample - loss: 0.2201 - acc: 0.9257\n",
      "Epoch 90/100\n",
      "202/202 [==============================] - 0s 248us/sample - loss: 0.2191 - acc: 0.9257\n",
      "Epoch 91/100\n",
      "202/202 [==============================] - 0s 166us/sample - loss: 0.2184 - acc: 0.9257\n",
      "Epoch 92/100\n",
      "202/202 [==============================] - 0s 274us/sample - loss: 0.2169 - acc: 0.9257\n",
      "Epoch 93/100\n",
      "202/202 [==============================] - 0s 170us/sample - loss: 0.2159 - acc: 0.9307\n",
      "Epoch 94/100\n",
      "202/202 [==============================] - 0s 259us/sample - loss: 0.2157 - acc: 0.9307\n",
      "Epoch 95/100\n",
      "202/202 [==============================] - 0s 252us/sample - loss: 0.2145 - acc: 0.9257\n",
      "Epoch 96/100\n",
      "202/202 [==============================] - 0s 183us/sample - loss: 0.2136 - acc: 0.9257\n",
      "Epoch 97/100\n",
      "202/202 [==============================] - 0s 261us/sample - loss: 0.2122 - acc: 0.9257\n",
      "Epoch 98/100\n",
      "202/202 [==============================] - 0s 245us/sample - loss: 0.2114 - acc: 0.9257\n",
      "Epoch 99/100\n",
      "202/202 [==============================] - 0s 175us/sample - loss: 0.2106 - acc: 0.9307\n",
      "Epoch 100/100\n",
      "202/202 [==============================] - 0s 223us/sample - loss: 0.2093 - acc: 0.9307\n",
      "101/101 [==============================] - 0s 2ms/sample - loss: 0.5389 - acc: 0.7723\n",
      "202/202 [==============================] - 0s 160us/sample - loss: 0.2076 - acc: 0.9307\n",
      "Epoch 1/100\n",
      "202/202 [==============================] - 0s 1ms/sample - loss: 0.9055 - acc: 0.4802\n",
      "Epoch 2/100\n",
      "202/202 [==============================] - 0s 222us/sample - loss: 0.8277 - acc: 0.4950\n",
      "Epoch 3/100\n",
      "202/202 [==============================] - 0s 190us/sample - loss: 0.7618 - acc: 0.5396\n",
      "Epoch 4/100\n",
      "202/202 [==============================] - 0s 278us/sample - loss: 0.7084 - acc: 0.5941\n",
      "Epoch 5/100\n",
      "202/202 [==============================] - 0s 200us/sample - loss: 0.6603 - acc: 0.6436\n",
      "Epoch 6/100\n",
      "202/202 [==============================] - 0s 257us/sample - loss: 0.6179 - acc: 0.6931\n",
      "Epoch 7/100\n",
      "202/202 [==============================] - 0s 193us/sample - loss: 0.5828 - acc: 0.7228\n",
      "Epoch 8/100\n",
      "202/202 [==============================] - 0s 269us/sample - loss: 0.5548 - acc: 0.7327\n",
      "Epoch 9/100\n",
      "202/202 [==============================] - 0s 200us/sample - loss: 0.5309 - acc: 0.7426\n",
      "Epoch 10/100\n",
      "202/202 [==============================] - 0s 272us/sample - loss: 0.5103 - acc: 0.7426\n",
      "Epoch 11/100\n",
      "202/202 [==============================] - 0s 224us/sample - loss: 0.4916 - acc: 0.7673\n",
      "Epoch 12/100\n",
      "202/202 [==============================] - 0s 193us/sample - loss: 0.4749 - acc: 0.7822\n",
      "Epoch 13/100\n",
      "202/202 [==============================] - 0s 263us/sample - loss: 0.4598 - acc: 0.7822\n",
      "Epoch 14/100\n",
      "202/202 [==============================] - 0s 202us/sample - loss: 0.4473 - acc: 0.7921\n",
      "Epoch 15/100\n",
      "202/202 [==============================] - 0s 253us/sample - loss: 0.4348 - acc: 0.7970\n",
      "Epoch 16/100\n",
      "202/202 [==============================] - 0s 200us/sample - loss: 0.4241 - acc: 0.8119\n",
      "Epoch 17/100\n",
      "202/202 [==============================] - 0s 256us/sample - loss: 0.4155 - acc: 0.8267\n",
      "Epoch 18/100\n",
      "202/202 [==============================] - 0s 201us/sample - loss: 0.4077 - acc: 0.8267\n",
      "Epoch 19/100\n",
      "202/202 [==============================] - 0s 265us/sample - loss: 0.4010 - acc: 0.8267\n",
      "Epoch 20/100\n",
      "202/202 [==============================] - 0s 214us/sample - loss: 0.3941 - acc: 0.8317\n",
      "Epoch 21/100\n",
      "202/202 [==============================] - 0s 189us/sample - loss: 0.3877 - acc: 0.8267\n",
      "Epoch 22/100\n",
      "202/202 [==============================] - 0s 292us/sample - loss: 0.3826 - acc: 0.8267\n",
      "Epoch 23/100\n",
      "202/202 [==============================] - 0s 207us/sample - loss: 0.3774 - acc: 0.8267\n",
      "Epoch 24/100\n",
      "202/202 [==============================] - 0s 251us/sample - loss: 0.3722 - acc: 0.8267\n",
      "Epoch 25/100\n",
      "202/202 [==============================] - 0s 203us/sample - loss: 0.3680 - acc: 0.8267\n",
      "Epoch 26/100\n",
      "202/202 [==============================] - 0s 255us/sample - loss: 0.3638 - acc: 0.8267\n",
      "Epoch 27/100\n",
      "202/202 [==============================] - 0s 218us/sample - loss: 0.3605 - acc: 0.8267\n",
      "Epoch 28/100\n",
      "202/202 [==============================] - 0s 259us/sample - loss: 0.3570 - acc: 0.8317\n",
      "Epoch 29/100\n",
      "202/202 [==============================] - 0s 204us/sample - loss: 0.3537 - acc: 0.8416\n",
      "Epoch 30/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202/202 [==============================] - 0s 264us/sample - loss: 0.3510 - acc: 0.8465\n",
      "Epoch 31/100\n",
      "202/202 [==============================] - 0s 223us/sample - loss: 0.3475 - acc: 0.8416\n",
      "Epoch 32/100\n",
      "202/202 [==============================] - 0s 202us/sample - loss: 0.3454 - acc: 0.8416\n",
      "Epoch 33/100\n",
      "202/202 [==============================] - 0s 253us/sample - loss: 0.3423 - acc: 0.8416\n",
      "Epoch 34/100\n",
      "202/202 [==============================] - 0s 199us/sample - loss: 0.3400 - acc: 0.8515\n",
      "Epoch 35/100\n",
      "202/202 [==============================] - 0s 256us/sample - loss: 0.3370 - acc: 0.8515\n",
      "Epoch 36/100\n",
      "202/202 [==============================] - 0s 201us/sample - loss: 0.3348 - acc: 0.8515\n",
      "Epoch 37/100\n",
      "202/202 [==============================] - 0s 260us/sample - loss: 0.3327 - acc: 0.8515\n",
      "Epoch 38/100\n",
      "202/202 [==============================] - 0s 200us/sample - loss: 0.3296 - acc: 0.8564\n",
      "Epoch 39/100\n",
      "202/202 [==============================] - 0s 264us/sample - loss: 0.3275 - acc: 0.8564\n",
      "Epoch 40/100\n",
      "202/202 [==============================] - 0s 213us/sample - loss: 0.3258 - acc: 0.8515\n",
      "Epoch 41/100\n",
      "202/202 [==============================] - 0s 210us/sample - loss: 0.3236 - acc: 0.8515\n",
      "Epoch 42/100\n",
      "202/202 [==============================] - 0s 266us/sample - loss: 0.3217 - acc: 0.8515\n",
      "Epoch 43/100\n",
      "202/202 [==============================] - 0s 217us/sample - loss: 0.3201 - acc: 0.8515\n",
      "Epoch 44/100\n",
      "202/202 [==============================] - 0s 263us/sample - loss: 0.3184 - acc: 0.8515\n",
      "Epoch 45/100\n",
      "202/202 [==============================] - 0s 207us/sample - loss: 0.3170 - acc: 0.8515\n",
      "Epoch 46/100\n",
      "202/202 [==============================] - 0s 198us/sample - loss: 0.3148 - acc: 0.8515\n",
      "Epoch 47/100\n",
      "202/202 [==============================] - 0s 259us/sample - loss: 0.3146 - acc: 0.8614\n",
      "Epoch 48/100\n",
      "202/202 [==============================] - 0s 249us/sample - loss: 0.3127 - acc: 0.8614\n",
      "Epoch 49/100\n",
      "202/202 [==============================] - 0s 195us/sample - loss: 0.3107 - acc: 0.8614\n",
      "Epoch 50/100\n",
      "202/202 [==============================] - 0s 269us/sample - loss: 0.3096 - acc: 0.8663\n",
      "Epoch 51/100\n",
      "202/202 [==============================] - 0s 203us/sample - loss: 0.3092 - acc: 0.8614\n",
      "Epoch 52/100\n",
      "202/202 [==============================] - 0s 260us/sample - loss: 0.3073 - acc: 0.8663\n",
      "Epoch 53/100\n",
      "202/202 [==============================] - 0s 192us/sample - loss: 0.3057 - acc: 0.8663\n",
      "Epoch 54/100\n",
      "202/202 [==============================] - 0s 260us/sample - loss: 0.3048 - acc: 0.8663\n",
      "Epoch 55/100\n",
      "202/202 [==============================] - 0s 198us/sample - loss: 0.3044 - acc: 0.8663\n",
      "Epoch 56/100\n",
      "202/202 [==============================] - 0s 255us/sample - loss: 0.3027 - acc: 0.8713\n",
      "Epoch 57/100\n",
      "202/202 [==============================] - 0s 191us/sample - loss: 0.3014 - acc: 0.8713\n",
      "Epoch 58/100\n",
      "202/202 [==============================] - 0s 259us/sample - loss: 0.3002 - acc: 0.8713\n",
      "Epoch 59/100\n",
      "202/202 [==============================] - 0s 199us/sample - loss: 0.2989 - acc: 0.8663\n",
      "Epoch 60/100\n",
      "202/202 [==============================] - 0s 273us/sample - loss: 0.2981 - acc: 0.8713\n",
      "Epoch 61/100\n",
      "202/202 [==============================] - 0s 206us/sample - loss: 0.2964 - acc: 0.8762\n",
      "Epoch 62/100\n",
      "202/202 [==============================] - 0s 266us/sample - loss: 0.2959 - acc: 0.8713\n",
      "Epoch 63/100\n",
      "202/202 [==============================] - 0s 197us/sample - loss: 0.2946 - acc: 0.8663\n",
      "Epoch 64/100\n",
      "202/202 [==============================] - 0s 265us/sample - loss: 0.2936 - acc: 0.8713\n",
      "Epoch 65/100\n",
      "202/202 [==============================] - 0s 241us/sample - loss: 0.2925 - acc: 0.8713\n",
      "Epoch 66/100\n",
      "202/202 [==============================] - 0s 226us/sample - loss: 0.2913 - acc: 0.8713\n",
      "Epoch 67/100\n",
      "202/202 [==============================] - 0s 234us/sample - loss: 0.2901 - acc: 0.8713\n",
      "Epoch 68/100\n",
      "202/202 [==============================] - 0s 247us/sample - loss: 0.2890 - acc: 0.8713\n",
      "Epoch 69/100\n",
      "202/202 [==============================] - 0s 263us/sample - loss: 0.2878 - acc: 0.8762\n",
      "Epoch 70/100\n",
      "202/202 [==============================] - 0s 347us/sample - loss: 0.2871 - acc: 0.8762\n",
      "Epoch 71/100\n",
      "202/202 [==============================] - 0s 637us/sample - loss: 0.2865 - acc: 0.8762\n",
      "Epoch 72/100\n",
      "202/202 [==============================] - 0s 506us/sample - loss: 0.2856 - acc: 0.8762\n",
      "Epoch 73/100\n",
      "202/202 [==============================] - 0s 1ms/sample - loss: 0.2842 - acc: 0.8812\n",
      "Epoch 74/100\n",
      "202/202 [==============================] - 0s 665us/sample - loss: 0.2830 - acc: 0.8812\n",
      "Epoch 75/100\n",
      "202/202 [==============================] - 0s 540us/sample - loss: 0.2824 - acc: 0.8812\n",
      "Epoch 76/100\n",
      "202/202 [==============================] - 0s 615us/sample - loss: 0.2812 - acc: 0.8812\n",
      "Epoch 77/100\n",
      "202/202 [==============================] - 0s 264us/sample - loss: 0.2810 - acc: 0.8812\n",
      "Epoch 78/100\n",
      "202/202 [==============================] - 0s 396us/sample - loss: 0.2792 - acc: 0.8762\n",
      "Epoch 79/100\n",
      "202/202 [==============================] - 0s 262us/sample - loss: 0.2784 - acc: 0.8762\n",
      "Epoch 80/100\n",
      "202/202 [==============================] - 0s 278us/sample - loss: 0.2775 - acc: 0.8812\n",
      "Epoch 81/100\n",
      "202/202 [==============================] - 0s 356us/sample - loss: 0.2765 - acc: 0.8762\n",
      "Epoch 82/100\n",
      "202/202 [==============================] - 0s 286us/sample - loss: 0.2753 - acc: 0.8762\n",
      "Epoch 83/100\n",
      "202/202 [==============================] - 0s 241us/sample - loss: 0.2756 - acc: 0.8812\n",
      "Epoch 84/100\n",
      "202/202 [==============================] - 0s 278us/sample - loss: 0.2745 - acc: 0.8762\n",
      "Epoch 85/100\n",
      "202/202 [==============================] - 0s 329us/sample - loss: 0.2727 - acc: 0.8762\n",
      "Epoch 86/100\n",
      "202/202 [==============================] - 0s 337us/sample - loss: 0.2720 - acc: 0.8861\n",
      "Epoch 87/100\n",
      "202/202 [==============================] - 0s 307us/sample - loss: 0.2710 - acc: 0.8861\n",
      "Epoch 88/100\n",
      "202/202 [==============================] - 0s 299us/sample - loss: 0.2702 - acc: 0.8861\n",
      "Epoch 89/100\n",
      "202/202 [==============================] - 0s 249us/sample - loss: 0.2702 - acc: 0.8861\n",
      "Epoch 90/100\n",
      "202/202 [==============================] - 0s 210us/sample - loss: 0.2678 - acc: 0.8861\n",
      "Epoch 91/100\n",
      "202/202 [==============================] - 0s 207us/sample - loss: 0.2673 - acc: 0.8911\n",
      "Epoch 92/100\n",
      "202/202 [==============================] - 0s 330us/sample - loss: 0.2663 - acc: 0.8861\n",
      "Epoch 93/100\n",
      "202/202 [==============================] - 0s 398us/sample - loss: 0.2656 - acc: 0.8861\n",
      "Epoch 94/100\n",
      "202/202 [==============================] - 0s 463us/sample - loss: 0.2643 - acc: 0.8911\n",
      "Epoch 95/100\n",
      "202/202 [==============================] - 0s 361us/sample - loss: 0.2632 - acc: 0.8911\n",
      "Epoch 96/100\n",
      "202/202 [==============================] - 0s 322us/sample - loss: 0.2625 - acc: 0.8911\n",
      "Epoch 97/100\n",
      "202/202 [==============================] - 0s 359us/sample - loss: 0.2612 - acc: 0.8861\n",
      "Epoch 98/100\n",
      "202/202 [==============================] - 0s 344us/sample - loss: 0.2599 - acc: 0.8911\n",
      "Epoch 99/100\n",
      "202/202 [==============================] - 0s 268us/sample - loss: 0.2596 - acc: 0.8861\n",
      "Epoch 100/100\n",
      "202/202 [==============================] - 0s 282us/sample - loss: 0.2584 - acc: 0.8861\n",
      "101/101 [==============================] - 0s 2ms/sample - loss: 0.3719 - acc: 0.8317\n",
      "202/202 [==============================] - 0s 180us/sample - loss: 0.2564 - acc: 0.8911\n",
      "Epoch 1/100\n",
      "202/202 [==============================] - 0s 1ms/sample - loss: 0.9171 - acc: 0.3366\n",
      "Epoch 2/100\n",
      "202/202 [==============================] - 0s 214us/sample - loss: 0.8445 - acc: 0.3812\n",
      "Epoch 3/100\n",
      "202/202 [==============================] - 0s 208us/sample - loss: 0.7849 - acc: 0.4703\n",
      "Epoch 4/100\n",
      "202/202 [==============================] - 0s 279us/sample - loss: 0.7358 - acc: 0.5495\n",
      "Epoch 5/100\n",
      "202/202 [==============================] - 0s 237us/sample - loss: 0.6940 - acc: 0.5891\n",
      "Epoch 6/100\n",
      "202/202 [==============================] - 0s 229us/sample - loss: 0.6562 - acc: 0.6584\n",
      "Epoch 7/100\n",
      "202/202 [==============================] - 0s 217us/sample - loss: 0.6245 - acc: 0.6733\n",
      "Epoch 8/100\n",
      "202/202 [==============================] - 0s 202us/sample - loss: 0.5946 - acc: 0.6980\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202/202 [==============================] - 0s 281us/sample - loss: 0.5682 - acc: 0.7129\n",
      "Epoch 10/100\n",
      "202/202 [==============================] - 0s 221us/sample - loss: 0.5458 - acc: 0.7376\n",
      "Epoch 11/100\n",
      "202/202 [==============================] - 0s 201us/sample - loss: 0.5252 - acc: 0.7525\n",
      "Epoch 12/100\n",
      "202/202 [==============================] - 0s 259us/sample - loss: 0.5056 - acc: 0.7772\n",
      "Epoch 13/100\n",
      "202/202 [==============================] - 0s 200us/sample - loss: 0.4900 - acc: 0.7871\n",
      "Epoch 14/100\n",
      "202/202 [==============================] - 0s 240us/sample - loss: 0.4752 - acc: 0.7970\n",
      "Epoch 15/100\n",
      "202/202 [==============================] - 0s 251us/sample - loss: 0.4623 - acc: 0.8069\n",
      "Epoch 16/100\n",
      "202/202 [==============================] - 0s 190us/sample - loss: 0.4509 - acc: 0.8119\n",
      "Epoch 17/100\n",
      "202/202 [==============================] - 0s 252us/sample - loss: 0.4410 - acc: 0.8069\n",
      "Epoch 18/100\n",
      "202/202 [==============================] - 0s 215us/sample - loss: 0.4313 - acc: 0.8119\n",
      "Epoch 19/100\n",
      "202/202 [==============================] - 0s 375us/sample - loss: 0.4237 - acc: 0.8218\n",
      "Epoch 20/100\n",
      "202/202 [==============================] - 0s 378us/sample - loss: 0.4156 - acc: 0.8267\n",
      "Epoch 21/100\n",
      "202/202 [==============================] - 0s 328us/sample - loss: 0.4087 - acc: 0.8317\n",
      "Epoch 22/100\n",
      "202/202 [==============================] - 0s 277us/sample - loss: 0.4020 - acc: 0.8366\n",
      "Epoch 23/100\n",
      "202/202 [==============================] - 0s 263us/sample - loss: 0.3959 - acc: 0.8317\n",
      "Epoch 24/100\n",
      "202/202 [==============================] - 0s 207us/sample - loss: 0.3904 - acc: 0.8416\n",
      "Epoch 25/100\n",
      "202/202 [==============================] - 0s 194us/sample - loss: 0.3857 - acc: 0.8416\n",
      "Epoch 26/100\n",
      "202/202 [==============================] - 0s 226us/sample - loss: 0.3813 - acc: 0.8465\n",
      "Epoch 27/100\n",
      "202/202 [==============================] - 0s 242us/sample - loss: 0.3764 - acc: 0.8515\n",
      "Epoch 28/100\n",
      "202/202 [==============================] - 0s 276us/sample - loss: 0.3721 - acc: 0.8465\n",
      "Epoch 29/100\n",
      "202/202 [==============================] - 0s 225us/sample - loss: 0.3673 - acc: 0.8416\n",
      "Epoch 30/100\n",
      "202/202 [==============================] - 0s 222us/sample - loss: 0.3632 - acc: 0.8416\n",
      "Epoch 31/100\n",
      "202/202 [==============================] - 0s 219us/sample - loss: 0.3593 - acc: 0.8416\n",
      "Epoch 32/100\n",
      "202/202 [==============================] - 0s 219us/sample - loss: 0.3557 - acc: 0.8416\n",
      "Epoch 33/100\n",
      "202/202 [==============================] - 0s 262us/sample - loss: 0.3523 - acc: 0.8416\n",
      "Epoch 34/100\n",
      "202/202 [==============================] - 0s 216us/sample - loss: 0.3487 - acc: 0.8465\n",
      "Epoch 35/100\n",
      "202/202 [==============================] - 0s 351us/sample - loss: 0.3462 - acc: 0.8465\n",
      "Epoch 36/100\n",
      "202/202 [==============================] - 0s 257us/sample - loss: 0.3429 - acc: 0.8465\n",
      "Epoch 37/100\n",
      "202/202 [==============================] - 0s 297us/sample - loss: 0.3404 - acc: 0.8465\n",
      "Epoch 38/100\n",
      "202/202 [==============================] - 0s 275us/sample - loss: 0.3378 - acc: 0.8465\n",
      "Epoch 39/100\n",
      "202/202 [==============================] - 0s 305us/sample - loss: 0.3348 - acc: 0.8515\n",
      "Epoch 40/100\n",
      "202/202 [==============================] - 0s 244us/sample - loss: 0.3324 - acc: 0.8564\n",
      "Epoch 41/100\n",
      "202/202 [==============================] - 0s 381us/sample - loss: 0.3300 - acc: 0.8564\n",
      "Epoch 42/100\n",
      "202/202 [==============================] - 0s 493us/sample - loss: 0.3279 - acc: 0.8564\n",
      "Epoch 43/100\n",
      "202/202 [==============================] - 0s 332us/sample - loss: 0.3253 - acc: 0.8564\n",
      "Epoch 44/100\n",
      "202/202 [==============================] - 0s 299us/sample - loss: 0.3232 - acc: 0.8564\n",
      "Epoch 45/100\n",
      "202/202 [==============================] - 0s 243us/sample - loss: 0.3213 - acc: 0.8564\n",
      "Epoch 46/100\n",
      "202/202 [==============================] - 0s 191us/sample - loss: 0.3195 - acc: 0.8564\n",
      "Epoch 47/100\n",
      "202/202 [==============================] - 0s 337us/sample - loss: 0.3169 - acc: 0.8564\n",
      "Epoch 48/100\n",
      "202/202 [==============================] - 0s 353us/sample - loss: 0.3155 - acc: 0.8564\n",
      "Epoch 49/100\n",
      "202/202 [==============================] - 0s 315us/sample - loss: 0.3134 - acc: 0.8614\n",
      "Epoch 50/100\n",
      "202/202 [==============================] - 0s 258us/sample - loss: 0.3116 - acc: 0.8614\n",
      "Epoch 51/100\n",
      "202/202 [==============================] - 0s 228us/sample - loss: 0.3094 - acc: 0.8614\n",
      "Epoch 52/100\n",
      "202/202 [==============================] - 0s 206us/sample - loss: 0.3079 - acc: 0.8663\n",
      "Epoch 53/100\n",
      "202/202 [==============================] - 0s 268us/sample - loss: 0.3061 - acc: 0.8713\n",
      "Epoch 54/100\n",
      "202/202 [==============================] - 0s 222us/sample - loss: 0.3048 - acc: 0.8713\n",
      "Epoch 55/100\n",
      "202/202 [==============================] - 0s 224us/sample - loss: 0.3034 - acc: 0.8713\n",
      "Epoch 56/100\n",
      "202/202 [==============================] - 0s 222us/sample - loss: 0.3019 - acc: 0.8663\n",
      "Epoch 57/100\n",
      "202/202 [==============================] - 0s 267us/sample - loss: 0.3009 - acc: 0.8713\n",
      "Epoch 58/100\n",
      "202/202 [==============================] - 0s 224us/sample - loss: 0.2994 - acc: 0.8762\n",
      "Epoch 59/100\n",
      "202/202 [==============================] - 0s 221us/sample - loss: 0.2978 - acc: 0.8812\n",
      "Epoch 60/100\n",
      "202/202 [==============================] - 0s 211us/sample - loss: 0.2967 - acc: 0.8812\n",
      "Epoch 61/100\n",
      "202/202 [==============================] - 0s 210us/sample - loss: 0.2958 - acc: 0.8812\n",
      "Epoch 62/100\n",
      "202/202 [==============================] - 0s 284us/sample - loss: 0.2943 - acc: 0.8861\n",
      "Epoch 63/100\n",
      "202/202 [==============================] - 0s 242us/sample - loss: 0.2931 - acc: 0.8861\n",
      "Epoch 64/100\n",
      "202/202 [==============================] - 0s 218us/sample - loss: 0.2916 - acc: 0.8861\n",
      "Epoch 65/100\n",
      "202/202 [==============================] - 0s 210us/sample - loss: 0.2910 - acc: 0.8812\n",
      "Epoch 66/100\n",
      "202/202 [==============================] - 0s 226us/sample - loss: 0.2895 - acc: 0.8812\n",
      "Epoch 67/100\n",
      "202/202 [==============================] - 0s 293us/sample - loss: 0.2883 - acc: 0.8812\n",
      "Epoch 68/100\n",
      "202/202 [==============================] - 0s 250us/sample - loss: 0.2874 - acc: 0.8812\n",
      "Epoch 69/100\n",
      "202/202 [==============================] - 0s 204us/sample - loss: 0.2861 - acc: 0.8861\n",
      "Epoch 70/100\n",
      "202/202 [==============================] - 0s 285us/sample - loss: 0.2853 - acc: 0.8911\n",
      "Epoch 71/100\n",
      "202/202 [==============================] - 0s 221us/sample - loss: 0.2842 - acc: 0.8911\n",
      "Epoch 72/100\n",
      "202/202 [==============================] - 0s 216us/sample - loss: 0.2829 - acc: 0.8911\n",
      "Epoch 73/100\n",
      "202/202 [==============================] - 0s 186us/sample - loss: 0.2817 - acc: 0.8911\n",
      "Epoch 74/100\n",
      "202/202 [==============================] - 0s 235us/sample - loss: 0.2810 - acc: 0.8911\n",
      "Epoch 75/100\n",
      "202/202 [==============================] - 0s 241us/sample - loss: 0.2800 - acc: 0.8911\n",
      "Epoch 76/100\n",
      "202/202 [==============================] - 0s 187us/sample - loss: 0.2789 - acc: 0.8960\n",
      "Epoch 77/100\n",
      "202/202 [==============================] - 0s 281us/sample - loss: 0.2778 - acc: 0.8960\n",
      "Epoch 78/100\n",
      "202/202 [==============================] - 0s 203us/sample - loss: 0.2765 - acc: 0.8960\n",
      "Epoch 79/100\n",
      "202/202 [==============================] - 0s 286us/sample - loss: 0.2755 - acc: 0.8960\n",
      "Epoch 80/100\n",
      "202/202 [==============================] - 0s 224us/sample - loss: 0.2745 - acc: 0.8960\n",
      "Epoch 81/100\n",
      "202/202 [==============================] - 0s 214us/sample - loss: 0.2734 - acc: 0.8911\n",
      "Epoch 82/100\n",
      "202/202 [==============================] - 0s 210us/sample - loss: 0.2725 - acc: 0.8960\n",
      "Epoch 83/100\n",
      "202/202 [==============================] - 0s 291us/sample - loss: 0.2717 - acc: 0.8911\n",
      "Epoch 84/100\n",
      "202/202 [==============================] - 0s 260us/sample - loss: 0.2705 - acc: 0.8911\n",
      "Epoch 85/100\n",
      "202/202 [==============================] - 0s 261us/sample - loss: 0.2695 - acc: 0.8911\n",
      "Epoch 86/100\n",
      "202/202 [==============================] - 0s 220us/sample - loss: 0.2687 - acc: 0.8911\n",
      "Epoch 87/100\n",
      "202/202 [==============================] - 0s 214us/sample - loss: 0.2674 - acc: 0.8911\n",
      "Epoch 88/100\n",
      "202/202 [==============================] - 0s 251us/sample - loss: 0.2664 - acc: 0.8911\n",
      "Epoch 89/100\n",
      "202/202 [==============================] - 0s 528us/sample - loss: 0.2653 - acc: 0.8960\n",
      "Epoch 90/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202/202 [==============================] - 0s 565us/sample - loss: 0.2645 - acc: 0.8911\n",
      "Epoch 91/100\n",
      "202/202 [==============================] - 0s 234us/sample - loss: 0.2638 - acc: 0.8960\n",
      "Epoch 92/100\n",
      "202/202 [==============================] - 0s 211us/sample - loss: 0.2626 - acc: 0.8911\n",
      "Epoch 93/100\n",
      "202/202 [==============================] - 0s 229us/sample - loss: 0.2622 - acc: 0.8911\n",
      "Epoch 94/100\n",
      "202/202 [==============================] - 0s 169us/sample - loss: 0.2608 - acc: 0.8911\n",
      "Epoch 95/100\n",
      "202/202 [==============================] - 0s 307us/sample - loss: 0.2601 - acc: 0.8911\n",
      "Epoch 96/100\n",
      "202/202 [==============================] - 0s 270us/sample - loss: 0.2595 - acc: 0.8960\n",
      "Epoch 97/100\n",
      "202/202 [==============================] - 0s 331us/sample - loss: 0.2587 - acc: 0.8960\n",
      "Epoch 98/100\n",
      "202/202 [==============================] - 0s 311us/sample - loss: 0.2571 - acc: 0.8960\n",
      "Epoch 99/100\n",
      "202/202 [==============================] - 0s 355us/sample - loss: 0.2566 - acc: 0.8960\n",
      "Epoch 100/100\n",
      "202/202 [==============================] - 0s 326us/sample - loss: 0.2553 - acc: 0.8960\n",
      "101/101 [==============================] - 0s 4ms/sample - loss: 0.4007 - acc: 0.8020\n",
      "202/202 [==============================] - 0s 152us/sample - loss: 0.2538 - acc: 0.8960\n",
      "Epoch 1/20\n",
      "303/303 [==============================] - 0s 1ms/sample - loss: 0.8599 - acc: 0.5182\n",
      "Epoch 2/20\n",
      "303/303 [==============================] - 0s 443us/sample - loss: 0.7540 - acc: 0.5710\n",
      "Epoch 3/20\n",
      "303/303 [==============================] - 0s 329us/sample - loss: 0.6694 - acc: 0.6304\n",
      "Epoch 4/20\n",
      "303/303 [==============================] - 0s 203us/sample - loss: 0.6057 - acc: 0.6568\n",
      "Epoch 5/20\n",
      "303/303 [==============================] - 0s 219us/sample - loss: 0.5564 - acc: 0.7129\n",
      "Epoch 6/20\n",
      "303/303 [==============================] - 0s 263us/sample - loss: 0.5187 - acc: 0.7459\n",
      "Epoch 7/20\n",
      "303/303 [==============================] - 0s 213us/sample - loss: 0.4879 - acc: 0.7657\n",
      "Epoch 8/20\n",
      "303/303 [==============================] - 0s 216us/sample - loss: 0.4632 - acc: 0.7921\n",
      "Epoch 9/20\n",
      "303/303 [==============================] - 0s 277us/sample - loss: 0.4445 - acc: 0.8020\n",
      "Epoch 10/20\n",
      "303/303 [==============================] - 0s 235us/sample - loss: 0.4281 - acc: 0.8119\n",
      "Epoch 11/20\n",
      "303/303 [==============================] - 0s 217us/sample - loss: 0.4157 - acc: 0.8218\n",
      "Epoch 12/20\n",
      "303/303 [==============================] - 0s 222us/sample - loss: 0.4045 - acc: 0.8251\n",
      "Epoch 13/20\n",
      "303/303 [==============================] - 0s 252us/sample - loss: 0.3945 - acc: 0.8350\n",
      "Epoch 14/20\n",
      "303/303 [==============================] - 0s 223us/sample - loss: 0.3859 - acc: 0.8383\n",
      "Epoch 15/20\n",
      "303/303 [==============================] - 0s 251us/sample - loss: 0.3781 - acc: 0.8416\n",
      "Epoch 16/20\n",
      "303/303 [==============================] - 0s 247us/sample - loss: 0.3716 - acc: 0.8482\n",
      "Epoch 17/20\n",
      "303/303 [==============================] - 0s 349us/sample - loss: 0.3663 - acc: 0.8515\n",
      "Epoch 18/20\n",
      "303/303 [==============================] - 0s 193us/sample - loss: 0.3601 - acc: 0.8548\n",
      "Epoch 19/20\n",
      "303/303 [==============================] - 0s 302us/sample - loss: 0.3555 - acc: 0.8515\n",
      "Epoch 20/20\n",
      "303/303 [==============================] - 0s 233us/sample - loss: 0.3506 - acc: 0.8482\n"
     ]
    }
   ],
   "source": [
    "# New model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, cv=3)\n",
    "grid_result = grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.8283828298250834 using {'batch_size': 10, 'epochs': 20}\n"
     ]
    }
   ],
   "source": [
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "242/242 [==============================] - 0s 1ms/sample - loss: 0.6473 - acc: 0.6446\n",
      "Epoch 2/20\n",
      "242/242 [==============================] - 0s 388us/sample - loss: 0.5906 - acc: 0.7066\n",
      "Epoch 3/20\n",
      "242/242 [==============================] - 0s 320us/sample - loss: 0.5466 - acc: 0.7438\n",
      "Epoch 4/20\n",
      "242/242 [==============================] - 0s 462us/sample - loss: 0.5088 - acc: 0.7727\n",
      "Epoch 5/20\n",
      "242/242 [==============================] - 0s 286us/sample - loss: 0.4775 - acc: 0.7893\n",
      "Epoch 6/20\n",
      "242/242 [==============================] - 0s 421us/sample - loss: 0.4519 - acc: 0.8099\n",
      "Epoch 7/20\n",
      "242/242 [==============================] - 0s 258us/sample - loss: 0.4298 - acc: 0.8099\n",
      "Epoch 8/20\n",
      "242/242 [==============================] - 0s 330us/sample - loss: 0.4108 - acc: 0.8223\n",
      "Epoch 9/20\n",
      "242/242 [==============================] - 0s 412us/sample - loss: 0.3946 - acc: 0.8388\n",
      "Epoch 10/20\n",
      "242/242 [==============================] - 0s 1ms/sample - loss: 0.3815 - acc: 0.8388\n",
      "Epoch 11/20\n",
      "242/242 [==============================] - 0s 570us/sample - loss: 0.3714 - acc: 0.8471\n",
      "Epoch 12/20\n",
      "242/242 [==============================] - 0s 723us/sample - loss: 0.3617 - acc: 0.8430\n",
      "Epoch 13/20\n",
      "242/242 [==============================] - 0s 739us/sample - loss: 0.3534 - acc: 0.8512\n",
      "Epoch 14/20\n",
      "242/242 [==============================] - 0s 777us/sample - loss: 0.3456 - acc: 0.8595\n",
      "Epoch 15/20\n",
      "242/242 [==============================] - 0s 709us/sample - loss: 0.3394 - acc: 0.8636\n",
      "Epoch 16/20\n",
      "242/242 [==============================] - 0s 897us/sample - loss: 0.3338 - acc: 0.8636\n",
      "Epoch 17/20\n",
      "242/242 [==============================] - 0s 2ms/sample - loss: 0.3282 - acc: 0.8678\n",
      "Epoch 18/20\n",
      "242/242 [==============================] - 0s 873us/sample - loss: 0.3235 - acc: 0.8678\n",
      "Epoch 19/20\n",
      "242/242 [==============================] - 0s 547us/sample - loss: 0.3196 - acc: 0.8678\n",
      "Epoch 20/20\n",
      "242/242 [==============================] - 0s 518us/sample - loss: 0.3155 - acc: 0.8719\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a3ea61f60>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New model\n",
    "model = create_model()\n",
    "model.fit(X_train, y_train, batch_size=10, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 0s 3ms/sample - loss: 0.5341 - acc: 0.7377\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5341193627138607, 0.73770493]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Accuracy: 73.77%**"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "0.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
