{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "41TS0Sa0rDNx"
   },
   "source": [
    "# Model Validation with Keras - Boston Housing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZyirqVC_sC64",
    "outputId": "530427b7-8aec-42b9-95f4-3f28b5430394"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import pandas as pd\n",
    "import numpy\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0s0o2pqBs88q"
   },
   "source": [
    "### Load Boston Housing Data\n",
    "\n",
    "Even though we can import this dataset from Keras, Keras already has it divided up into test and train datasets for us I don't want that for this demonstration, so I'm going to import it from Scikit-Learn instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "cMMt4MtAsVx0",
    "outputId": "1eace308-7957-4ffc-fcfe-700b53060682"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX  ...    TAX  PTRATIO       B  LSTAT  MEDV\n",
       "0  0.00632  18.0   2.31   0.0  0.538  ...  296.0     15.3  396.90   4.98  24.0\n",
       "1  0.02731   0.0   7.07   0.0  0.469  ...  242.0     17.8  396.90   9.14  21.6\n",
       "2  0.02729   0.0   7.07   0.0  0.469  ...  242.0     17.8  392.83   4.03  34.7\n",
       "3  0.03237   0.0   2.18   0.0  0.458  ...  222.0     18.7  394.63   2.94  33.4\n",
       "4  0.06905   0.0   2.18   0.0  0.458  ...  222.0     18.7  396.90   5.33  36.2\n",
       "\n",
       "[5 rows x 14 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "from sklearn.datasets import load_boston\n",
    "boston_dataset = load_boston()\n",
    "df = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n",
    "df['MEDV'] = boston_dataset.target\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pm7zow5IvaTt"
   },
   "source": [
    "## Normalizing Input Data\n",
    "\n",
    "It's not 100% necessary to normalize/scale your input data before feeding it to a neural network, the network can learn the appropriate weights to deal with data of as long as it is numerically represented,  but it is recommended as it can help **make training faster** and **reduces the chances that gradient descent might get stuck in a local optimum**.\n",
    "\n",
    "<https://stackoverflow.com/questions/4674623/why-do-we-have-to-normalize-the-input-for-an-artificial-neural-network>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "id": "q5qDpUj9vZVw",
    "outputId": "79522be5-9c34-4d73-d465-9b345957cb19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.41978194  0.28482986 -1.2879095  ... -1.45900038  0.44105193\n",
      "  -1.0755623 ]\n",
      " [-0.41733926 -0.48772236 -0.59338101 ... -0.30309415  0.44105193\n",
      "  -0.49243937]\n",
      " [-0.41734159 -0.48772236 -0.59338101 ... -0.30309415  0.39642699\n",
      "  -1.2087274 ]\n",
      " ...\n",
      " [-0.41344658 -0.48772236  0.11573841 ...  1.17646583  0.44105193\n",
      "  -0.98304761]\n",
      " [-0.40776407 -0.48772236  0.11573841 ...  1.17646583  0.4032249\n",
      "  -0.86530163]\n",
      " [-0.41500016 -0.48772236  0.11573841 ...  1.17646583  0.44105193\n",
      "  -0.66905833]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Split into X and y and turn into numpy arays\n",
    "y = df.MEDV.values\n",
    "X = df.drop(\"MEDV\", axis='columns').values\n",
    "\n",
    "# Scale input data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "qdr1B7EmaiOw",
    "outputId": "9483d4cc-b2d5-4fc9-b5f9-a9e85e6ac2f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class StandardScaler in module sklearn.preprocessing.data:\n",
      "\n",
      "class StandardScaler(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin)\n",
      " |  Standardize features by removing the mean and scaling to unit variance\n",
      " |  \n",
      " |  The standard score of a sample `x` is calculated as:\n",
      " |  \n",
      " |      z = (x - u) / s\n",
      " |  \n",
      " |  where `u` is the mean of the training samples or zero if `with_mean=False`,\n",
      " |  and `s` is the standard deviation of the training samples or one if\n",
      " |  `with_std=False`.\n",
      " |  \n",
      " |  Centering and scaling happen independently on each feature by computing\n",
      " |  the relevant statistics on the samples in the training set. Mean and\n",
      " |  standard deviation are then stored to be used on later data using the\n",
      " |  `transform` method.\n",
      " |  \n",
      " |  Standardization of a dataset is a common requirement for many\n",
      " |  machine learning estimators: they might behave badly if the\n",
      " |  individual features do not more or less look like standard normally\n",
      " |  distributed data (e.g. Gaussian with 0 mean and unit variance).\n",
      " |  \n",
      " |  For instance many elements used in the objective function of\n",
      " |  a learning algorithm (such as the RBF kernel of Support Vector\n",
      " |  Machines or the L1 and L2 regularizers of linear models) assume that\n",
      " |  all features are centered around 0 and have variance in the same\n",
      " |  order. If a feature has a variance that is orders of magnitude larger\n",
      " |  that others, it might dominate the objective function and make the\n",
      " |  estimator unable to learn from other features correctly as expected.\n",
      " |  \n",
      " |  This scaler can also be applied to sparse CSR or CSC matrices by passing\n",
      " |  `with_mean=False` to avoid breaking the sparsity structure of the data.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  copy : boolean, optional, default True\n",
      " |      If False, try to avoid a copy and do inplace scaling instead.\n",
      " |      This is not guaranteed to always work inplace; e.g. if the data is\n",
      " |      not a NumPy array or scipy.sparse CSR matrix, a copy may still be\n",
      " |      returned.\n",
      " |  \n",
      " |  with_mean : boolean, True by default\n",
      " |      If True, center the data before scaling.\n",
      " |      This does not work (and will raise an exception) when attempted on\n",
      " |      sparse matrices, because centering them entails building a dense\n",
      " |      matrix which in common use cases is likely to be too large to fit in\n",
      " |      memory.\n",
      " |  \n",
      " |  with_std : boolean, True by default\n",
      " |      If True, scale the data to unit variance (or equivalently,\n",
      " |      unit standard deviation).\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  scale_ : ndarray or None, shape (n_features,)\n",
      " |      Per feature relative scaling of the data. This is calculated using\n",
      " |      `np.sqrt(var_)`. Equal to ``None`` when ``with_std=False``.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *scale_*\n",
      " |  \n",
      " |  mean_ : ndarray or None, shape (n_features,)\n",
      " |      The mean value for each feature in the training set.\n",
      " |      Equal to ``None`` when ``with_mean=False``.\n",
      " |  \n",
      " |  var_ : ndarray or None, shape (n_features,)\n",
      " |      The variance for each feature in the training set. Used to compute\n",
      " |      `scale_`. Equal to ``None`` when ``with_std=False``.\n",
      " |  \n",
      " |  n_samples_seen_ : int or array, shape (n_features,)\n",
      " |      The number of samples processed by the estimator for each feature.\n",
      " |      If there are not missing samples, the ``n_samples_seen`` will be an\n",
      " |      integer, otherwise it will be an array.\n",
      " |      Will be reset on new calls to fit, but increments across\n",
      " |      ``partial_fit`` calls.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.preprocessing import StandardScaler\n",
      " |  >>> data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n",
      " |  >>> scaler = StandardScaler()\n",
      " |  >>> print(scaler.fit(data))\n",
      " |  StandardScaler(copy=True, with_mean=True, with_std=True)\n",
      " |  >>> print(scaler.mean_)\n",
      " |  [0.5 0.5]\n",
      " |  >>> print(scaler.transform(data))\n",
      " |  [[-1. -1.]\n",
      " |   [-1. -1.]\n",
      " |   [ 1.  1.]\n",
      " |   [ 1.  1.]]\n",
      " |  >>> print(scaler.transform([[2, 2]]))\n",
      " |  [[3. 3.]]\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  scale: Equivalent function without the estimator API.\n",
      " |  \n",
      " |  :class:`sklearn.decomposition.PCA`\n",
      " |      Further removes the linear correlation across features with 'whiten=True'.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  NaNs are treated as missing values: disregarded in fit, and maintained in\n",
      " |  transform.\n",
      " |  \n",
      " |  We use a biased estimator for the standard deviation, equivalent to\n",
      " |  `numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to\n",
      " |  affect model performance.\n",
      " |  \n",
      " |  For a comparison of the different scalers, transformers, and normalizers,\n",
      " |  see :ref:`examples/preprocessing/plot_all_scaling.py\n",
      " |  <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      StandardScaler\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, copy=True, with_mean=True, with_std=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Compute the mean and std to be used for later scaling.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape [n_samples, n_features]\n",
      " |          The data used to compute the mean and standard deviation\n",
      " |          used for later scaling along the features axis.\n",
      " |      \n",
      " |      y\n",
      " |          Ignored\n",
      " |  \n",
      " |  inverse_transform(self, X, copy=None)\n",
      " |      Scale back the data to the original representation\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape [n_samples, n_features]\n",
      " |          The data used to scale along the features axis.\n",
      " |      copy : bool, optional (default: None)\n",
      " |          Copy the input X or not.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_tr : array-like, shape [n_samples, n_features]\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  partial_fit(self, X, y=None)\n",
      " |      Online computation of mean and std on X for later scaling.\n",
      " |      All of X is processed as a single batch. This is intended for cases\n",
      " |      when `fit` is not feasible due to very large number of `n_samples`\n",
      " |      or because X is read from a continuous stream.\n",
      " |      \n",
      " |      The algorithm for incremental mean and std is given in Equation 1.5a,b\n",
      " |      in Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. \"Algorithms\n",
      " |      for computing the sample variance: Analysis and recommendations.\"\n",
      " |      The American Statistician 37.3 (1983): 242-247:\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape [n_samples, n_features]\n",
      " |          The data used to compute the mean and standard deviation\n",
      " |          used for later scaling along the features axis.\n",
      " |      \n",
      " |      y\n",
      " |          Ignored\n",
      " |  \n",
      " |  transform(self, X, copy=None)\n",
      " |      Perform standardization by centering and scaling\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape [n_samples, n_features]\n",
      " |          The data used to scale along the features axis.\n",
      " |      copy : bool, optional (default: None)\n",
      " |          Copy the input X or not.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to X and y with optional parameters fit_params\n",
      " |      and returns a transformed version of X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : numpy array of shape [n_samples, n_features]\n",
      " |          Training set.\n",
      " |      \n",
      " |      y : numpy array of shape [n_samples]\n",
      " |          Target values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : numpy array of shape [n_samples, n_features_new]\n",
      " |          Transformed array.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(StandardScaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l6hgCWbir90R"
   },
   "source": [
    "## Model Validation using an automatic verification Dataset\n",
    "\n",
    "Instead of doing a train test split, Keras has a really nice feature that you can set the validation.split argument when fitting your model and Keras will take that portion of your test data and use it as a validation dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "GMXVfmzXp1Oo",
    "outputId": "d7fa6e3f-4c10-4ad3-d658-edeaa9b7f4af"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0718 16:31:39.725516 140220120881024 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0718 16:31:39.785847 140220120881024 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0718 16:31:39.795482 140220120881024 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0718 16:31:39.845210 140220120881024 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0718 16:31:40.029989 140220120881024 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0718 16:31:40.131572 140220120881024 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 339 samples, validate on 167 samples\n",
      "Epoch 1/50\n",
      "339/339 [==============================] - 1s 2ms/step - loss: 657.0090 - val_loss: 283.9354\n",
      "Epoch 2/50\n",
      "339/339 [==============================] - 0s 133us/step - loss: 441.2104 - val_loss: 176.4909\n",
      "Epoch 3/50\n",
      "339/339 [==============================] - 0s 148us/step - loss: 124.2437 - val_loss: 120.0018\n",
      "Epoch 4/50\n",
      "339/339 [==============================] - 0s 152us/step - loss: 34.3569 - val_loss: 93.9619\n",
      "Epoch 5/50\n",
      "339/339 [==============================] - 0s 160us/step - loss: 18.5256 - val_loss: 89.3310\n",
      "Epoch 6/50\n",
      "339/339 [==============================] - 0s 149us/step - loss: 14.8768 - val_loss: 90.1059\n",
      "Epoch 7/50\n",
      "339/339 [==============================] - 0s 143us/step - loss: 12.9358 - val_loss: 84.6417\n",
      "Epoch 8/50\n",
      "339/339 [==============================] - 0s 141us/step - loss: 11.9425 - val_loss: 80.1941\n",
      "Epoch 9/50\n",
      "339/339 [==============================] - 0s 154us/step - loss: 11.1646 - val_loss: 76.3299\n",
      "Epoch 10/50\n",
      "339/339 [==============================] - 0s 134us/step - loss: 10.6084 - val_loss: 74.3779\n",
      "Epoch 11/50\n",
      "339/339 [==============================] - 0s 154us/step - loss: 10.2920 - val_loss: 70.3953\n",
      "Epoch 12/50\n",
      "339/339 [==============================] - 0s 143us/step - loss: 9.4568 - val_loss: 67.6377\n",
      "Epoch 13/50\n",
      "339/339 [==============================] - 0s 165us/step - loss: 8.9720 - val_loss: 64.4233\n",
      "Epoch 14/50\n",
      "339/339 [==============================] - 0s 135us/step - loss: 8.5606 - val_loss: 63.0749\n",
      "Epoch 15/50\n",
      "339/339 [==============================] - 0s 145us/step - loss: 8.1571 - val_loss: 60.4792\n",
      "Epoch 16/50\n",
      "339/339 [==============================] - 0s 138us/step - loss: 7.9086 - val_loss: 59.0250\n",
      "Epoch 17/50\n",
      "339/339 [==============================] - 0s 139us/step - loss: 7.6783 - val_loss: 57.3049\n",
      "Epoch 18/50\n",
      "339/339 [==============================] - 0s 147us/step - loss: 7.4192 - val_loss: 56.1561\n",
      "Epoch 19/50\n",
      "339/339 [==============================] - 0s 175us/step - loss: 7.0916 - val_loss: 53.8954\n",
      "Epoch 20/50\n",
      "339/339 [==============================] - 0s 145us/step - loss: 6.8301 - val_loss: 51.6414\n",
      "Epoch 21/50\n",
      "339/339 [==============================] - 0s 144us/step - loss: 6.6729 - val_loss: 50.7284\n",
      "Epoch 22/50\n",
      "339/339 [==============================] - 0s 142us/step - loss: 6.5703 - val_loss: 48.5246\n",
      "Epoch 23/50\n",
      "339/339 [==============================] - 0s 141us/step - loss: 6.3323 - val_loss: 48.1738\n",
      "Epoch 24/50\n",
      "339/339 [==============================] - 0s 131us/step - loss: 6.3167 - val_loss: 47.9815\n",
      "Epoch 25/50\n",
      "339/339 [==============================] - 0s 145us/step - loss: 5.9130 - val_loss: 45.2311\n",
      "Epoch 26/50\n",
      "339/339 [==============================] - 0s 163us/step - loss: 5.8202 - val_loss: 45.2131\n",
      "Epoch 27/50\n",
      "339/339 [==============================] - 0s 152us/step - loss: 5.7004 - val_loss: 43.6795\n",
      "Epoch 28/50\n",
      "339/339 [==============================] - 0s 175us/step - loss: 5.5595 - val_loss: 42.5275\n",
      "Epoch 29/50\n",
      "339/339 [==============================] - 0s 143us/step - loss: 5.6014 - val_loss: 42.5296\n",
      "Epoch 30/50\n",
      "339/339 [==============================] - 0s 142us/step - loss: 5.4691 - val_loss: 41.7794\n",
      "Epoch 31/50\n",
      "339/339 [==============================] - 0s 142us/step - loss: 5.2407 - val_loss: 40.6310\n",
      "Epoch 32/50\n",
      "339/339 [==============================] - 0s 147us/step - loss: 5.3110 - val_loss: 41.4662\n",
      "Epoch 33/50\n",
      "339/339 [==============================] - 0s 142us/step - loss: 5.0646 - val_loss: 39.9169\n",
      "Epoch 34/50\n",
      "339/339 [==============================] - 0s 164us/step - loss: 5.0963 - val_loss: 38.8307\n",
      "Epoch 35/50\n",
      "339/339 [==============================] - 0s 144us/step - loss: 5.1229 - val_loss: 39.7172\n",
      "Epoch 36/50\n",
      "339/339 [==============================] - 0s 144us/step - loss: 4.7784 - val_loss: 38.4727\n",
      "Epoch 37/50\n",
      "339/339 [==============================] - 0s 130us/step - loss: 4.8280 - val_loss: 38.9072\n",
      "Epoch 38/50\n",
      "339/339 [==============================] - 0s 168us/step - loss: 4.6528 - val_loss: 37.7141\n",
      "Epoch 39/50\n",
      "339/339 [==============================] - 0s 137us/step - loss: 4.7104 - val_loss: 37.7570\n",
      "Epoch 40/50\n",
      "339/339 [==============================] - 0s 141us/step - loss: 4.6080 - val_loss: 37.0969\n",
      "Epoch 41/50\n",
      "339/339 [==============================] - 0s 151us/step - loss: 4.6186 - val_loss: 38.0980\n",
      "Epoch 42/50\n",
      "339/339 [==============================] - 0s 165us/step - loss: 4.4325 - val_loss: 37.2696\n",
      "Epoch 43/50\n",
      "339/339 [==============================] - 0s 142us/step - loss: 4.4345 - val_loss: 37.6661\n",
      "Epoch 44/50\n",
      "339/339 [==============================] - 0s 141us/step - loss: 4.9506 - val_loss: 36.2959\n",
      "Epoch 45/50\n",
      "339/339 [==============================] - 0s 147us/step - loss: 4.4424 - val_loss: 37.1898\n",
      "Epoch 46/50\n",
      "339/339 [==============================] - 0s 153us/step - loss: 4.3080 - val_loss: 37.3141\n",
      "Epoch 47/50\n",
      "339/339 [==============================] - 0s 150us/step - loss: 4.2036 - val_loss: 37.4852\n",
      "Epoch 48/50\n",
      "339/339 [==============================] - 0s 151us/step - loss: 4.2409 - val_loss: 36.8323\n",
      "Epoch 49/50\n",
      "339/339 [==============================] - 0s 137us/step - loss: 4.1821 - val_loss: 35.8628\n",
      "Epoch 50/50\n",
      "339/339 [==============================] - 0s 139us/step - loss: 4.1663 - val_loss: 37.1778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8748610518>"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Important Hyperparameters\n",
    "inputs = X.shape[1]\n",
    "epochs = 50\n",
    "batch_size = 10\n",
    "\n",
    "# Create Model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(inputs,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "# Compile Model\n",
    "model.compile(optimizer='adam', loss='mse')#, metrics=['mse'])\n",
    "# Fit Model\n",
    "model.fit(X, y, validation_split=0.33, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FYgb6yiKzyK5"
   },
   "source": [
    "## Model Verification using a Manual Verification Dataset\n",
    "\n",
    "Even though Keras has this nice feature you can still do train test split and pass it both X_test and y_test datasets as a tuple using the `validation_data` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "D6SenwGVzQlK",
    "outputId": "a4524b47-8241-4cdf-aff4-8bd455b3f502"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 339 samples, validate on 167 samples\n",
      "Epoch 1/50\n",
      "339/339 [==============================] - 0s 955us/step - loss: 566.7569 - val_loss: 434.1867\n",
      "Epoch 2/50\n",
      "339/339 [==============================] - 0s 141us/step - loss: 390.9120 - val_loss: 228.1325\n",
      "Epoch 3/50\n",
      "339/339 [==============================] - 0s 146us/step - loss: 148.4245 - val_loss: 65.3349\n",
      "Epoch 4/50\n",
      "339/339 [==============================] - 0s 143us/step - loss: 52.9822 - val_loss: 37.1925\n",
      "Epoch 5/50\n",
      "339/339 [==============================] - 0s 158us/step - loss: 32.9778 - val_loss: 26.6480\n",
      "Epoch 6/50\n",
      "339/339 [==============================] - 0s 139us/step - loss: 26.0195 - val_loss: 22.1330\n",
      "Epoch 7/50\n",
      "339/339 [==============================] - 0s 154us/step - loss: 22.9841 - val_loss: 19.6079\n",
      "Epoch 8/50\n",
      "339/339 [==============================] - 0s 141us/step - loss: 20.8186 - val_loss: 18.3036\n",
      "Epoch 9/50\n",
      "339/339 [==============================] - 0s 143us/step - loss: 19.4724 - val_loss: 17.4211\n",
      "Epoch 10/50\n",
      "339/339 [==============================] - 0s 141us/step - loss: 18.5898 - val_loss: 16.5648\n",
      "Epoch 11/50\n",
      "339/339 [==============================] - 0s 148us/step - loss: 18.1152 - val_loss: 15.6838\n",
      "Epoch 12/50\n",
      "339/339 [==============================] - 0s 136us/step - loss: 16.6302 - val_loss: 15.2401\n",
      "Epoch 13/50\n",
      "339/339 [==============================] - 0s 147us/step - loss: 15.9293 - val_loss: 14.8921\n",
      "Epoch 14/50\n",
      "339/339 [==============================] - 0s 138us/step - loss: 15.3378 - val_loss: 14.4107\n",
      "Epoch 15/50\n",
      "339/339 [==============================] - 0s 152us/step - loss: 14.9160 - val_loss: 14.2170\n",
      "Epoch 16/50\n",
      "339/339 [==============================] - 0s 143us/step - loss: 14.4469 - val_loss: 13.8880\n",
      "Epoch 17/50\n",
      "339/339 [==============================] - 0s 171us/step - loss: 14.0249 - val_loss: 13.7287\n",
      "Epoch 18/50\n",
      "339/339 [==============================] - 0s 142us/step - loss: 13.5823 - val_loss: 13.2861\n",
      "Epoch 19/50\n",
      "339/339 [==============================] - 0s 145us/step - loss: 13.0466 - val_loss: 13.0372\n",
      "Epoch 20/50\n",
      "339/339 [==============================] - 0s 145us/step - loss: 13.0507 - val_loss: 13.2522\n",
      "Epoch 21/50\n",
      "339/339 [==============================] - 0s 152us/step - loss: 12.8242 - val_loss: 12.6006\n",
      "Epoch 22/50\n",
      "339/339 [==============================] - 0s 153us/step - loss: 12.3180 - val_loss: 12.4620\n",
      "Epoch 23/50\n",
      "339/339 [==============================] - 0s 149us/step - loss: 12.2356 - val_loss: 12.2926\n",
      "Epoch 24/50\n",
      "339/339 [==============================] - 0s 140us/step - loss: 12.0117 - val_loss: 11.9667\n",
      "Epoch 25/50\n",
      "339/339 [==============================] - 0s 143us/step - loss: 11.8344 - val_loss: 11.7761\n",
      "Epoch 26/50\n",
      "339/339 [==============================] - 0s 159us/step - loss: 11.2415 - val_loss: 11.8677\n",
      "Epoch 27/50\n",
      "339/339 [==============================] - 0s 152us/step - loss: 11.0982 - val_loss: 11.5067\n",
      "Epoch 28/50\n",
      "339/339 [==============================] - 0s 145us/step - loss: 11.0162 - val_loss: 11.7205\n",
      "Epoch 29/50\n",
      "339/339 [==============================] - 0s 144us/step - loss: 10.7725 - val_loss: 11.4640\n",
      "Epoch 30/50\n",
      "339/339 [==============================] - 0s 154us/step - loss: 10.7148 - val_loss: 11.1729\n",
      "Epoch 31/50\n",
      "339/339 [==============================] - 0s 143us/step - loss: 10.4480 - val_loss: 11.2846\n",
      "Epoch 32/50\n",
      "339/339 [==============================] - 0s 155us/step - loss: 10.1909 - val_loss: 11.0946\n",
      "Epoch 33/50\n",
      "339/339 [==============================] - 0s 168us/step - loss: 9.9671 - val_loss: 11.0050\n",
      "Epoch 34/50\n",
      "339/339 [==============================] - 0s 143us/step - loss: 9.7955 - val_loss: 10.9717\n",
      "Epoch 35/50\n",
      "339/339 [==============================] - 0s 158us/step - loss: 9.5960 - val_loss: 11.3618\n",
      "Epoch 36/50\n",
      "339/339 [==============================] - 0s 144us/step - loss: 9.7466 - val_loss: 11.6410\n",
      "Epoch 37/50\n",
      "339/339 [==============================] - 0s 134us/step - loss: 9.6775 - val_loss: 10.8775\n",
      "Epoch 38/50\n",
      "339/339 [==============================] - 0s 152us/step - loss: 9.1237 - val_loss: 10.8203\n",
      "Epoch 39/50\n",
      "339/339 [==============================] - 0s 144us/step - loss: 9.0086 - val_loss: 10.5888\n",
      "Epoch 40/50\n",
      "339/339 [==============================] - 0s 132us/step - loss: 8.9937 - val_loss: 10.6772\n",
      "Epoch 41/50\n",
      "339/339 [==============================] - 0s 138us/step - loss: 8.9854 - val_loss: 10.3810\n",
      "Epoch 42/50\n",
      "339/339 [==============================] - 0s 162us/step - loss: 8.6652 - val_loss: 10.5512\n",
      "Epoch 43/50\n",
      "339/339 [==============================] - 0s 147us/step - loss: 8.3563 - val_loss: 11.0222\n",
      "Epoch 44/50\n",
      "339/339 [==============================] - 0s 136us/step - loss: 8.2444 - val_loss: 10.1766\n",
      "Epoch 45/50\n",
      "339/339 [==============================] - 0s 165us/step - loss: 8.2073 - val_loss: 10.2537\n",
      "Epoch 46/50\n",
      "339/339 [==============================] - 0s 148us/step - loss: 8.0452 - val_loss: 10.9200\n",
      "Epoch 47/50\n",
      "339/339 [==============================] - 0s 139us/step - loss: 7.9320 - val_loss: 10.9995\n",
      "Epoch 48/50\n",
      "339/339 [==============================] - 0s 144us/step - loss: 7.8962 - val_loss: 10.2226\n",
      "Epoch 49/50\n",
      "339/339 [==============================] - 0s 143us/step - loss: 7.5985 - val_loss: 10.0543\n",
      "Epoch 50/50\n",
      "339/339 [==============================] - 0s 148us/step - loss: 7.4927 - val_loss: 10.5215\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8742c33be0>"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Random Seed\n",
    "seed = 42\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# Load Dataset\n",
    "boston_dataset = load_boston()\n",
    "df = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n",
    "df['MEDV'] = boston_dataset.target\n",
    "\n",
    "# Split into X and y and turn into numpy arays\n",
    "y = df.MEDV.values\n",
    "X = df.drop(\"MEDV\", axis='columns').values\n",
    "\n",
    "# Scale inputs\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Manual Validation Dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=seed)\n",
    "\n",
    "# Important Hyperparameters\n",
    "inputs = X.shape[1]\n",
    "epochs = 50\n",
    "batch_size = 10\n",
    "\n",
    "# Create Model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(inputs,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "# Compile Model\n",
    "model.compile(optimizer='adam', loss='mse')#, metrics=['mse'])\n",
    "# Fit Model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k3FOE1vxx7yn"
   },
   "source": [
    "## Manual K-fold Cross Validation (regression)\n",
    "\n",
    "K-fold Cross Validation is the best way to validate your model as it will reduce the variance of your overall accuracy measure by splitting the data into smaller cross validation batches using one of the split out datasets as the validation dataset. The model is then fit K times rotating the validation set until each portion of the data has served as the validation dataset, the accuracy results from each subset are then averaged and reported as the overall test accuracy. The upside is that you can be more confident in your model's final reported accuracy. The downside is that you have to train your model K times which is computationally expensive and just not pheasible for some large networks that can take days to train a single time.\n",
    "\n",
    "K-fold Cross Validation gets tricky if you want to standardize your data because you will have to standardize your X values for each fold of your model separately. Due to this, we're going to use a Scikit-Learn Pipeline to help us do this for each fold of our dataset. \n",
    "\n",
    "Also, it's weird, but cross_val_score returns MSE values as negative values:\n",
    "<https://github.com/scikit-learn/scikit-learn/issues/2439>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "OnIHhAYV3Xt-",
    "outputId": "ec70aa07-92d6-48a3-9c1d-6a6ec3e25ec7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "404/404 [==============================] - 0s 773us/step - loss: 546.7741\n",
      "Epoch 2/50\n",
      "404/404 [==============================] - 0s 116us/step - loss: 310.7928\n",
      "Epoch 3/50\n",
      "404/404 [==============================] - 0s 115us/step - loss: 85.0224\n",
      "Epoch 4/50\n",
      "404/404 [==============================] - 0s 129us/step - loss: 39.6435\n",
      "Epoch 5/50\n",
      "404/404 [==============================] - 0s 113us/step - loss: 28.1213\n",
      "Epoch 6/50\n",
      "404/404 [==============================] - 0s 126us/step - loss: 23.4682\n",
      "Epoch 7/50\n",
      "404/404 [==============================] - 0s 113us/step - loss: 20.9045\n",
      "Epoch 8/50\n",
      "404/404 [==============================] - 0s 119us/step - loss: 19.2325\n",
      "Epoch 9/50\n",
      "404/404 [==============================] - 0s 119us/step - loss: 18.2092\n",
      "Epoch 10/50\n",
      "404/404 [==============================] - 0s 127us/step - loss: 16.7901\n",
      "Epoch 11/50\n",
      "404/404 [==============================] - 0s 122us/step - loss: 16.0157\n",
      "Epoch 12/50\n",
      "404/404 [==============================] - 0s 126us/step - loss: 15.4801\n",
      "Epoch 13/50\n",
      "404/404 [==============================] - 0s 118us/step - loss: 14.7867\n",
      "Epoch 14/50\n",
      "404/404 [==============================] - 0s 129us/step - loss: 14.2349\n",
      "Epoch 15/50\n",
      "404/404 [==============================] - 0s 116us/step - loss: 13.8844\n",
      "Epoch 16/50\n",
      "404/404 [==============================] - 0s 149us/step - loss: 13.3373\n",
      "Epoch 17/50\n",
      "404/404 [==============================] - 0s 118us/step - loss: 13.0761\n",
      "Epoch 18/50\n",
      "404/404 [==============================] - 0s 119us/step - loss: 12.6376\n",
      "Epoch 19/50\n",
      "404/404 [==============================] - 0s 119us/step - loss: 12.3398\n",
      "Epoch 20/50\n",
      "404/404 [==============================] - 0s 116us/step - loss: 11.9955\n",
      "Epoch 21/50\n",
      "404/404 [==============================] - 0s 131us/step - loss: 11.7749\n",
      "Epoch 22/50\n",
      "404/404 [==============================] - 0s 137us/step - loss: 11.8150\n",
      "Epoch 23/50\n",
      "404/404 [==============================] - 0s 131us/step - loss: 11.3099\n",
      "Epoch 24/50\n",
      "404/404 [==============================] - 0s 119us/step - loss: 10.9789\n",
      "Epoch 25/50\n",
      "404/404 [==============================] - 0s 114us/step - loss: 11.0491\n",
      "Epoch 26/50\n",
      "404/404 [==============================] - 0s 113us/step - loss: 10.7401\n",
      "Epoch 27/50\n",
      "404/404 [==============================] - 0s 132us/step - loss: 10.5614\n",
      "Epoch 28/50\n",
      "404/404 [==============================] - 0s 107us/step - loss: 10.2462\n",
      "Epoch 29/50\n",
      "404/404 [==============================] - 0s 114us/step - loss: 10.6647\n",
      "Epoch 30/50\n",
      "404/404 [==============================] - 0s 130us/step - loss: 10.0695\n",
      "Epoch 31/50\n",
      "404/404 [==============================] - 0s 117us/step - loss: 10.0244\n",
      "Epoch 32/50\n",
      "404/404 [==============================] - 0s 113us/step - loss: 9.7569\n",
      "Epoch 33/50\n",
      "404/404 [==============================] - 0s 150us/step - loss: 9.7199\n",
      "Epoch 34/50\n",
      "404/404 [==============================] - 0s 111us/step - loss: 9.5651\n",
      "Epoch 35/50\n",
      "404/404 [==============================] - 0s 118us/step - loss: 9.4003\n",
      "Epoch 36/50\n",
      "404/404 [==============================] - 0s 123us/step - loss: 9.4860\n",
      "Epoch 37/50\n",
      "404/404 [==============================] - 0s 129us/step - loss: 9.1759\n",
      "Epoch 38/50\n",
      "404/404 [==============================] - 0s 115us/step - loss: 9.0169\n",
      "Epoch 39/50\n",
      "404/404 [==============================] - 0s 124us/step - loss: 9.0562\n",
      "Epoch 40/50\n",
      "404/404 [==============================] - 0s 113us/step - loss: 8.5651\n",
      "Epoch 41/50\n",
      "404/404 [==============================] - 0s 124us/step - loss: 8.5478\n",
      "Epoch 42/50\n",
      "404/404 [==============================] - 0s 124us/step - loss: 8.7197\n",
      "Epoch 43/50\n",
      "404/404 [==============================] - 0s 180us/step - loss: 8.2833\n",
      "Epoch 44/50\n",
      "404/404 [==============================] - 0s 118us/step - loss: 8.1785\n",
      "Epoch 45/50\n",
      "404/404 [==============================] - 0s 118us/step - loss: 8.1602\n",
      "Epoch 46/50\n",
      "404/404 [==============================] - 0s 120us/step - loss: 7.8376\n",
      "Epoch 47/50\n",
      "404/404 [==============================] - 0s 124us/step - loss: 7.7481\n",
      "Epoch 48/50\n",
      "404/404 [==============================] - 0s 121us/step - loss: 7.5348\n",
      "Epoch 49/50\n",
      "404/404 [==============================] - 0s 125us/step - loss: 7.5341\n",
      "Epoch 50/50\n",
      "404/404 [==============================] - 0s 119us/step - loss: 7.4397\n",
      "102/102 [==============================] - 0s 716us/step\n",
      "Epoch 1/50\n",
      "405/405 [==============================] - 0s 930us/step - loss: 504.2921\n",
      "Epoch 2/50\n",
      "405/405 [==============================] - 0s 127us/step - loss: 270.9113\n",
      "Epoch 3/50\n",
      "405/405 [==============================] - 0s 121us/step - loss: 73.5950\n",
      "Epoch 4/50\n",
      "405/405 [==============================] - 0s 125us/step - loss: 39.6248\n",
      "Epoch 5/50\n",
      "405/405 [==============================] - 0s 119us/step - loss: 27.2294\n",
      "Epoch 6/50\n",
      "405/405 [==============================] - 0s 116us/step - loss: 23.1322\n",
      "Epoch 7/50\n",
      "405/405 [==============================] - 0s 130us/step - loss: 20.7838\n",
      "Epoch 8/50\n",
      "405/405 [==============================] - 0s 118us/step - loss: 19.3316\n",
      "Epoch 9/50\n",
      "405/405 [==============================] - 0s 121us/step - loss: 18.2560\n",
      "Epoch 10/50\n",
      "405/405 [==============================] - 0s 123us/step - loss: 17.4567\n",
      "Epoch 11/50\n",
      "405/405 [==============================] - 0s 123us/step - loss: 16.6363\n",
      "Epoch 12/50\n",
      "405/405 [==============================] - 0s 120us/step - loss: 15.6740\n",
      "Epoch 13/50\n",
      "405/405 [==============================] - 0s 119us/step - loss: 15.0954\n",
      "Epoch 14/50\n",
      "405/405 [==============================] - 0s 118us/step - loss: 14.4488\n",
      "Epoch 15/50\n",
      "405/405 [==============================] - 0s 120us/step - loss: 13.9108\n",
      "Epoch 16/50\n",
      "405/405 [==============================] - 0s 120us/step - loss: 13.2848\n",
      "Epoch 17/50\n",
      "405/405 [==============================] - 0s 139us/step - loss: 12.9841\n",
      "Epoch 18/50\n",
      "405/405 [==============================] - 0s 146us/step - loss: 12.7620\n",
      "Epoch 19/50\n",
      "405/405 [==============================] - 0s 116us/step - loss: 12.2480\n",
      "Epoch 20/50\n",
      "405/405 [==============================] - 0s 135us/step - loss: 12.1748\n",
      "Epoch 21/50\n",
      "405/405 [==============================] - 0s 116us/step - loss: 11.7277\n",
      "Epoch 22/50\n",
      "405/405 [==============================] - 0s 122us/step - loss: 11.6226\n",
      "Epoch 23/50\n",
      "405/405 [==============================] - 0s 124us/step - loss: 11.3487\n",
      "Epoch 24/50\n",
      "405/405 [==============================] - 0s 114us/step - loss: 10.9722\n",
      "Epoch 25/50\n",
      "405/405 [==============================] - 0s 127us/step - loss: 10.8177\n",
      "Epoch 26/50\n",
      "405/405 [==============================] - 0s 137us/step - loss: 11.1469\n",
      "Epoch 27/50\n",
      "405/405 [==============================] - 0s 119us/step - loss: 10.4371\n",
      "Epoch 28/50\n",
      "405/405 [==============================] - 0s 148us/step - loss: 10.1707\n",
      "Epoch 29/50\n",
      "405/405 [==============================] - 0s 116us/step - loss: 10.1931\n",
      "Epoch 30/50\n",
      "405/405 [==============================] - 0s 123us/step - loss: 9.8668\n",
      "Epoch 31/50\n",
      "405/405 [==============================] - 0s 120us/step - loss: 9.7811\n",
      "Epoch 32/50\n",
      "405/405 [==============================] - 0s 119us/step - loss: 9.6774\n",
      "Epoch 33/50\n",
      "405/405 [==============================] - 0s 149us/step - loss: 9.2438\n",
      "Epoch 34/50\n",
      "405/405 [==============================] - 0s 134us/step - loss: 9.2495\n",
      "Epoch 35/50\n",
      "405/405 [==============================] - 0s 117us/step - loss: 8.9152\n",
      "Epoch 36/50\n",
      "405/405 [==============================] - 0s 129us/step - loss: 9.0562\n",
      "Epoch 37/50\n",
      "405/405 [==============================] - 0s 120us/step - loss: 8.9071\n",
      "Epoch 38/50\n",
      "405/405 [==============================] - 0s 121us/step - loss: 8.6637\n",
      "Epoch 39/50\n",
      "405/405 [==============================] - 0s 123us/step - loss: 8.4853\n",
      "Epoch 40/50\n",
      "405/405 [==============================] - 0s 126us/step - loss: 8.0851\n",
      "Epoch 41/50\n",
      "405/405 [==============================] - 0s 167us/step - loss: 8.0626\n",
      "Epoch 42/50\n",
      "405/405 [==============================] - 0s 117us/step - loss: 7.9412\n",
      "Epoch 43/50\n",
      "405/405 [==============================] - 0s 133us/step - loss: 7.9165\n",
      "Epoch 44/50\n",
      "405/405 [==============================] - 0s 124us/step - loss: 7.4531\n",
      "Epoch 45/50\n",
      "405/405 [==============================] - 0s 126us/step - loss: 7.4686\n",
      "Epoch 46/50\n",
      "405/405 [==============================] - 0s 124us/step - loss: 7.5709\n",
      "Epoch 47/50\n",
      "405/405 [==============================] - 0s 121us/step - loss: 7.2854\n",
      "Epoch 48/50\n",
      "405/405 [==============================] - 0s 122us/step - loss: 7.2473\n",
      "Epoch 49/50\n",
      "405/405 [==============================] - 0s 127us/step - loss: 7.1370\n",
      "Epoch 50/50\n",
      "405/405 [==============================] - 0s 117us/step - loss: 7.0374\n",
      "101/101 [==============================] - 0s 1ms/step\n",
      "Epoch 1/50\n",
      "405/405 [==============================] - 0s 1ms/step - loss: 429.2826\n",
      "Epoch 2/50\n",
      "405/405 [==============================] - 0s 127us/step - loss: 216.8970\n",
      "Epoch 3/50\n",
      "405/405 [==============================] - 0s 126us/step - loss: 64.4856\n",
      "Epoch 4/50\n",
      "405/405 [==============================] - 0s 117us/step - loss: 37.7895\n",
      "Epoch 5/50\n",
      "405/405 [==============================] - 0s 122us/step - loss: 28.8368\n",
      "Epoch 6/50\n",
      "405/405 [==============================] - 0s 135us/step - loss: 24.2095\n",
      "Epoch 7/50\n",
      "405/405 [==============================] - 0s 121us/step - loss: 21.6532\n",
      "Epoch 8/50\n",
      "405/405 [==============================] - 0s 123us/step - loss: 19.7997\n",
      "Epoch 9/50\n",
      "405/405 [==============================] - 0s 151us/step - loss: 18.4796\n",
      "Epoch 10/50\n",
      "405/405 [==============================] - 0s 111us/step - loss: 17.3576\n",
      "Epoch 11/50\n",
      "405/405 [==============================] - 0s 121us/step - loss: 16.6893\n",
      "Epoch 12/50\n",
      "405/405 [==============================] - 0s 121us/step - loss: 15.9306\n",
      "Epoch 13/50\n",
      "405/405 [==============================] - 0s 131us/step - loss: 15.5705\n",
      "Epoch 14/50\n",
      "405/405 [==============================] - 0s 126us/step - loss: 14.8873\n",
      "Epoch 15/50\n",
      "405/405 [==============================] - 0s 121us/step - loss: 14.9425\n",
      "Epoch 16/50\n",
      "405/405 [==============================] - 0s 121us/step - loss: 14.4278\n",
      "Epoch 17/50\n",
      "405/405 [==============================] - 0s 129us/step - loss: 14.1450\n",
      "Epoch 18/50\n",
      "405/405 [==============================] - 0s 128us/step - loss: 13.7056\n",
      "Epoch 19/50\n",
      "405/405 [==============================] - 0s 123us/step - loss: 13.3808\n",
      "Epoch 20/50\n",
      "405/405 [==============================] - 0s 148us/step - loss: 13.0306\n",
      "Epoch 21/50\n",
      "405/405 [==============================] - 0s 121us/step - loss: 12.8738\n",
      "Epoch 22/50\n",
      "405/405 [==============================] - 0s 128us/step - loss: 12.7733\n",
      "Epoch 23/50\n",
      "405/405 [==============================] - 0s 120us/step - loss: 12.4580\n",
      "Epoch 24/50\n",
      "405/405 [==============================] - 0s 124us/step - loss: 12.3600\n",
      "Epoch 25/50\n",
      "405/405 [==============================] - 0s 127us/step - loss: 12.1012\n",
      "Epoch 26/50\n",
      "405/405 [==============================] - 0s 128us/step - loss: 11.7442\n",
      "Epoch 27/50\n",
      "405/405 [==============================] - 0s 126us/step - loss: 11.5608\n",
      "Epoch 28/50\n",
      "405/405 [==============================] - 0s 151us/step - loss: 11.1951\n",
      "Epoch 29/50\n",
      "405/405 [==============================] - 0s 117us/step - loss: 11.0012\n",
      "Epoch 30/50\n",
      "405/405 [==============================] - 0s 131us/step - loss: 10.8984\n",
      "Epoch 31/50\n",
      "405/405 [==============================] - 0s 122us/step - loss: 10.4728\n",
      "Epoch 32/50\n",
      "405/405 [==============================] - 0s 118us/step - loss: 10.4754\n",
      "Epoch 33/50\n",
      "405/405 [==============================] - 0s 128us/step - loss: 10.4071\n",
      "Epoch 34/50\n",
      "405/405 [==============================] - 0s 123us/step - loss: 10.3860\n",
      "Epoch 35/50\n",
      "405/405 [==============================] - 0s 121us/step - loss: 10.0234\n",
      "Epoch 36/50\n",
      "405/405 [==============================] - 0s 123us/step - loss: 9.7858\n",
      "Epoch 37/50\n",
      "405/405 [==============================] - 0s 126us/step - loss: 9.4922\n",
      "Epoch 38/50\n",
      "405/405 [==============================] - 0s 116us/step - loss: 9.3507\n",
      "Epoch 39/50\n",
      "405/405 [==============================] - 0s 140us/step - loss: 9.1416\n",
      "Epoch 40/50\n",
      "405/405 [==============================] - 0s 125us/step - loss: 9.4176\n",
      "Epoch 41/50\n",
      "405/405 [==============================] - 0s 124us/step - loss: 9.0337\n",
      "Epoch 42/50\n",
      "405/405 [==============================] - 0s 127us/step - loss: 8.8868\n",
      "Epoch 43/50\n",
      "405/405 [==============================] - 0s 135us/step - loss: 8.6010\n",
      "Epoch 44/50\n",
      "405/405 [==============================] - 0s 116us/step - loss: 8.5902\n",
      "Epoch 45/50\n",
      "405/405 [==============================] - 0s 120us/step - loss: 8.4622\n",
      "Epoch 46/50\n",
      "405/405 [==============================] - 0s 123us/step - loss: 8.1030\n",
      "Epoch 47/50\n",
      "405/405 [==============================] - 0s 125us/step - loss: 7.9079\n",
      "Epoch 48/50\n",
      "405/405 [==============================] - 0s 120us/step - loss: 7.8708\n",
      "Epoch 49/50\n",
      "405/405 [==============================] - 0s 153us/step - loss: 7.7593\n",
      "Epoch 50/50\n",
      "405/405 [==============================] - 0s 121us/step - loss: 7.5468\n",
      "101/101 [==============================] - 0s 1ms/step\n",
      "Epoch 1/50\n",
      "405/405 [==============================] - 0s 1ms/step - loss: 520.2589\n",
      "Epoch 2/50\n",
      "405/405 [==============================] - 0s 120us/step - loss: 227.1890\n",
      "Epoch 3/50\n",
      "405/405 [==============================] - 0s 122us/step - loss: 50.3501\n",
      "Epoch 4/50\n",
      "405/405 [==============================] - 0s 114us/step - loss: 25.7130\n",
      "Epoch 5/50\n",
      "405/405 [==============================] - 0s 123us/step - loss: 18.3957\n",
      "Epoch 6/50\n",
      "405/405 [==============================] - 0s 114us/step - loss: 15.6819\n",
      "Epoch 7/50\n",
      "405/405 [==============================] - 0s 125us/step - loss: 13.9447\n",
      "Epoch 8/50\n",
      "405/405 [==============================] - 0s 138us/step - loss: 12.6961\n",
      "Epoch 9/50\n",
      "405/405 [==============================] - 0s 123us/step - loss: 11.8712\n",
      "Epoch 10/50\n",
      "405/405 [==============================] - 0s 124us/step - loss: 11.0788\n",
      "Epoch 11/50\n",
      "405/405 [==============================] - 0s 127us/step - loss: 10.5434\n",
      "Epoch 12/50\n",
      "405/405 [==============================] - 0s 121us/step - loss: 9.8136\n",
      "Epoch 13/50\n",
      "405/405 [==============================] - 0s 123us/step - loss: 9.3647\n",
      "Epoch 14/50\n",
      "405/405 [==============================] - 0s 124us/step - loss: 8.9509\n",
      "Epoch 15/50\n",
      "405/405 [==============================] - 0s 126us/step - loss: 8.5464\n",
      "Epoch 16/50\n",
      "405/405 [==============================] - 0s 132us/step - loss: 8.1887\n",
      "Epoch 17/50\n",
      "405/405 [==============================] - 0s 121us/step - loss: 7.9844\n",
      "Epoch 18/50\n",
      "405/405 [==============================] - 0s 120us/step - loss: 7.7927\n",
      "Epoch 19/50\n",
      "405/405 [==============================] - 0s 152us/step - loss: 7.6109\n",
      "Epoch 20/50\n",
      "405/405 [==============================] - 0s 121us/step - loss: 7.3247\n",
      "Epoch 21/50\n",
      "405/405 [==============================] - 0s 123us/step - loss: 7.1319\n",
      "Epoch 22/50\n",
      "405/405 [==============================] - 0s 119us/step - loss: 6.9780\n",
      "Epoch 23/50\n",
      "405/405 [==============================] - 0s 127us/step - loss: 6.9231\n",
      "Epoch 24/50\n",
      "405/405 [==============================] - 0s 136us/step - loss: 6.6189\n",
      "Epoch 25/50\n",
      "405/405 [==============================] - 0s 131us/step - loss: 6.4460\n",
      "Epoch 26/50\n",
      "405/405 [==============================] - 0s 118us/step - loss: 6.3933\n",
      "Epoch 27/50\n",
      "405/405 [==============================] - 0s 123us/step - loss: 6.4550\n",
      "Epoch 28/50\n",
      "405/405 [==============================] - 0s 123us/step - loss: 6.0480\n",
      "Epoch 29/50\n",
      "405/405 [==============================] - 0s 120us/step - loss: 6.0802\n",
      "Epoch 30/50\n",
      "405/405 [==============================] - 0s 132us/step - loss: 5.8962\n",
      "Epoch 31/50\n",
      "405/405 [==============================] - 0s 117us/step - loss: 5.9803\n",
      "Epoch 32/50\n",
      "405/405 [==============================] - 0s 125us/step - loss: 5.8395\n",
      "Epoch 33/50\n",
      "405/405 [==============================] - 0s 121us/step - loss: 5.6520\n",
      "Epoch 34/50\n",
      "405/405 [==============================] - 0s 121us/step - loss: 5.5573\n",
      "Epoch 35/50\n",
      "405/405 [==============================] - 0s 120us/step - loss: 5.4403\n",
      "Epoch 36/50\n",
      "405/405 [==============================] - 0s 130us/step - loss: 5.3641\n",
      "Epoch 37/50\n",
      "405/405 [==============================] - 0s 121us/step - loss: 5.2604\n",
      "Epoch 38/50\n",
      "405/405 [==============================] - 0s 122us/step - loss: 5.2859\n",
      "Epoch 39/50\n",
      "405/405 [==============================] - 0s 131us/step - loss: 5.2549\n",
      "Epoch 40/50\n",
      "405/405 [==============================] - 0s 127us/step - loss: 5.2658\n",
      "Epoch 41/50\n",
      "405/405 [==============================] - 0s 128us/step - loss: 5.1759\n",
      "Epoch 42/50\n",
      "405/405 [==============================] - 0s 144us/step - loss: 5.0784\n",
      "Epoch 43/50\n",
      "405/405 [==============================] - 0s 120us/step - loss: 5.0477\n",
      "Epoch 44/50\n",
      "405/405 [==============================] - 0s 135us/step - loss: 4.9371\n",
      "Epoch 45/50\n",
      "405/405 [==============================] - 0s 123us/step - loss: 4.9691\n",
      "Epoch 46/50\n",
      "405/405 [==============================] - 0s 116us/step - loss: 4.8125\n",
      "Epoch 47/50\n",
      "405/405 [==============================] - 0s 128us/step - loss: 4.8165\n",
      "Epoch 48/50\n",
      "405/405 [==============================] - 0s 130us/step - loss: 4.8994\n",
      "Epoch 49/50\n",
      "405/405 [==============================] - 0s 119us/step - loss: 4.7855\n",
      "Epoch 50/50\n",
      "405/405 [==============================] - 0s 129us/step - loss: 4.7998\n",
      "101/101 [==============================] - 0s 1ms/step\n",
      "Epoch 1/50\n",
      "405/405 [==============================] - 1s 1ms/step - loss: 599.3654\n",
      "Epoch 2/50\n",
      "405/405 [==============================] - 0s 123us/step - loss: 358.9278\n",
      "Epoch 3/50\n",
      "405/405 [==============================] - 0s 125us/step - loss: 112.3309\n",
      "Epoch 4/50\n",
      "405/405 [==============================] - 0s 129us/step - loss: 50.0640\n",
      "Epoch 5/50\n",
      "405/405 [==============================] - 0s 126us/step - loss: 31.6083\n",
      "Epoch 6/50\n",
      "405/405 [==============================] - 0s 131us/step - loss: 25.7755\n",
      "Epoch 7/50\n",
      "405/405 [==============================] - 0s 122us/step - loss: 23.0357\n",
      "Epoch 8/50\n",
      "405/405 [==============================] - 0s 145us/step - loss: 21.1740\n",
      "Epoch 9/50\n",
      "405/405 [==============================] - 0s 117us/step - loss: 19.1440\n",
      "Epoch 10/50\n",
      "405/405 [==============================] - 0s 128us/step - loss: 17.8257\n",
      "Epoch 11/50\n",
      "405/405 [==============================] - 0s 138us/step - loss: 16.6750\n",
      "Epoch 12/50\n",
      "405/405 [==============================] - 0s 119us/step - loss: 15.7630\n",
      "Epoch 13/50\n",
      "405/405 [==============================] - 0s 124us/step - loss: 14.8530\n",
      "Epoch 14/50\n",
      "405/405 [==============================] - 0s 123us/step - loss: 14.3997\n",
      "Epoch 15/50\n",
      "405/405 [==============================] - 0s 150us/step - loss: 14.0138\n",
      "Epoch 16/50\n",
      "405/405 [==============================] - 0s 120us/step - loss: 13.4111\n",
      "Epoch 17/50\n",
      "405/405 [==============================] - 0s 125us/step - loss: 12.6991\n",
      "Epoch 18/50\n",
      "405/405 [==============================] - 0s 134us/step - loss: 12.3918\n",
      "Epoch 19/50\n",
      "405/405 [==============================] - 0s 126us/step - loss: 12.1796\n",
      "Epoch 20/50\n",
      "405/405 [==============================] - 0s 125us/step - loss: 11.9692\n",
      "Epoch 21/50\n",
      "405/405 [==============================] - 0s 131us/step - loss: 11.6671\n",
      "Epoch 22/50\n",
      "405/405 [==============================] - 0s 119us/step - loss: 11.2818\n",
      "Epoch 23/50\n",
      "405/405 [==============================] - 0s 122us/step - loss: 11.0372\n",
      "Epoch 24/50\n",
      "405/405 [==============================] - 0s 166us/step - loss: 11.0372\n",
      "Epoch 25/50\n",
      "405/405 [==============================] - 0s 115us/step - loss: 10.6164\n",
      "Epoch 26/50\n",
      "405/405 [==============================] - 0s 128us/step - loss: 10.3073\n",
      "Epoch 27/50\n",
      "405/405 [==============================] - 0s 117us/step - loss: 10.2280\n",
      "Epoch 28/50\n",
      "405/405 [==============================] - 0s 146us/step - loss: 10.5303\n",
      "Epoch 29/50\n",
      "405/405 [==============================] - 0s 172us/step - loss: 10.2417\n",
      "Epoch 30/50\n",
      "405/405 [==============================] - 0s 116us/step - loss: 9.7166\n",
      "Epoch 31/50\n",
      "405/405 [==============================] - 0s 129us/step - loss: 9.8918\n",
      "Epoch 32/50\n",
      "405/405 [==============================] - 0s 125us/step - loss: 9.5379\n",
      "Epoch 33/50\n",
      "405/405 [==============================] - 0s 123us/step - loss: 9.3084\n",
      "Epoch 34/50\n",
      "405/405 [==============================] - 0s 119us/step - loss: 9.0930\n",
      "Epoch 35/50\n",
      "405/405 [==============================] - 0s 135us/step - loss: 9.5003\n",
      "Epoch 36/50\n",
      "405/405 [==============================] - 0s 120us/step - loss: 9.0785\n",
      "Epoch 37/50\n",
      "405/405 [==============================] - 0s 127us/step - loss: 8.5422\n",
      "Epoch 38/50\n",
      "405/405 [==============================] - 0s 123us/step - loss: 8.6577\n",
      "Epoch 39/50\n",
      "405/405 [==============================] - 0s 122us/step - loss: 8.4342\n",
      "Epoch 40/50\n",
      "405/405 [==============================] - 0s 160us/step - loss: 8.2147\n",
      "Epoch 41/50\n",
      "405/405 [==============================] - 0s 120us/step - loss: 8.0876\n",
      "Epoch 42/50\n",
      "405/405 [==============================] - 0s 145us/step - loss: 7.8495\n",
      "Epoch 43/50\n",
      "405/405 [==============================] - 0s 117us/step - loss: 7.7338\n",
      "Epoch 44/50\n",
      "405/405 [==============================] - 0s 117us/step - loss: 7.7468\n",
      "Epoch 45/50\n",
      "405/405 [==============================] - 0s 125us/step - loss: 7.4079\n",
      "Epoch 46/50\n",
      "405/405 [==============================] - 0s 130us/step - loss: 7.5735\n",
      "Epoch 47/50\n",
      "405/405 [==============================] - 0s 116us/step - loss: 7.2366\n",
      "Epoch 48/50\n",
      "405/405 [==============================] - 0s 118us/step - loss: 7.0941\n",
      "Epoch 49/50\n",
      "405/405 [==============================] - 0s 125us/step - loss: 7.0716\n",
      "Epoch 50/50\n",
      "405/405 [==============================] - 0s 130us/step - loss: 6.9195\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "K-fold Cross-Val Results - Mean: 24.93 StDev: 13.59 MSE\n"
     ]
    }
   ],
   "source": [
    "# Keras imports\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "# sklearn imports\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Random Seed\n",
    "seed = 42\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# Load Dataset\n",
    "boston_dataset = load_boston()\n",
    "df = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n",
    "df['MEDV'] = boston_dataset.target\n",
    "\n",
    "# Split into X and y and turn into numpy arays\n",
    "y = df.MEDV.values\n",
    "X = df.drop(\"MEDV\", axis='columns').values\n",
    "\n",
    "# Important Hyperparameters\n",
    "inputs = X.shape[1]\n",
    "epochs = 50\n",
    "batch_size = 10\n",
    "\n",
    "# define base model\n",
    "def baseline_model():\n",
    "  # create model\n",
    "  model = Sequential()\n",
    "  model.add(Dense(64, input_dim=inputs, activation='relu')) \n",
    "  model.add(Dense(64, activation='relu')) \n",
    "  model.add(Dense(1))\n",
    "  # Compile model\n",
    "  model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "  return model\n",
    "\n",
    "# evaluate model with standardized dataset\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=baseline_model, epochs=epochs,\n",
    "                                         batch_size=batch_size, verbose=1)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=5, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, y, cv=kfold)\n",
    "print(f\"K-fold Cross-Val Results - Mean: {-results.mean():.2f} StDev: {results.std():.2f} MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "FNdkddrZeTnh",
    "outputId": "03cfa731-a1b2-4f52-9041-04d987f52428"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('standardize', StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       " ('mlp', <keras.wrappers.scikit_learn.KerasRegressor at 0x7f37816c8d68>)]"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 6046
    },
    "colab_type": "code",
    "id": "5ThinLwEebK6",
    "outputId": "bf2feda9-aff4-4dfc-9793-f14d391fd4c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Pipeline in module sklearn.pipeline:\n",
      "\n",
      "class Pipeline(sklearn.utils.metaestimators._BaseComposition)\n",
      " |  Pipeline of transforms with a final estimator.\n",
      " |  \n",
      " |  Sequentially apply a list of transforms and a final estimator.\n",
      " |  Intermediate steps of the pipeline must be 'transforms', that is, they\n",
      " |  must implement fit and transform methods.\n",
      " |  The final estimator only needs to implement fit.\n",
      " |  The transformers in the pipeline can be cached using ``memory`` argument.\n",
      " |  \n",
      " |  The purpose of the pipeline is to assemble several steps that can be\n",
      " |  cross-validated together while setting different parameters.\n",
      " |  For this, it enables setting parameters of the various steps using their\n",
      " |  names and the parameter name separated by a '__', as in the example below.\n",
      " |  A step's estimator may be replaced entirely by setting the parameter\n",
      " |  with its name to another estimator, or a transformer removed by setting\n",
      " |  to None.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <pipeline>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  steps : list\n",
      " |      List of (name, transform) tuples (implementing fit/transform) that are\n",
      " |      chained, in the order in which they are chained, with the last object\n",
      " |      an estimator.\n",
      " |  \n",
      " |  memory : None, str or object with the joblib.Memory interface, optional\n",
      " |      Used to cache the fitted transformers of the pipeline. By default,\n",
      " |      no caching is performed. If a string is given, it is the path to\n",
      " |      the caching directory. Enabling caching triggers a clone of\n",
      " |      the transformers before fitting. Therefore, the transformer\n",
      " |      instance given to the pipeline cannot be inspected\n",
      " |      directly. Use the attribute ``named_steps`` or ``steps`` to\n",
      " |      inspect estimators within the pipeline. Caching the\n",
      " |      transformers is advantageous when fitting is time consuming.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  named_steps : bunch object, a dictionary with attribute access\n",
      " |      Read-only attribute to access any step parameter by user given name.\n",
      " |      Keys are step names and values are steps parameters.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  sklearn.pipeline.make_pipeline : convenience function for simplified\n",
      " |      pipeline construction.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn import svm\n",
      " |  >>> from sklearn.datasets import samples_generator\n",
      " |  >>> from sklearn.feature_selection import SelectKBest\n",
      " |  >>> from sklearn.feature_selection import f_regression\n",
      " |  >>> from sklearn.pipeline import Pipeline\n",
      " |  >>> # generate some data to play with\n",
      " |  >>> X, y = samples_generator.make_classification(\n",
      " |  ...     n_informative=5, n_redundant=0, random_state=42)\n",
      " |  >>> # ANOVA SVM-C\n",
      " |  >>> anova_filter = SelectKBest(f_regression, k=5)\n",
      " |  >>> clf = svm.SVC(kernel='linear')\n",
      " |  >>> anova_svm = Pipeline([('anova', anova_filter), ('svc', clf)])\n",
      " |  >>> # You can set the parameters using the names issued\n",
      " |  >>> # For instance, fit using a k of 10 in the SelectKBest\n",
      " |  >>> # and a parameter 'C' of the svm\n",
      " |  >>> anova_svm.set_params(anova__k=10, svc__C=.1).fit(X, y)\n",
      " |  ...                      # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE\n",
      " |  Pipeline(memory=None,\n",
      " |           steps=[('anova', SelectKBest(...)),\n",
      " |                  ('svc', SVC(...))])\n",
      " |  >>> prediction = anova_svm.predict(X)\n",
      " |  >>> anova_svm.score(X, y)                        # doctest: +ELLIPSIS\n",
      " |  0.83\n",
      " |  >>> # getting the selected features chosen by anova_filter\n",
      " |  >>> anova_svm.named_steps['anova'].get_support()\n",
      " |  ... # doctest: +NORMALIZE_WHITESPACE\n",
      " |  array([False, False,  True,  True, False, False, True,  True, False,\n",
      " |         True,  False,  True,  True, False, True,  False, True, True,\n",
      " |         False, False])\n",
      " |  >>> # Another way to get selected features chosen by anova_filter\n",
      " |  >>> anova_svm.named_steps.anova.get_support()\n",
      " |  ... # doctest: +NORMALIZE_WHITESPACE\n",
      " |  array([False, False,  True,  True, False, False, True,  True, False,\n",
      " |         True,  False,  True,  True, False, True,  False, True, True,\n",
      " |         False, False])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Pipeline\n",
      " |      sklearn.utils.metaestimators._BaseComposition\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, steps, memory=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Apply transforms, and decision_function of the final estimator\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : iterable\n",
      " |          Data to predict on. Must fulfill input requirements of first step\n",
      " |          of the pipeline.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_score : array-like, shape = [n_samples, n_classes]\n",
      " |  \n",
      " |  fit(self, X, y=None, **fit_params)\n",
      " |      Fit the model\n",
      " |      \n",
      " |      Fit all the transforms one after the other and transform the\n",
      " |      data, then fit the transformed data using the final estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : iterable\n",
      " |          Training data. Must fulfill input requirements of first step of the\n",
      " |          pipeline.\n",
      " |      \n",
      " |      y : iterable, default=None\n",
      " |          Training targets. Must fulfill label requirements for all steps of\n",
      " |          the pipeline.\n",
      " |      \n",
      " |      **fit_params : dict of string -> object\n",
      " |          Parameters passed to the ``fit`` method of each step, where\n",
      " |          each parameter name is prefixed such that parameter ``p`` for step\n",
      " |          ``s`` has key ``s__p``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : Pipeline\n",
      " |          This estimator\n",
      " |  \n",
      " |  fit_predict(self, X, y=None, **fit_params)\n",
      " |      Applies fit_predict of last step in pipeline after transforms.\n",
      " |      \n",
      " |      Applies fit_transforms of a pipeline to the data, followed by the\n",
      " |      fit_predict method of the final estimator in the pipeline. Valid\n",
      " |      only if the final estimator implements fit_predict.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : iterable\n",
      " |          Training data. Must fulfill input requirements of first step of\n",
      " |          the pipeline.\n",
      " |      \n",
      " |      y : iterable, default=None\n",
      " |          Training targets. Must fulfill label requirements for all steps\n",
      " |          of the pipeline.\n",
      " |      \n",
      " |      **fit_params : dict of string -> object\n",
      " |          Parameters passed to the ``fit`` method of each step, where\n",
      " |          each parameter name is prefixed such that parameter ``p`` for step\n",
      " |          ``s`` has key ``s__p``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_pred : array-like\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit the model and transform with the final estimator\n",
      " |      \n",
      " |      Fits all the transforms one after the other and transforms the\n",
      " |      data, then uses fit_transform on transformed data with the final\n",
      " |      estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : iterable\n",
      " |          Training data. Must fulfill input requirements of first step of the\n",
      " |          pipeline.\n",
      " |      \n",
      " |      y : iterable, default=None\n",
      " |          Training targets. Must fulfill label requirements for all steps of\n",
      " |          the pipeline.\n",
      " |      \n",
      " |      **fit_params : dict of string -> object\n",
      " |          Parameters passed to the ``fit`` method of each step, where\n",
      " |          each parameter name is prefixed such that parameter ``p`` for step\n",
      " |          ``s`` has key ``s__p``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Xt : array-like, shape = [n_samples, n_transformed_features]\n",
      " |          Transformed samples\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  predict(self, X, **predict_params)\n",
      " |      Apply transforms to the data, and predict with the final estimator\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : iterable\n",
      " |          Data to predict on. Must fulfill input requirements of first step\n",
      " |          of the pipeline.\n",
      " |      \n",
      " |      **predict_params : dict of string -> object\n",
      " |          Parameters to the ``predict`` called at the end of all\n",
      " |          transformations in the pipeline. Note that while this may be\n",
      " |          used to return uncertainties from some models with return_std\n",
      " |          or return_cov, uncertainties that are generated by the\n",
      " |          transformations in the pipeline are not propagated to the\n",
      " |          final estimator.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_pred : array-like\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Apply transforms, and predict_log_proba of the final estimator\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : iterable\n",
      " |          Data to predict on. Must fulfill input requirements of first step\n",
      " |          of the pipeline.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_score : array-like, shape = [n_samples, n_classes]\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Apply transforms, and predict_proba of the final estimator\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : iterable\n",
      " |          Data to predict on. Must fulfill input requirements of first step\n",
      " |          of the pipeline.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_proba : array-like, shape = [n_samples, n_classes]\n",
      " |  \n",
      " |  score(self, X, y=None, sample_weight=None)\n",
      " |      Apply transforms, and score with the final estimator\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : iterable\n",
      " |          Data to predict on. Must fulfill input requirements of first step\n",
      " |          of the pipeline.\n",
      " |      \n",
      " |      y : iterable, default=None\n",
      " |          Targets used for scoring. Must fulfill label requirements for all\n",
      " |          steps of the pipeline.\n",
      " |      \n",
      " |      sample_weight : array-like, default=None\n",
      " |          If not None, this argument is passed as ``sample_weight`` keyword\n",
      " |          argument to the ``score`` method of the final estimator.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |  \n",
      " |  set_params(self, **kwargs)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      Valid parameter keys can be listed with ``get_params()``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  classes_\n",
      " |  \n",
      " |  inverse_transform\n",
      " |      Apply inverse transformations in reverse order\n",
      " |      \n",
      " |      All estimators in the pipeline must support ``inverse_transform``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      Xt : array-like, shape = [n_samples, n_transformed_features]\n",
      " |          Data samples, where ``n_samples`` is the number of samples and\n",
      " |          ``n_features`` is the number of features. Must fulfill\n",
      " |          input requirements of last step of pipeline's\n",
      " |          ``inverse_transform`` method.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Xt : array-like, shape = [n_samples, n_features]\n",
      " |  \n",
      " |  named_steps\n",
      " |  \n",
      " |  transform\n",
      " |      Apply transforms, and transform with the final estimator\n",
      " |      \n",
      " |      This also works where final estimator is ``None``: all prior\n",
      " |      transformations are applied.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : iterable\n",
      " |          Data to transform. Must fulfill input requirements of first step\n",
      " |          of the pipeline.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Xt : array-like, shape = [n_samples, n_transformed_features]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N95xwMJw-5Di"
   },
   "source": [
    "## Manual K-fold Cross Validation (classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "722BauYV_4Y4",
    "outputId": "35d94c13-bcc1-4da3-bd99-eebd876fef31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-07-18 17:20:29--  https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 23278 (23K) [text/plain]\n",
      "Saving to: pima-indians-diabetes.data.csv\n",
      "\n",
      "pima-indians-diabet 100%[===================>]  22.73K  --.-KB/s    in 0.01s   \n",
      "\n",
      "2019-07-18 17:20:34 (1.84 MB/s) - pima-indians-diabetes.data.csv saved [23278/23278]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "4YNTT4Tm6E0W",
    "outputId": "1b25844d-af43-40c8-808f-99a2dc53c675"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 64.94%\n",
      "acc: 65.58%\n",
      "acc: 75.32%\n",
      "acc: 74.51%\n",
      "acc: 71.90%\n",
      "70.45% +/- 4.39%\n"
     ]
    }
   ],
   "source": [
    "# MLP for Pima Indians Dataset with 5-fold cross validation\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 42\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.data.csv\", delimiter=\",\")\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "# define 5-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "cvscores = []\n",
    "for train, test in kfold.split(X, Y):\n",
    "  # create model\n",
    "  model = Sequential()\n",
    "  model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "  model.add(Dense(8, activation='relu'))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "  # Compile model\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) # Fit the model\n",
    "  model.fit(X[train], Y[train], epochs=50, batch_size=10, verbose=0)\n",
    "  # evaluate the model\n",
    "  scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "  print(f'{model.metrics_names[1]}: {(scores[1]*100):.2f}%') \n",
    "  cvscores.append(scores[1]*100)\n",
    "print(f'{numpy.mean(cvscores):.2f}% +/- {numpy.std(cvscores):.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JILi3KJjAkfF"
   },
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "Hyperparameter tuning is much more important with neural networks than it has been with any other models that we have considered up to this point. Other supervised learning models might have a couple of parameters, but neural networks can have dozens. These can substantially affect the accuracy of our models and although it can be a time consuming process is a necessary step when working with neural networks.\n",
    "\n",
    "Hyperparameter tuning comes with a challenge. How can we compare models specified with different hyperparameters if our model's final error metric can vary somewhat erratically? How do we avoid just getting unlucky and selecting the wrong hyperparameter? This is a problem that to a certain degree we just have to live with as we test and test again. However, we can minimize it somewhat by pairing our experiments with Cross Validation to reduce the variance of our final accuracy values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sYJ8t_ezHP4W"
   },
   "source": [
    "## Hyperparameter Tuning Approaches:\n",
    "\n",
    "### 1) Babysitting AKA \"Grad Student Descent\".\n",
    "\n",
    "If you fiddled with any hyperparameters yesterday, this is basically what you did. This approach is 100% manual and is pretty common among researchers where finding that 1 exact specification that jumps your model to a level of accuracy never seen before is the difference between publishing and not publishing a paper. Of course the professors don't do this themselves, that's grunt work. This is also known as the fiddle with hyperparameters until you run out of time method.\n",
    "\n",
    "### 2) Grid Search\n",
    "\n",
    "Grid Search is the Grad Student galaxy brain realization of: why don't I just specify all the experiments I want to run and let the computer try every possible combination of them while I go and grab lunch. This has a specific downside in that if I specify 5 hyperparameters with 5 options each then I've just created 5^5 combinations of hyperparameters to check. Which means that I have to train 3125 different versions of my model Then if I use 5-fold Cross Validation on that then my model has to run 15,525 times. This is the brute-force method of hyperparameter tuning, but it can be very profitable if done wisely. \n",
    "\n",
    "When using Grid Search here's what I suggest: don't use it to test combinations of different hyperparameters, only use it to test different specifications of **a single** hyperparameter. It's rare that combinations between different hyperparameters lead to big performance gains. You'll get 90-95% of the way there if you just Grid Search one parameter and take the best result, then retain that best result while you test another, and then retain the best specification from that while you train another. This at least makes the situation much more manageable and leads to pretty good results. \n",
    "\n",
    "### 3) Random Search\n",
    "\n",
    "Do Grid Search for a couple of hours and you'll say to yourself - \"There's got to be a better way.\" Enter Random Search. For Random search you specify a hyperparameter space and it picks specifications from that randomly, tries them out, gives you the best results and says - That's going to have to be good enough, go home and spend time with your family. \n",
    "\n",
    "Grid Search treats every parameter as if it was equally important, but this just isn't the case, some are known to move the needle a lot more than others (we'll talk about that in a minute). Random Search allows searching to be specified along the most important parameter and experiments less along the dimensions of less important hyperparameters. The downside of Random search is that it won't find the absolute best hyperparameters, but it is much less costly to perform than Grid Search. \n",
    "\n",
    "### 4) Bayesian Methods\n",
    "\n",
    "One thing that can make more manual methods like babysitting and gridsearch effective is that as the experimenter sees results he can then make updates to his future searches taking into account the results of past specifications. If only we could hyperparameter tune our hyperparameter tuning. Well, we kind of can. Enter Bayesian Optimization. Neural Networks are like an optimization problem within an optimization problem, and Bayesian Optimization is a search strategy that tries to take into account the results of past searches in order to improve future ones. This is the most advanced method but can be a little bit tricky to implement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HfQ7D043OMMn"
   },
   "source": [
    "## What Hyperparameters are there to test?\n",
    "\n",
    "- batch_size\n",
    "- training epochs\n",
    "- optimization algorithms\n",
    "- learning rate\n",
    "- momentum\n",
    "- activation functions\n",
    "- dropout regularization\n",
    "- number of neurons in the hidden layer\n",
    "\n",
    "There are more, but these are the most important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Xl9c2VPPITI"
   },
   "source": [
    "## Creating a \"Test Harness\"\n",
    "\n",
    "You want it to be easy to specify different version of your model and for that model to undergo the same conditions every time with nothing different except for the hyperparameter(s) that you are testing. \n",
    "\n",
    "In order to do this we're going to use GridSearchCV from scikit learn to run our model with different hyperparameters and then report back to us the best one. In order to do this we have to wrap our model in either a KerasRegressor or KerasClassifier object to prepare it to work with Scikit-Learn. We also have to put our model specification within a function that we can call in order to generate it. This is required in order to use KerasRegressor or KerasClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mri5-kXzVKAa"
   },
   "source": [
    "## Batch Size\n",
    "\n",
    "Batch size determines how many observations the model is shown before it calculates loss/error and updates the model weights via gradient descent. You're looking for a sweet spot here where you're showing it enough observations that you have enough information to updates the weights, but not such a large batch size that you don't get a lot of weight update iterations performed in a given epoch. Feed-forward Neural Networks aren't as sensitive to bach_size as other networks, but it is still an important hyperparameter to tune. Smaller batch sizes will also take longer to train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 888
    },
    "colab_type": "code",
    "id": "2smXfriNAGn7",
    "outputId": "a724a3a4-0f6b-422e-e613-2b4486d0cbcc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 5.1220 - acc: 0.6315\n",
      "Epoch 2/20\n",
      "768/768 [==============================] - 0s 146us/step - loss: 4.9426 - acc: 0.6302\n",
      "Epoch 3/20\n",
      "768/768 [==============================] - 0s 136us/step - loss: 4.4087 - acc: 0.6289\n",
      "Epoch 4/20\n",
      "768/768 [==============================] - 0s 133us/step - loss: 2.8173 - acc: 0.5964\n",
      "Epoch 5/20\n",
      "768/768 [==============================] - 0s 151us/step - loss: 1.8757 - acc: 0.5859\n",
      "Epoch 6/20\n",
      "768/768 [==============================] - 0s 150us/step - loss: 1.2367 - acc: 0.6263\n",
      "Epoch 7/20\n",
      "768/768 [==============================] - 0s 159us/step - loss: 1.0719 - acc: 0.6172\n",
      "Epoch 8/20\n",
      "768/768 [==============================] - 0s 159us/step - loss: 0.9703 - acc: 0.6380\n",
      "Epoch 9/20\n",
      "768/768 [==============================] - 0s 146us/step - loss: 0.8832 - acc: 0.6458\n",
      "Epoch 10/20\n",
      "768/768 [==============================] - 0s 146us/step - loss: 0.8324 - acc: 0.6445\n",
      "Epoch 11/20\n",
      "768/768 [==============================] - 0s 139us/step - loss: 0.8248 - acc: 0.6237\n",
      "Epoch 12/20\n",
      "768/768 [==============================] - 0s 153us/step - loss: 0.7473 - acc: 0.6445\n",
      "Epoch 13/20\n",
      "768/768 [==============================] - 0s 147us/step - loss: 0.7388 - acc: 0.6419\n",
      "Epoch 14/20\n",
      "768/768 [==============================] - 0s 137us/step - loss: 0.6897 - acc: 0.6458\n",
      "Epoch 15/20\n",
      "768/768 [==============================] - 0s 150us/step - loss: 0.6911 - acc: 0.6706\n",
      "Epoch 16/20\n",
      "768/768 [==============================] - 0s 140us/step - loss: 0.6818 - acc: 0.6589\n",
      "Epoch 17/20\n",
      "768/768 [==============================] - 0s 140us/step - loss: 0.7203 - acc: 0.6406\n",
      "Epoch 18/20\n",
      "768/768 [==============================] - 0s 154us/step - loss: 0.6278 - acc: 0.6914\n",
      "Epoch 19/20\n",
      "768/768 [==============================] - 0s 135us/step - loss: 0.6634 - acc: 0.6706\n",
      "Epoch 20/20\n",
      "768/768 [==============================] - 0s 148us/step - loss: 0.6219 - acc: 0.6992\n",
      "Best: 0.654947919693465 using {'batch_size': 10, 'epochs': 20}\n",
      "Means: 0.654947919693465, Stdev: 0.03226435201507085 with: {'batch_size': 10, 'epochs': 20}\n",
      "Means: 0.6132812529491881, Stdev: 0.005524270630446193 with: {'batch_size': 20, 'epochs': 20}\n",
      "Means: 0.5820312493791183, Stdev: 0.022326079145164223 with: {'batch_size': 40, 'epochs': 20}\n",
      "Means: 0.6171874959642688, Stdev: 0.04521810785973977 with: {'batch_size': 60, 'epochs': 20}\n",
      "Means: 0.5729166710128387, Stdev: 0.13457521442560474 with: {'batch_size': 80, 'epochs': 20}\n",
      "Means: 0.5091145851959785, Stdev: 0.11456853379592558 with: {'batch_size': 100, 'epochs': 20}\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.data.csv\", delimiter=\",\")\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(12, input_dim=8, activation='relu'))\n",
    "\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "                metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "# define the grid search parameters\n",
    "# batch_size = [10, 20, 40, 60, 80, 100]\n",
    "# param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "# define the grid search parameters\n",
    "param_grid = {'batch_size': [10, 20, 40, 60, 80, 100],\n",
    "              'epochs': [20]}\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, verbose = 0)\n",
    "grid_result = grid.fit(X, Y)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pmABfjlvXbqi"
   },
   "source": [
    "## Epochs\n",
    "\n",
    "The number of training epochs has a large and direct affect on the accuracy, However, more epochs is almost always goign to better than less epochs. This means that if you tune this parameter at the beginning and try and maintain the same value all throughout your training, you're going to be waiting a long time for each iteration of GridSearch. I suggest picking a fixed moderat # of epochs all throughout your training and then Grid Searching this parameter at the very end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 923
    },
    "colab_type": "code",
    "id": "bAmxP3N7TmFh",
    "outputId": "221f606f-145d-45b3-ac77-50e75507365b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "768/768 [==============================] - 1s 2ms/step - loss: 10.3792 - acc: 0.3490\n",
      "Epoch 2/20\n",
      "768/768 [==============================] - 0s 26us/step - loss: 10.3792 - acc: 0.3490\n",
      "Epoch 3/20\n",
      "768/768 [==============================] - 0s 24us/step - loss: 10.3792 - acc: 0.3490\n",
      "Epoch 4/20\n",
      "768/768 [==============================] - 0s 22us/step - loss: 10.3792 - acc: 0.3490\n",
      "Epoch 5/20\n",
      "768/768 [==============================] - 0s 23us/step - loss: 10.3792 - acc: 0.3490\n",
      "Epoch 6/20\n",
      "768/768 [==============================] - 0s 24us/step - loss: 10.3792 - acc: 0.3490\n",
      "Epoch 7/20\n",
      "768/768 [==============================] - 0s 22us/step - loss: 10.3792 - acc: 0.3490\n",
      "Epoch 8/20\n",
      "768/768 [==============================] - 0s 22us/step - loss: 10.3792 - acc: 0.3490\n",
      "Epoch 9/20\n",
      "768/768 [==============================] - 0s 23us/step - loss: 10.3792 - acc: 0.3490\n",
      "Epoch 10/20\n",
      "768/768 [==============================] - 0s 27us/step - loss: 10.3792 - acc: 0.3490\n",
      "Epoch 11/20\n",
      "768/768 [==============================] - 0s 28us/step - loss: 10.3792 - acc: 0.3490\n",
      "Epoch 12/20\n",
      "768/768 [==============================] - 0s 29us/step - loss: 10.3792 - acc: 0.3490\n",
      "Epoch 13/20\n",
      "768/768 [==============================] - 0s 30us/step - loss: 10.3792 - acc: 0.3490\n",
      "Epoch 14/20\n",
      "768/768 [==============================] - 0s 24us/step - loss: 10.3792 - acc: 0.3490\n",
      "Epoch 15/20\n",
      "768/768 [==============================] - 0s 24us/step - loss: 10.3792 - acc: 0.3490\n",
      "Epoch 16/20\n",
      "768/768 [==============================] - 0s 32us/step - loss: 10.3792 - acc: 0.3490\n",
      "Epoch 17/20\n",
      "768/768 [==============================] - 0s 38us/step - loss: 10.3792 - acc: 0.3490\n",
      "Epoch 18/20\n",
      "768/768 [==============================] - 0s 27us/step - loss: 10.3792 - acc: 0.3490\n",
      "Epoch 19/20\n",
      "768/768 [==============================] - 0s 26us/step - loss: 10.3792 - acc: 0.3490\n",
      "Epoch 20/20\n",
      "768/768 [==============================] - 0s 29us/step - loss: 10.3792 - acc: 0.3490\n",
      "Best: 0.6406250037252903 using {'batch_size': 80, 'epochs': 20}\n",
      "Means: 0.6406250037252903, Stdev: 0.025315395077631607 with: {'batch_size': 80, 'epochs': 20}\n",
      "Means: 0.6328125, Stdev: 0.030757845150308393 with: {'batch_size': 80, 'epochs': 40}\n",
      "Means: 0.6354166579743227, Stdev: 0.021236338020535706 with: {'batch_size': 80, 'epochs': 60}\n",
      "Means: 0.615885424738129, Stdev: 0.04342014660807894 with: {'batch_size': 100, 'epochs': 20}\n",
      "Means: 0.5924479252037903, Stdev: 0.0779512048088127 with: {'batch_size': 100, 'epochs': 40}\n",
      "Means: 0.6119791641831398, Stdev: 0.0036828522089767334 with: {'batch_size': 100, 'epochs': 60}\n"
     ]
    }
   ],
   "source": [
    "# define the grid search parameters\n",
    "param_grid = {'batch_size': [80, 100],\n",
    "              'epochs': [20, 40, 60]}\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, verbose=0)\n",
    "grid_result = grid.fit(X, Y)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EKcuY6OiaLfz"
   },
   "source": [
    "## Optimizer\n",
    "\n",
    "Remember that there's a different optimizers [optimizers](https://keras.io/optimizers/). At some point, take some time to read up on them a little bit. \"adam\" usually gives the best results. The thing to know about choosing an optimizer is that different optimizers have different hyperparameters like learning rate, momentum, etc. So based on the optimizer you choose you might also have to tune the learning rate and momentum of those optimizers after that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DG3wq5iOaLig"
   },
   "source": [
    "## Learning Rate\n",
    "\n",
    "Remember that the Learning Rate is a hyperparameter that is specific to your gradient-descent based optimizer selection. A learning rate that is too high will cause divergent behavior, but a Learning Rate that is too low will fail to converge, again, you're looking for the sweet spot. I would start out tuning learning rates by orders of magnitude: [.001, .01, .1, .2, .3, .5] etc. I wouldn't go above .5, but you can try it and see what the behavior is like. \n",
    "\n",
    "Once you have narrowed it down, make the window even smaller and try it again. If after running the above specification your model reports that .1 is the best optimizer, then you should probably try things like [.05, .08, .1, .12, .15] to try and narrow it down. \n",
    "\n",
    "It can also be good to tune the number of epochs in combination with the learning rate since the number of iterations that you allow the learning rate to reach the minimum can determine if you have let it run long enough to converge to the minimum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zIIEnsa4MvvH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gNTBUWd1aLlA"
   },
   "source": [
    "## Momentum\n",
    "\n",
    "Momentum is a hyperparameter that is more commonly associated with Stochastic Gradient Descent. SGD is a common optimizer because it's what people understand and know, but I doubt it will get you the best results, you can try hyperparameter tuning its attributes and see if you can beat the performance from adam. Momentum is a property that decides the willingness of an optimizer to overshoot the minimum. Imagine a ball rolling down one side of a bowl and then up the opposite side a little bit before settling back to the bottom. The purpose of momentum is to try and escale local minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xnEG-bCJaLnZ"
   },
   "source": [
    "## Activation Functions\n",
    "\n",
    "We've talked about this a little bit, typically you'l want to use ReLU for hidden layers and either Sigmoid, or Softmax for output layers of binary and multi-class classification implementations respectively, but try other activation functions and see if you can get any better results with sigmoid or tanh or something. There are a lot of activation functions that we haven't really talked about. Maybe you'll get good results with them. Maybe you won't. :) <https://keras.io/activations/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jbdAMGYKJQBA"
   },
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(12, input_dim=8, activation=activation))\n",
    "\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "                metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "# define the grid search parameters\n",
    "# batch_size = [10, 20, 40, 60, 80, 100]\n",
    "# param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "# define the grid search parameters\n",
    "param_grid = {'batch_size': [10, 20, 40, 60, 80, 100],\n",
    "              'epochs': [20]}\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, verbose = 0)\n",
    "grid_result = grid.fit(X, Y)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oul9sPq-dU-h"
   },
   "source": [
    "## Network Weight Initialization\n",
    "\n",
    "You saw how big of an effect the way that we initialize our network's weights can have on our results. There are **a lot** of what are called initialization modes. I don't understand all of them, but they can have a big affect on your model's initial accuracy. Your model will get further with less epochs if you initialize it with weights that are well suited to the problem you're trying to solve.\n",
    "\n",
    "`init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bqtEuxeQaLqE"
   },
   "source": [
    "## Dropout Regularization and the Weight Constraint\n",
    "\n",
    "the Dropout Regularization value is a percentage of neurons that you want to be randomly deactivated during training. The weight constraint is a second regularization parameter that works in tandem with dropout regularization. You should tune these two values at the same time. \n",
    "\n",
    "Using dropout on visible vs hidden layers might have a different effect. Using dropout on hidden layers might not have any effect while using dropout on hidden layers might have a substantial effect. You don't necessarily need to turn use dropout unless you see that your model has overfitting and generalizability problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P2c5Cv6oaLtO"
   },
   "source": [
    "## Neurons in Hidden Layer \n",
    "\n",
    "Remember that when we only had a single perceptron our model was only able to fit to linearly separable data, but as we have added layers and nodes to those layers our network has become a powerhouse of fitting nonlinearity in data. The larger the network and the more nodes generally the stronger the network's capacity to fit nonlinear patterns in data. The more nodes and layers the longer it will take to train a network, and higher the probability of overfitting. The larger your network gets the more you'll need dropout regularization or other regularization techniques to keep it in check. \n",
    "\n",
    "Typically depth (more layers) is more important than width (more nodes) for neural networks. This is part of why Deep Learning is so highly touted. Certain deep learning architectures have truly been huge breakthroughs for certain machine learning tasks. \n",
    "\n",
    "You might borrow ideas from other network architectures. For example if I was doing image recognition and I wasn't taking cues from state of the art architectures like resnet, alexnet, googlenet, etc. Then I'm probably going to have to do a lot more experimentation on my own before I find something that works.\n",
    "\n",
    "There are some heuristics, but I am highly skeptical of them. I think you're better off experimenting on your own and forming your own intuition for these kinds of problems. \n",
    "\n",
    "- https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3LvRYQOwd-aZ"
   },
   "source": [
    "# Hyperparameter Tuning Tips:\n",
    "\n",
    "1) Use **at least** 3-fold cross validation. 5-fold or 10-fold would be better if you're patient enough.\n",
    "\n",
    "2) Sampling your dataset can speed up iterations if you have enough data to justify it.\n",
    "\n",
    "3) Start with \"Coarse Grids\" we gave an example of this when we were talking about Learning Rate, start with wide gaps between values that you're testing and then run additional tests with more narrow gaps between values until you find an optimum.\n",
    "\n",
    "4) Even though we're using Cross Validation, reproducibility can still be a challenge. Don't be afraid to run some tests multiple times to see if you get the same results. Sometimes the accuracy differences are so slight that grid search picking one hyperparameter over another is just luck of the draw.\n",
    "\n",
    "5) Run your tests in verbose mode. Not only will you be less stressed out wondering if it's actually running, but you will be able to more quickly identify if something has gone haywire by glancing at the output every now and again.\n",
    "\n",
    "6) Pick a moderate amount of epochs and stick with that all throughout your training, until the very end, then tune your epochs. \n",
    "\n",
    "7) As you begin to have a lot of hyperparameters narrowed down, test them along with narrow versions of other hyperparameters in order to catch the small effects of interactions between specific hyperparameters.\n",
    "\n",
    "8) Being a perfectionist here is going to be a long and painful journey, so please do a cost-benefit analysis of how you're using your time here. Learn the concept, have the experience, but don't spend 3 days hyperparameter tuning your assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z2cC0lpAhDQ_"
   },
   "source": [
    "## Additional Reading:\n",
    "\n",
    "I know I sound like a shill for Jason Brownlee, but honestly he has some of the very best articles on the internet in regards to hyperparameter tuning with Keras. I have leaned on his articles heavily when doing hyperparameter tuning before and saw great results from following his advice. He has articles on lots and lots of Keras related topics, so keep an ey out for his website: <http://machinelearningmastery.com> as you look through search results today.\n",
    "\n",
    "- https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n",
    "- https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/\n",
    "- https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "- https://machinelearningmastery.com/introduction-to-weight-constraints-to-reduce-generalization-error-in-deep-learning/\n",
    "- https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of LS_DS2_434_Hyperparameter_Tuning.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
