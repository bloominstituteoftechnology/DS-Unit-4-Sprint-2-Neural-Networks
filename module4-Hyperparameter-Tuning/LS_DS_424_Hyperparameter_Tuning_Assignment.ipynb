{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Ryp-TVm4njD"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 4*\n",
    "\n",
    "## Your Mission, should you choose to accept it...\n",
    "\n",
    "To hyperparameter tune and extract every ounce of accuracy out of this telecom customer churn dataset: <https://lambdaschool-data-science.s3.amazonaws.com/telco-churn/WA_Fn-UseC_-Telco-Customer-Churn+(1).csv> \n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Load the data\n",
    "- Clean the data if necessary (it will be)\n",
    "- Create and fit a baseline Keras MLP model to the data.\n",
    "- Hyperparameter tune (at least) the following parameters:\n",
    " - batch_size\n",
    " - training epochs\n",
    " - optimizer\n",
    " - learning rate (if applicable to optimizer)\n",
    " - momentum (if applicable to optimizer)\n",
    " - activation functions\n",
    " - network weight initialization\n",
    " - dropout regularization\n",
    " - number of neurons in the hidden layer\n",
    " \n",
    " You must use Grid Search and Cross Validation for your initial pass of the above hyperparameters\n",
    " \n",
    " Try and get the maximum accuracy possible out of this data! You'll save big telecoms millions! Doesn't that sound great?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(7043, 21)\n"
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>customerID</th>\n      <th>gender</th>\n      <th>SeniorCitizen</th>\n      <th>Partner</th>\n      <th>Dependents</th>\n      <th>tenure</th>\n      <th>PhoneService</th>\n      <th>MultipleLines</th>\n      <th>InternetService</th>\n      <th>OnlineSecurity</th>\n      <th>...</th>\n      <th>DeviceProtection</th>\n      <th>TechSupport</th>\n      <th>StreamingTV</th>\n      <th>StreamingMovies</th>\n      <th>Contract</th>\n      <th>PaperlessBilling</th>\n      <th>PaymentMethod</th>\n      <th>MonthlyCharges</th>\n      <th>TotalCharges</th>\n      <th>Churn</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7590-VHVEG</td>\n      <td>Female</td>\n      <td>0</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>1</td>\n      <td>No</td>\n      <td>No phone service</td>\n      <td>DSL</td>\n      <td>No</td>\n      <td>...</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>Month-to-month</td>\n      <td>Yes</td>\n      <td>Electronic check</td>\n      <td>29.85</td>\n      <td>29.85</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5575-GNVDE</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>No</td>\n      <td>No</td>\n      <td>34</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>DSL</td>\n      <td>Yes</td>\n      <td>...</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>One year</td>\n      <td>No</td>\n      <td>Mailed check</td>\n      <td>56.95</td>\n      <td>1889.5</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3668-QPYBK</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>No</td>\n      <td>No</td>\n      <td>2</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>DSL</td>\n      <td>Yes</td>\n      <td>...</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>Month-to-month</td>\n      <td>Yes</td>\n      <td>Mailed check</td>\n      <td>53.85</td>\n      <td>108.15</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7795-CFOCW</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>No</td>\n      <td>No</td>\n      <td>45</td>\n      <td>No</td>\n      <td>No phone service</td>\n      <td>DSL</td>\n      <td>Yes</td>\n      <td>...</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>No</td>\n      <td>One year</td>\n      <td>No</td>\n      <td>Bank transfer (automatic)</td>\n      <td>42.30</td>\n      <td>1840.75</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9237-HQITU</td>\n      <td>Female</td>\n      <td>0</td>\n      <td>No</td>\n      <td>No</td>\n      <td>2</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>Fiber optic</td>\n      <td>No</td>\n      <td>...</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>Month-to-month</td>\n      <td>Yes</td>\n      <td>Electronic check</td>\n      <td>70.70</td>\n      <td>151.65</td>\n      <td>Yes</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 21 columns</p>\n</div>",
      "text/plain": "   customerID  gender  SeniorCitizen Partner Dependents  tenure PhoneService  \\\n0  7590-VHVEG  Female              0     Yes         No       1           No   \n1  5575-GNVDE    Male              0      No         No      34          Yes   \n2  3668-QPYBK    Male              0      No         No       2          Yes   \n3  7795-CFOCW    Male              0      No         No      45           No   \n4  9237-HQITU  Female              0      No         No       2          Yes   \n\n      MultipleLines InternetService OnlineSecurity  ... DeviceProtection  \\\n0  No phone service             DSL             No  ...               No   \n1                No             DSL            Yes  ...              Yes   \n2                No             DSL            Yes  ...               No   \n3  No phone service             DSL            Yes  ...              Yes   \n4                No     Fiber optic             No  ...               No   \n\n  TechSupport StreamingTV StreamingMovies        Contract PaperlessBilling  \\\n0          No          No              No  Month-to-month              Yes   \n1          No          No              No        One year               No   \n2          No          No              No  Month-to-month              Yes   \n3         Yes          No              No        One year               No   \n4          No          No              No  Month-to-month              Yes   \n\n               PaymentMethod MonthlyCharges  TotalCharges Churn  \n0           Electronic check          29.85         29.85    No  \n1               Mailed check          56.95        1889.5    No  \n2               Mailed check          53.85        108.15   Yes  \n3  Bank transfer (automatic)          42.30       1840.75    No  \n4           Electronic check          70.70        151.65   Yes  \n\n[5 rows x 21 columns]"
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "url = 'https://lambdaschool-data-science.s3.amazonaws.com/telco-churn/WA_Fn-UseC_-Telco-Customer-Churn+(1).csv'\n",
    "df = pd.read_csv(url)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle_data(df):\n",
    "    \"\"\" Wrangle function for cleaning and factorizing dataframe\"\"\"\n",
    "\n",
    "    # Dropping unecessary column\n",
    "    df.drop(['customerID'], axis=1, inplace=True)\n",
    "\n",
    "    # Factorizing and renaming columns\n",
    "    col_list = ['MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', \n",
    "                'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', \n",
    "                'Contract', 'PaymentMethod'] \n",
    "\n",
    "    for col in col_list:\n",
    "        df[col] = pd.factorize(df[col])[0] + 1\n",
    "\n",
    "    df = pd.get_dummies(\n",
    "        df, prefix=['gender', 'Partner', 'Dependents', 'PhoneService', 'PaperlessBilling', 'Churn'],\n",
    "        columns=['gender', 'Partner', 'Dependents', 'PhoneService', 'PaperlessBilling', 'Churn'],\n",
    "        drop_first=True)\n",
    "\n",
    "    df = df.rename(\n",
    "        columns={'gender_Male':'gender', 'Partner_Yes':'Partner', \n",
    "        'Dependents_Yes':'Dependents', 'PhoneService_Yes':'PhoneService', \n",
    "        'PaperlessBilling_Yes':'PaperlessBilling', 'Churn_Yes':'Churn'})\n",
    "\n",
    "    # Setting 'TotalCharges' to float instead of object\n",
    "    df['TotalCharges'] = pd.to_numeric(df['TotalCharges'],errors='coerce')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SeniorCitizen</th>\n      <th>tenure</th>\n      <th>MultipleLines</th>\n      <th>InternetService</th>\n      <th>OnlineSecurity</th>\n      <th>OnlineBackup</th>\n      <th>DeviceProtection</th>\n      <th>TechSupport</th>\n      <th>StreamingTV</th>\n      <th>StreamingMovies</th>\n      <th>Contract</th>\n      <th>PaymentMethod</th>\n      <th>MonthlyCharges</th>\n      <th>TotalCharges</th>\n      <th>gender</th>\n      <th>Partner</th>\n      <th>Dependents</th>\n      <th>PhoneService</th>\n      <th>PaperlessBilling</th>\n      <th>Churn</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>29.85</td>\n      <td>29.85</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>34</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>56.95</td>\n      <td>1889.50</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>53.85</td>\n      <td>108.15</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>45</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>3</td>\n      <td>42.30</td>\n      <td>1840.75</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>70.70</td>\n      <td>151.65</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   SeniorCitizen  tenure  MultipleLines  InternetService  OnlineSecurity  \\\n0              0       1              1                1               1   \n1              0      34              2                1               2   \n2              0       2              2                1               2   \n3              0      45              1                1               2   \n4              0       2              2                2               1   \n\n   OnlineBackup  DeviceProtection  TechSupport  StreamingTV  StreamingMovies  \\\n0             1                 1            1            1                1   \n1             2                 2            1            1                1   \n2             1                 1            1            1                1   \n3             2                 2            2            1                1   \n4             2                 1            1            1                1   \n\n   Contract  PaymentMethod  MonthlyCharges  TotalCharges  gender  Partner  \\\n0         1              1           29.85         29.85       0        1   \n1         2              2           56.95       1889.50       1        0   \n2         1              2           53.85        108.15       1        0   \n3         2              3           42.30       1840.75       1        0   \n4         1              1           70.70        151.65       0        0   \n\n   Dependents  PhoneService  PaperlessBilling  Churn  \n0           0             0                 1      0  \n1           0             1                 0      0  \n2           0             1                 1      1  \n3           0             0                 0      0  \n4           0             1                 1      1  "
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = wrangle_data(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(5634, 19) (5634,) (1409, 19) (1409,)\n"
    },
    {
     "data": {
      "text/plain": "array([-4.37749204e-01, -4.65683364e-01, -5.07614974e-01, -1.18234670e+00,\n        3.55775250e-01,  1.81845392e-01,  2.90697062e-01, -9.05426063e-01,\n       -1.07437424e+00,  2.35570413e-01,  3.72908354e-01, -2.78517888e-01,\n       -4.73723375e-04, -4.21730250e-01, -1.02516569e+00, -9.69578591e-01,\n        1.53218588e+00,  3.29573443e-01, -1.20000722e+00])"
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Setting target and features\n",
    "target = 'Churn'\n",
    "\n",
    "# Dropping 'result' and 'age' as they seem to be confounding and not helpful\n",
    "features = df.columns.drop([target])\n",
    "\n",
    "# Setting X and y\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Checking data\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train on 5634 samples, validate on 1409 samples\nEpoch 1/75\n5634/5634 [==============================] - 1s 170us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 2/75\n5634/5634 [==============================] - 1s 108us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 3/75\n5634/5634 [==============================] - 1s 110us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 4/75\n5634/5634 [==============================] - 1s 111us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 5/75\n5634/5634 [==============================] - 1s 110us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 6/75\n5634/5634 [==============================] - 1s 112us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 7/75\n5634/5634 [==============================] - 1s 110us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 8/75\n5634/5634 [==============================] - 1s 110us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 9/75\n5634/5634 [==============================] - 1s 109us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 10/75\n5634/5634 [==============================] - 1s 112us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 11/75\n5634/5634 [==============================] - 1s 110us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 12/75\n5634/5634 [==============================] - 1s 112us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 13/75\n5634/5634 [==============================] - 1s 108us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 14/75\n5634/5634 [==============================] - 1s 108us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 15/75\n5634/5634 [==============================] - 1s 105us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 16/75\n5634/5634 [==============================] - 1s 113us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 17/75\n5634/5634 [==============================] - 1s 106us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 18/75\n5634/5634 [==============================] - 1s 112us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 19/75\n5634/5634 [==============================] - 1s 110us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 20/75\n5634/5634 [==============================] - 1s 110us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 21/75\n5634/5634 [==============================] - 1s 109us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 22/75\n5634/5634 [==============================] - 1s 111us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 23/75\n5634/5634 [==============================] - 1s 112us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 24/75\n5634/5634 [==============================] - 1s 112us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 25/75\n5634/5634 [==============================] - 1s 113us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 26/75\n5634/5634 [==============================] - 1s 118us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 27/75\n5634/5634 [==============================] - 1s 107us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 28/75\n5634/5634 [==============================] - 1s 109us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 29/75\n5634/5634 [==============================] - 1s 106us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 30/75\n5634/5634 [==============================] - 1s 106us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 31/75\n5634/5634 [==============================] - 1s 114us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 32/75\n5634/5634 [==============================] - 1s 111us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 33/75\n5634/5634 [==============================] - 1s 110us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 34/75\n5634/5634 [==============================] - 1s 108us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 35/75\n5634/5634 [==============================] - 1s 109us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 36/75\n5634/5634 [==============================] - 1s 113us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 37/75\n5634/5634 [==============================] - 1s 111us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 38/75\n5634/5634 [==============================] - 1s 109us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 39/75\n5634/5634 [==============================] - 1s 108us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 40/75\n5634/5634 [==============================] - 1s 106us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 41/75\n5634/5634 [==============================] - 1s 106us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 42/75\n5634/5634 [==============================] - 1s 110us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 43/75\n5634/5634 [==============================] - 1s 112us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 44/75\n5634/5634 [==============================] - 1s 110us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 45/75\n5634/5634 [==============================] - 1s 107us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 46/75\n5634/5634 [==============================] - 1s 107us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 47/75\n5634/5634 [==============================] - 1s 115us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 48/75\n5634/5634 [==============================] - 1s 121us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 49/75\n5634/5634 [==============================] - 1s 120us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 50/75\n5634/5634 [==============================] - 1s 118us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 51/75\n5634/5634 [==============================] - 1s 118us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 52/75\n5634/5634 [==============================] - 1s 113us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 53/75\n5634/5634 [==============================] - 1s 121us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 54/75\n5634/5634 [==============================] - 1s 122us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 55/75\n5634/5634 [==============================] - 1s 117us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 56/75\n5634/5634 [==============================] - 1s 119us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 57/75\n5634/5634 [==============================] - 1s 122us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 58/75\n5634/5634 [==============================] - 1s 118us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 59/75\n5634/5634 [==============================] - 1s 116us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 60/75\n5634/5634 [==============================] - 1s 118us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 61/75\n5634/5634 [==============================] - 1s 116us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 62/75\n5634/5634 [==============================] - 1s 117us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 63/75\n5634/5634 [==============================] - 1s 115us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 64/75\n5634/5634 [==============================] - 1s 121us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 65/75\n5634/5634 [==============================] - 1s 116us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 66/75\n5634/5634 [==============================] - 1s 116us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 67/75\n5634/5634 [==============================] - 1s 115us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 68/75\n5634/5634 [==============================] - 1s 120us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 69/75\n5634/5634 [==============================] - 1s 122us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 70/75\n5634/5634 [==============================] - 1s 120us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 71/75\n5634/5634 [==============================] - 1s 118us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 72/75\n5634/5634 [==============================] - 1s 119us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 73/75\n5634/5634 [==============================] - 1s 120us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 74/75\n5634/5634 [==============================] - 1s 116us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\nEpoch 75/75\n5634/5634 [==============================] - 1s 119us/sample - loss: nan - mse: nan - mae: nan - val_loss: nan - val_mse: nan - val_mae: nan\n"
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x2944c2a3508>"
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Important Hyperparameters\n",
    "inputs = X_train.shape[1]\n",
    "epochs = 75\n",
    "batch_size = 10\n",
    "\n",
    "\n",
    "# Create Model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(inputs,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse', 'mae'])\n",
    "\n",
    "# Fit Model\n",
    "model.fit(X_train, y_train, \n",
    "          validation_data=(X_test,y_test), \n",
    "          epochs=epochs, \n",
    "          batch_size=batch_size\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using GridSearchCV to tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Best: 0.7346286535263061 using {'batch_size': 10, 'epochs': 20}\nMeans: 0.7346286535263061, Stdev: 0.004757632470334884 with: {'batch_size': 10, 'epochs': 20}\nMeans: 0.7346286535263061, Stdev: 0.004757632470334884 with: {'batch_size': 20, 'epochs': 20}\nMeans: 0.7346286535263061, Stdev: 0.004757632470334884 with: {'batch_size': 40, 'epochs': 20}\nMeans: 0.7346286535263061, Stdev: 0.004757632470334884 with: {'batch_size': 60, 'epochs': 20}\nMeans: 0.7346286535263061, Stdev: 0.004757632470334884 with: {'batch_size': 80, 'epochs': 20}\nMeans: 0.7346286535263061, Stdev: 0.004757632470334884 with: {'batch_size': 100, 'epochs': 20}\n"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Load data\n",
    "url = 'https://lambdaschool-data-science.s3.amazonaws.com/telco-churn/WA_Fn-UseC_-Telco-Customer-Churn+(1).csv'\n",
    "data = pd.read_csv(url)\n",
    "data = wrangle_data(data)\n",
    "# data.shape\n",
    "# data.head()\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "X = data.iloc[:, 0:18]\n",
    "y = data.iloc[:, 19]\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=18, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "param_grid = {'batch_size': [10, 20, 40, 60, 80, 100],\n",
    "              'epochs': [20]}\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Trying with different params + optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Best: 0.7346286535263061 using {'batch_size': 20, 'epochs': 20}\nMeans: 0.7346286535263061, Stdev: 0.004757632470334884 with: {'batch_size': 20, 'epochs': 20}\nMeans: 0.7346286535263061, Stdev: 0.004757632470334884 with: {'batch_size': 20, 'epochs': 40}\nMeans: 0.7346286535263061, Stdev: 0.004757632470334884 with: {'batch_size': 20, 'epochs': 60}\nMeans: 0.7346286535263061, Stdev: 0.004757632470334884 with: {'batch_size': 20, 'epochs': 80}\nMeans: 0.7346286535263061, Stdev: 0.004757632470334884 with: {'batch_size': 20, 'epochs': 100}\nMeans: 0.7346286535263061, Stdev: 0.004757632470334884 with: {'batch_size': 20, 'epochs': 200}\nMeans: 0.7346286535263061, Stdev: 0.004757632470334884 with: {'batch_size': 20, 'epochs': 300}\n"
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "def optimized_model():\n",
    "    # Create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=18, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Optimizer\n",
    "    # sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer='sgd',\n",
    "        metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=optimized_model, verbose=0)\n",
    "\n",
    "\n",
    "# define the grid search parameters\n",
    "param_grid = {'batch_size': [20],\n",
    "              'epochs': [20, 40, 60, 80, 100, 200, 300]}\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FfZRtJ7MCN3x"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Try to implement Random Search Hyperparameter Tuning on this dataset\n",
    "- Try to implement Bayesian Optimiation tuning on this dataset using hyperas or hyperopt (if you're brave)\n",
    "- Practice hyperparameter tuning other datasets that we have looked at. How high can you get MNIST? Above 99%?\n",
    "- Study for the Sprint Challenge\n",
    " - Can you implement both perceptron and MLP models from scratch with forward and backpropagation?\n",
    " - Can you implement both perceptron and MLP models in keras and tune their hyperparameters with cross validation?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "LS_DS_434_Hyperparameter_Tuning_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python (NN)",
   "language": "python",
   "name": "python-nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}