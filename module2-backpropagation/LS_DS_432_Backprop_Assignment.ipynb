{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NGGrt9EYlCqY"
   },
   "source": [
    "# Backpropagation Practice\n",
    "\n",
    "Implement a 3 input, 4 node hidden-layer, 1 output node Multilayer Perceptron on the following dataset:\n",
    "\n",
    "| x1 | x2 | x3 | y |\n",
    "|----|----|----|---|\n",
    "| 0  | 0  | 1  | 0 |\n",
    "| 0  | 1  | 1  | 1 |\n",
    "| 1  | 0  | 1  | 1 |\n",
    "| 0  | 1  | 0  | 1 |\n",
    "| 1  | 0  | 0  | 1 |\n",
    "| 1  | 1  | 1  | 0 |\n",
    "| 0  | 0  | 0  | 0 |\n",
    "\n",
    "If you look at the data you'll notice that the first two columns behave like an XOR gate while the last column is mostly just noise. Remember that creating an XOR gate was what the perceptron was criticized for not being able to learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0,1,1], \n",
    "               [0,1,1,1], \n",
    "               [1,0,1,1], \n",
    "               [0,1,0,1],\n",
    "               [1,0,0,1],\n",
    "               [1,1,1,1],\n",
    "               [0,0,0,1]\n",
    "              ])\n",
    "y = np.array([[0],[1],[1],[1],[1],[0],[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nEREYT-3wI1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------- EPOCH 1 -----------+\n",
      "Input: \n",
      " [[0 0 1 1]\n",
      " [0 1 1 1]\n",
      " [1 0 1 1]\n",
      " [0 1 0 1]\n",
      " [1 0 0 1]\n",
      " [1 1 1 1]\n",
      " [0 0 0 1]]\n",
      "Actual Output: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "Predicted Output: \n",
      "[[0.38437549]\n",
      " [0.43509213]\n",
      " [0.37539768]\n",
      " [0.46105702]\n",
      " [0.39715178]\n",
      " [0.42380196]\n",
      " [0.40795155]]\n",
      "Loss: \n",
      "0.26527307768567193\n",
      "\n",
      "\n",
      "+---------- EPOCH 2 -----------+\n",
      "Input: \n",
      " [[0 0 1 1]\n",
      " [0 1 1 1]\n",
      " [1 0 1 1]\n",
      " [0 1 0 1]\n",
      " [1 0 0 1]\n",
      " [1 1 1 1]\n",
      " [0 0 0 1]]\n",
      "Actual Output: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "Predicted Output: \n",
      "[[0.44750743]\n",
      " [0.48544965]\n",
      " [0.46547912]\n",
      " [0.53085521]\n",
      " [0.5104323 ]\n",
      " [0.50198359]\n",
      " [0.49330093]]\n",
      "Loss: \n",
      "0.24369203247795088\n",
      "\n",
      "\n",
      "+---------- EPOCH 3 -----------+\n",
      "Input: \n",
      " [[0 0 1 1]\n",
      " [0 1 1 1]\n",
      " [1 0 1 1]\n",
      " [0 1 0 1]\n",
      " [1 0 0 1]\n",
      " [1 1 1 1]\n",
      " [0 0 0 1]]\n",
      "Actual Output: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "Predicted Output: \n",
      "[[0.47943535]\n",
      " [0.5114311 ]\n",
      " [0.51435195]\n",
      " [0.56944445]\n",
      " [0.57327779]\n",
      " [0.54427825]\n",
      " [0.53970182]]\n",
      "Loss: \n",
      "0.23705695155234988\n",
      "\n",
      "\n",
      "+---------- EPOCH 4 -----------+\n",
      "Input: \n",
      " [[0 0 1 1]\n",
      " [0 1 1 1]\n",
      " [1 0 1 1]\n",
      " [0 1 0 1]\n",
      " [1 0 0 1]\n",
      " [1 1 1 1]\n",
      " [0 0 0 1]]\n",
      "Actual Output: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "Predicted Output: \n",
      "[[0.4920777 ]\n",
      " [0.52301085]\n",
      " [0.53696301]\n",
      " [0.58956066]\n",
      " [0.60467503]\n",
      " [0.5650305 ]\n",
      " [0.5619877 ]]\n",
      "Loss: \n",
      "0.2348420423967196\n",
      "\n",
      "\n",
      "+---------- EPOCH 5 -----------+\n",
      "Input: \n",
      " [[0 0 1 1]\n",
      " [0 1 1 1]\n",
      " [1 0 1 1]\n",
      " [0 1 0 1]\n",
      " [1 0 0 1]\n",
      " [1 1 1 1]\n",
      " [0 0 0 1]]\n",
      "Actual Output: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "Predicted Output: \n",
      "[[0.49530085]\n",
      " [0.52758594]\n",
      " [0.54672567]\n",
      " [0.60077224]\n",
      " [0.62109449]\n",
      " [0.57534964]\n",
      " [0.57258551]]\n",
      "Loss: \n",
      "0.23368416570455883\n",
      "\n",
      "\n",
      "+---------- EPOCH 50 -----------+\n",
      "Input: \n",
      " [[0 0 1 1]\n",
      " [0 1 1 1]\n",
      " [1 0 1 1]\n",
      " [0 1 0 1]\n",
      " [1 0 0 1]\n",
      " [1 1 1 1]\n",
      " [0 0 0 1]]\n",
      "Actual Output: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "Predicted Output: \n",
      "[[0.30615085]\n",
      " [0.54515163]\n",
      " [0.58088775]\n",
      " [0.68174457]\n",
      " [0.73780608]\n",
      " [0.61825606]\n",
      " [0.51258904]]\n",
      "Loss: \n",
      "0.18447010098646155\n",
      "\n",
      "\n",
      "+---------- EPOCH 100 -----------+\n",
      "Input: \n",
      " [[0 0 1 1]\n",
      " [0 1 1 1]\n",
      " [1 0 1 1]\n",
      " [0 1 0 1]\n",
      " [1 0 0 1]\n",
      " [1 1 1 1]\n",
      " [0 0 0 1]]\n",
      "Actual Output: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "Predicted Output: \n",
      "[[0.1105777 ]\n",
      " [0.70258827]\n",
      " [0.7504293 ]\n",
      " [0.82185961]\n",
      " [0.83904858]\n",
      " [0.40781659]\n",
      " [0.23229309]]\n",
      "Loss: \n",
      "0.06298292984660818\n",
      "\n",
      "\n",
      "+---------- EPOCH 150 -----------+\n",
      "Input: \n",
      " [[0 0 1 1]\n",
      " [0 1 1 1]\n",
      " [1 0 1 1]\n",
      " [0 1 0 1]\n",
      " [1 0 0 1]\n",
      " [1 1 1 1]\n",
      " [0 0 0 1]]\n",
      "Actual Output: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "Predicted Output: \n",
      "[[0.05604848]\n",
      " [0.83344984]\n",
      " [0.85461825]\n",
      " [0.90284063]\n",
      " [0.89666229]\n",
      " [0.23055759]\n",
      " [0.11116585]]\n",
      "Loss: \n",
      "0.01966421684979581\n",
      "\n",
      "\n",
      "+---------- EPOCH 200 -----------+\n",
      "Input: \n",
      " [[0 0 1 1]\n",
      " [0 1 1 1]\n",
      " [1 0 1 1]\n",
      " [0 1 0 1]\n",
      " [1 0 0 1]\n",
      " [1 1 1 1]\n",
      " [0 0 0 1]]\n",
      "Actual Output: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "Predicted Output: \n",
      "[[0.03920721]\n",
      " [0.88400466]\n",
      " [0.89643672]\n",
      " [0.93117113]\n",
      " [0.92322023]\n",
      " [0.16179523]\n",
      " [0.07417514]]\n",
      "Loss: \n",
      "0.009718524340119052\n",
      "\n",
      "\n",
      "+---------- EPOCH 250 -----------+\n",
      "Input: \n",
      " [[0 0 1 1]\n",
      " [0 1 1 1]\n",
      " [1 0 1 1]\n",
      " [0 1 0 1]\n",
      " [1 0 0 1]\n",
      " [1 1 1 1]\n",
      " [0 0 0 1]]\n",
      "Actual Output: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "Predicted Output: \n",
      "[[0.03109274]\n",
      " [0.90859458]\n",
      " [0.91745537]\n",
      " [0.94490343]\n",
      " [0.93742336]\n",
      " [0.12821939]\n",
      " [0.05707692]]\n",
      "Loss: \n",
      "0.0061121113844206385\n",
      "\n",
      "\n",
      "+---------- EPOCH 300 -----------+\n",
      "Input: \n",
      " [[0 0 1 1]\n",
      " [0 1 1 1]\n",
      " [1 0 1 1]\n",
      " [0 1 0 1]\n",
      " [1 0 0 1]\n",
      " [1 1 1 1]\n",
      " [0 0 0 1]]\n",
      "Actual Output: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "Predicted Output: \n",
      "[[0.02624128]\n",
      " [0.92316019]\n",
      " [0.9301285 ]\n",
      " [0.95310613]\n",
      " [0.94627814]\n",
      " [0.10825467]\n",
      " [0.0471791 ]]\n",
      "Loss: \n",
      "0.004357857507556325\n",
      "\n",
      "\n",
      "+---------- EPOCH 350 -----------+\n",
      "Input: \n",
      " [[0 0 1 1]\n",
      " [0 1 1 1]\n",
      " [1 0 1 1]\n",
      " [0 1 0 1]\n",
      " [1 0 0 1]\n",
      " [1 1 1 1]\n",
      " [0 0 0 1]]\n",
      "Actual Output: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "Predicted Output: \n",
      "[[0.02297152]\n",
      " [0.93288346]\n",
      " [0.93868204]\n",
      " [0.95863377]\n",
      " [0.9523839 ]\n",
      " [0.09488249]\n",
      " [0.04067043]]\n",
      "Loss: \n",
      "0.003346777362449586\n",
      "\n",
      "\n",
      "+---------- EPOCH 400 -----------+\n",
      "Input: \n",
      " [[0 0 1 1]\n",
      " [0 1 1 1]\n",
      " [1 0 1 1]\n",
      " [0 1 0 1]\n",
      " [1 0 0 1]\n",
      " [1 1 1 1]\n",
      " [0 0 0 1]]\n",
      "Actual Output: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "Predicted Output: \n",
      "[[0.02059517]\n",
      " [0.93989439]\n",
      " [0.94489642]\n",
      " [0.96265438]\n",
      " [0.95688784]\n",
      " [0.08521259]\n",
      " [0.03603082]]\n",
      "Loss: \n",
      "0.002698001448920184\n",
      "\n",
      "\n",
      "+---------- EPOCH 450 -----------+\n",
      "Input: \n",
      " [[0 0 1 1]\n",
      " [0 1 1 1]\n",
      " [1 0 1 1]\n",
      " [0 1 0 1]\n",
      " [1 0 0 1]\n",
      " [1 1 1 1]\n",
      " [0 0 0 1]]\n",
      "Actual Output: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "Predicted Output: \n",
      "[[0.01877665]\n",
      " [0.94522583]\n",
      " [0.9496486 ]\n",
      " [0.96573567]\n",
      " [0.96037195]\n",
      " [0.07784063]\n",
      " [0.03253557]]\n",
      "Loss: \n",
      "0.0022500269139106035\n",
      "\n",
      "\n",
      "+---------- EPOCH 500 -----------+\n",
      "Input: \n",
      " [[0 0 1 1]\n",
      " [0 1 1 1]\n",
      " [1 0 1 1]\n",
      " [0 1 0 1]\n",
      " [1 0 0 1]\n",
      " [1 1 1 1]\n",
      " [0 0 0 1]]\n",
      "Actual Output: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "Predicted Output: \n",
      "[[0.01733189]\n",
      " [0.94944016]\n",
      " [0.95342139]\n",
      " [0.96818813]\n",
      " [0.96316339]\n",
      " [0.07200042]\n",
      " [0.02979468]]\n",
      "Loss: \n",
      "0.0019238531736656757\n",
      "\n",
      "\n",
      "+---------- EPOCH 550 -----------+\n",
      "Input: \n",
      " [[0 0 1 1]\n",
      " [0 1 1 1]\n",
      " [1 0 1 1]\n",
      " [0 1 0 1]\n",
      " [1 0 0 1]\n",
      " [1 1 1 1]\n",
      " [0 0 0 1]]\n",
      "Actual Output: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "Predicted Output: \n",
      "[[0.01615102]\n",
      " [0.95287068]\n",
      " [0.95650324]\n",
      " [0.97019673]\n",
      " [0.96546074]\n",
      " [0.06723703]\n",
      " [0.02757907]]\n",
      "Loss: \n",
      "0.0016766593200425901\n",
      "\n",
      "\n",
      "+---------- EPOCH 600 -----------+\n",
      "Input: \n",
      " [[0 0 1 1]\n",
      " [0 1 1 1]\n",
      " [1 0 1 1]\n",
      " [0 1 0 1]\n",
      " [1 0 0 1]\n",
      " [1 1 1 1]\n",
      " [0 0 0 1]]\n",
      "Actual Output: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "Predicted Output: \n",
      "[[0.01516415]\n",
      " [0.95572808]\n",
      " [0.95907766]\n",
      " [0.97187894]\n",
      " [0.96739191]\n",
      " [0.06326241]\n",
      " [0.02574503]]\n",
      "Loss: \n",
      "0.0014833731690905734\n",
      "\n",
      "\n",
      "+---------- EPOCH 650 -----------+\n",
      "Input: \n",
      " [[0 0 1 1]\n",
      " [0 1 1 1]\n",
      " [1 0 1 1]\n",
      " [0 1 0 1]\n",
      " [1 0 0 1]\n",
      " [1 1 1 1]\n",
      " [0 0 0 1]]\n",
      "Actual Output: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "Predicted Output: \n",
      "[[0.01432454]\n",
      " [0.95815243]\n",
      " [0.96126728]\n",
      " [0.97331329]\n",
      " [0.9690433 ]\n",
      " [0.05988474]\n",
      " [0.02419763]]\n",
      "Loss: \n",
      "0.0013284057708274383\n",
      "\n",
      "\n",
      "+---------- EPOCH 700 -----------+\n",
      "Input: \n",
      " [[0 0 1 1]\n",
      " [0 1 1 1]\n",
      " [1 0 1 1]\n",
      " [0 1 0 1]\n",
      " [1 0 0 1]\n",
      " [1 1 1 1]\n",
      " [0 0 0 1]]\n",
      "Actual Output: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "Predicted Output: \n",
      "[[0.01359966]\n",
      " [0.96024073]\n",
      " [0.96315737]\n",
      " [0.97455436]\n",
      " [0.97047546]\n",
      " [0.05697102]\n",
      " [0.02287149]]\n",
      "Loss: \n",
      "0.001201587214960399\n",
      "\n",
      "\n",
      "+---------- EPOCH 750 -----------+\n",
      "Input: \n",
      " [[0 0 1 1]\n",
      " [0 1 1 1]\n",
      " [1 0 1 1]\n",
      " [0 1 0 1]\n",
      " [1 0 0 1]\n",
      " [1 1 1 1]\n",
      " [0 0 0 1]]\n",
      "Actual Output: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "Predicted Output: \n",
      "[[0.01296612]\n",
      " [0.96206241]\n",
      " [0.96480919]\n",
      " [0.97564143]\n",
      " [0.97173224]\n",
      " [0.05442587]\n",
      " [0.02172007]]\n",
      "Loss: \n",
      "0.0010960167787569788\n",
      "\n",
      "\n",
      "+---------- EPOCH 800 -----------+\n",
      "Input: \n",
      " [[0 0 1 1]\n",
      " [0 1 1 1]\n",
      " [1 0 1 1]\n",
      " [0 1 0 1]\n",
      " [1 0 0 1]\n",
      " [1 1 1 1]\n",
      " [0 0 0 1]]\n",
      "Actual Output: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "Predicted Output: \n",
      "[[0.01240663]\n",
      " [0.96366858]\n",
      " [0.96626797]\n",
      " [0.97660352]\n",
      " [0.97284623]\n",
      " [0.05217903]\n",
      " [0.02070924]]\n",
      "Loss: \n",
      "0.0010068561119214348\n",
      "\n",
      "\n",
      "+---------- EPOCH 850 -----------+\n",
      "Input: \n",
      " [[0 0 1 1]\n",
      " [0 1 1 1]\n",
      " [1 0 1 1]\n",
      " [0 1 0 1]\n",
      " [1 0 0 1]\n",
      " [1 1 1 1]\n",
      " [0 0 0 1]]\n",
      "Actual Output: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "Predicted Output: \n",
      "[[0.01190811]\n",
      " [0.96509778]\n",
      " [0.96756791]\n",
      " [0.97746258]\n",
      " [0.97384219]\n",
      " [0.05017745]\n",
      " [0.0198134 ]]\n",
      "Loss: \n",
      "0.0009306175862961617\n",
      "\n",
      "\n",
      "+---------- EPOCH 900 -----------+\n",
      "Input: \n",
      " [[0 0 1 1]\n",
      " [0 1 1 1]\n",
      " [1 0 1 1]\n",
      " [0 1 0 1]\n",
      " [1 0 0 1]\n",
      " [1 1 1 1]\n",
      " [0 0 0 1]]\n",
      "Actual Output: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "Predicted Output: \n",
      "[[0.01146048]\n",
      " [0.96637962]\n",
      " [0.96873536]\n",
      " [0.97823556]\n",
      " [0.97473932]\n",
      " [0.0483803 ]\n",
      " [0.01901294]]\n",
      "Loss: \n",
      "0.0008647268498742321\n",
      "\n",
      "\n",
      "+---------- EPOCH 950 -----------+\n",
      "Input: \n",
      " [[0 0 1 1]\n",
      " [0 1 1 1]\n",
      " [1 0 1 1]\n",
      " [0 1 0 1]\n",
      " [1 0 0 1]\n",
      " [1 1 1 1]\n",
      " [0 0 0 1]]\n",
      "Actual Output: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "Predicted Output: \n",
      "[[0.0110558 ]\n",
      " [0.96753732]\n",
      " [0.96979099]\n",
      " [0.97893578]\n",
      " [0.97555274]\n",
      " [0.04675556]\n",
      " [0.01829254]]\n",
      "Loss: \n",
      "0.0008072442217660385\n",
      "\n",
      "\n",
      "+---------- EPOCH 1000 -----------+\n",
      "Input: \n",
      " [[0 0 1 1]\n",
      " [0 1 1 1]\n",
      " [1 0 1 1]\n",
      " [0 1 0 1]\n",
      " [1 0 0 1]\n",
      " [1 1 1 1]\n",
      " [0 0 0 1]]\n",
      "Actual Output: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "Predicted Output: \n",
      "[[0.01068776]\n",
      " [0.96858932]\n",
      " [0.97075129]\n",
      " [0.97957386]\n",
      " [0.97629453]\n",
      " [0.04527777]\n",
      " [0.01764009]]\n",
      "Loss: \n",
      "0.0007566817261468392\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### Your Code Here #####\n",
    "\n",
    "class Neural_Network(object):\n",
    "  def __init__(self):\n",
    "    self.inputs = 4\n",
    "    self.hiddenNodes = 4\n",
    "    self.outputNodes = 1\n",
    "\n",
    "    # Initialize Weights:\n",
    "    self.L1_weights = np.random.randn(self.inputs, self.hiddenNodes) # (3x2)\n",
    "    self.L2_weights = np.random.randn(self.hiddenNodes, self.outputNodes) # (3x1)\n",
    "\n",
    "  def feed_forward(self, X):\n",
    "    \n",
    "    # Weighted sum between inputs and hidden layer:\n",
    "    self.hidden_sum = np.dot(X, self.L1_weights)\n",
    "    \n",
    "    # Activations of weighted sum:\n",
    "    self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "    \n",
    "    # Weighted sum between hidden and output:\n",
    "    self.output_sum = np.dot(self.activated_hidden, self.L2_weights)\n",
    "    \n",
    "    # final activation of output:\n",
    "    self.activated_output = self.sigmoid(self.output_sum)\n",
    "    \n",
    "    return self.activated_output\n",
    "    \n",
    "  def sigmoid(self, s):\n",
    "    return 1/(1+np.exp(-s))\n",
    "  \n",
    "  #sigmoid derivative  \n",
    "  def sigmoidPrime(self, s):\n",
    "    return s * (1 - s)\n",
    "  \n",
    " #dCost/dWeights =    \n",
    "    \n",
    "  def backward(self, X, y, output):\n",
    "    # backward propgate through the network\n",
    "    \n",
    "    # error in output:\n",
    "    self.output_error = y - output \n",
    "    \n",
    "    # applying derivative of sigmoid to error:\n",
    "    self.output_delta = self.output_error * self.sigmoidPrime(output) \n",
    "    \n",
    "    # z2 error: how much our hidden layer weights contributed to output error:\n",
    "    self.z2_error = self.output_delta.dot(self.L2_weights.T)\n",
    "    \n",
    "    # applying derivative of sigmoid to z2 error:\n",
    "    self.z2_delta = self.z2_error*self.sigmoidPrime(self.activated_hidden) \n",
    "    \n",
    "    # adjusting first set (input --> hidden) weights:\n",
    "    self.L1_weights += X.T.dot(self.z2_delta) \n",
    "    \n",
    "    # adjusting second set (hidden --> output) weights:\n",
    "    self.L2_weights += self.activated_hidden.T.dot(self.output_delta) \n",
    "    \n",
    "  def train (self, X, y):\n",
    "    output = self.feed_forward(X)\n",
    "    self.backward(X, y, output)\n",
    "    \n",
    "NN = Neural_Network()\n",
    "for i in range(1000): # trains the NN 1,000 times\n",
    "  if i+1 in [1,2,3,4,5] or (i+1) % 50 == 0:\n",
    "    print('+---------- EPOCH', i+1, '-----------+')\n",
    "    print(\"Input: \\n\", X) \n",
    "    print(\"Actual Output: \\n\", y)  \n",
    "    print(\"Predicted Output: \\n\" + str(NN.feed_forward(X))) \n",
    "    print(\"Loss: \\n\" + str(np.mean(np.square(y - NN.feed_forward(X))))) # mean sum squared loss\n",
    "    print(\"\\n\")\n",
    "  NN.train(X, y)\n",
    "\n",
    "#Introducing bias 1 helped to descrease the error from 0.24 to 0.00075"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8b-r70o8p2Dm"
   },
   "source": [
    "## Try building/training a more complex MLP on a bigger dataset.\n",
    "\n",
    "Use the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) to build the cannonical handwriting digit recognizer and see what kind of accuracy you can achieve. \n",
    "\n",
    "If you need inspiration, the internet is chalk-full of tutorials, but I want you to see how far you can get on your own first. I've linked to the original MNIST dataset above but it will probably be easier to download data through a neural network library. If you reference outside resources make sure you understand every line of code that you're using from other sources, and share with your fellow students helpful resources that you find.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5MOPtYdk1HgA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions: 5000 x 784\n",
      "1st row [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.  51. 159. 253. 159.  50.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  48. 238. 252. 252. 252. 237.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  54.\n",
      " 227. 253. 252. 239. 233. 252.  57.   6.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  10.  60. 224.\n",
      " 252. 253. 252. 202.  84. 252. 253. 122.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 163. 252. 252.\n",
      " 252. 253. 252. 252.  96. 189. 253. 167.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  51. 238. 253. 253.\n",
      " 190. 114. 253. 228.  47.  79. 255. 168.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.  48. 238. 252. 252. 179.\n",
      "  12.  75. 121.  21.   0.   0. 253. 243.  50.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.  38. 165. 253. 233. 208.  84.\n",
      "   0.   0.   0.   0.   0.   0. 253. 252. 165.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   7. 178. 252. 240.  71.  19.  28.\n",
      "   0.   0.   0.   0.   0.   0. 253. 252. 195.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.  57. 252. 252.  63.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0. 253. 252. 195.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0. 198. 253. 190.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0. 255. 253. 196.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  76. 246. 252. 112.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0. 253. 252. 148.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  85. 252. 230.  25.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   7. 135. 253. 186.  12.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  85. 252. 223.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   7. 131. 252. 225.  71.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  85. 252. 145.   0.   0.   0.   0.   0.\n",
      "   0.   0.  48. 165. 252. 173.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  86. 253. 225.   0.   0.   0.   0.   0.\n",
      "   0. 114. 238. 253. 162.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  85. 252. 249. 146.  48.  29.  85. 178.\n",
      " 225. 253. 223. 167.  56.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  85. 252. 252. 252. 229. 215. 252. 252.\n",
      " 252. 196. 130.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  28. 199. 252. 252. 253. 252. 252. 233.\n",
      " 145.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.  25. 128. 252. 253. 252. 141.  37.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n"
     ]
    }
   ],
   "source": [
    "##### Your Code Here #####\n",
    "from mlxtend.data import mnist_data\n",
    "X, y = mnist_data()\n",
    "\n",
    "print('Dimensions: %s x %s' % (X.shape[0], X.shape[1]))\n",
    "print('1st row', X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_digit(X, y, idx):\n",
    "    img = X[idx].reshape(28,28)\n",
    "    plt.imshow(img, cmap='Greys',  interpolation='nearest')\n",
    "    plt.title('true label: %d' % y[idx])\n",
    "    plt.show()\n",
    "plot_digit(X, y, 600)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 3s 0us/step\n",
      "WARNING:tensorflow:From /Users/zarrina/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/zarrina/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 22s 373us/sample - loss: 0.2201 - acc: 0.9349\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 21s 354us/sample - loss: 0.0962 - acc: 0.9703\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 19s 322us/sample - loss: 0.0699 - acc: 0.9777\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 19s 321us/sample - loss: 0.0544 - acc: 0.9829\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 18s 307us/sample - loss: 0.0427 - acc: 0.9861\n",
      "10000/10000 [==============================] - 1s 97us/sample - loss: 0.0657 - acc: 0.9804\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06570803621758241, 0.9804]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using tensorflow\n",
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FwlRJSfBlCvy"
   },
   "source": [
    "## Stretch Goals: \n",
    "\n",
    "- Implement Cross Validation model evaluation on your MNIST implementation \n",
    "- Research different [Gradient Descent Based Optimizers](https://keras.io/optimizers/)\n",
    " - [Siraj Raval the evolution of gradient descent](https://www.youtube.com/watch?v=nhqo0u1a6fw)\n",
    "- Build a housing price estimation model using a neural network. How does its accuracy compare with the regression models that we fit earlier on in class?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "LS_DS_432_Backprop_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
