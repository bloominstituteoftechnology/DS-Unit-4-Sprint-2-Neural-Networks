{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y6SKlgYrpcym"
   },
   "source": [
    "# Neural Networks Sprint Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BrEbRrjVphPM"
   },
   "source": [
    "## 1) Define the following terms:\n",
    "\n",
    "- Neuron\n",
    "- Input Layer\n",
    "- Hidden Layer\n",
    "- Output Layer\n",
    "- Activation\n",
    "- Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q5EksLqnp4oB"
   },
   "source": [
    "  - Neuron:\n",
    "In machine learning, a neuron is a function that takes multiple inputs and outputs one value. If the neuron is in the first hidden layer, it's inputs are all the feature values of the input layer + the bias value. If the neuron is in a deeper hidden layer, it's inputs are all the neurons to it's left + the bias value. Each neuron multiplies the weights and inputs coming into it, adds the bias, and applies the activation function to the result to get the output value of the neuron.\n",
    "\n",
    "\n",
    "  - Input Layer:\n",
    "Input layer contains a set of cells. Each cell represents one feature, and contains the value present in that feature for that observation.\n",
    "\n",
    "\n",
    "  - Hidden Layer:\n",
    "Hidden layers are all the layers to the right of the input layer. These are called hidden, because we cannot directly influence them.\n",
    "\n",
    "\n",
    "  - Output Layer:\n",
    "An output layer contains as many neurons as you expect to solve your particular machine-learning problem. The output layer is activated by an activation function specific to the problem. For example for regression, the activation function is the linear function. The output values will be unbounded with this function, which is what you want. For binary classfication, you might use the sigmoid function to restrict values between 0 and 1. These values are then the probability of predicting the primary class.\n",
    "\n",
    "\n",
    "  - Activation Function (also called Transfer Function):\n",
    "An activation function takes a value and returns another value. It has to be differentiable, since we will differentiate it during the backpropagation step. You may choose different activation functions for different layers. An activation function is applied after a neuron has weighed all input values, added them together, and then added a bias.\n",
    "Common activation functions are the Sigmoid, tanh, step, and relu.\n",
    "\n",
    "  - Backpropagation\n",
    "On each iteration, we will get an output value. Backpropagation is the step that updates the weights/bias in the correct direction so that our error between the output value and the training output value is lower than the last error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ri_gRA2Jp728"
   },
   "source": [
    "## 2) Create a perceptron class that can model the behavior of an AND gate. You can use the following table as your training data:\n",
    "\n",
    "| x1 | x2 | x3 | y |\n",
    "|----|----|----|---|\n",
    "| 1  | 1  | 1  | 1 |\n",
    "| 1  | 0  | 1  | 0 |\n",
    "| 0  | 1  | 1  | 0 |\n",
    "| 0  | 0  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold, \\\n",
    "                                    cross_val_score, \\\n",
    "                                    GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3\n",
       "0  1  1  1  1\n",
       "1  1  0  1  0\n",
       "2  0  1  1  0\n",
       "3  0  0  1  0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[1, 1, 1], \n",
    "              [1, 0, 1],\n",
    "              [0, 1, 1], \n",
    "              [0, 0, 1]])\n",
    "y = np.array([[1], \n",
    "              [0], \n",
    "              [0], \n",
    "              [0]])\n",
    "df = pd.DataFrame(data=np.concatenate([X, y], axis=1))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, inputs, weights, bias, outputs):\n",
    "        self.inputs = inputs\n",
    "        self.weights = weights\n",
    "        self.outputs = outputs\n",
    "    \n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # Explanation for why this is is given here:\n",
    "    # https://towardsdatascience.com/derivative-of-the-sigmoid-function-536880cf918e\n",
    "    def sigmoid_derivative(x):\n",
    "        return sigmoid(x) * (1 - sigmoid(x))\n",
    "    \n",
    "    def iterate(self, num_iters):\n",
    "        for iteration in range(num_iters):\n",
    "\n",
    "          # Weighted sum of inputs and weights\n",
    "          weighted_sum = np.dot(self.inputs, \n",
    "                                self.weights) + bias\n",
    "\n",
    "          # Activate with sigmoid function\n",
    "          activated_output = sigmoid(weighted_sum)\n",
    "\n",
    "          # Calculate Error\n",
    "          error = self.outputs - activated_output\n",
    "\n",
    "          # Calculate weight adjustments with sigmoid_derivative\n",
    "          adjustments = error * sigmoid_derivative(activated_output)\n",
    "\n",
    "          # Update weights\n",
    "          self.weights += np.dot(self.inputs.T, adjustments)\n",
    "\n",
    "        print('optimized weights after training: ')\n",
    "        print(weights)\n",
    "\n",
    "        np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "        print('activated_output:', activated_output)\n",
    "\n",
    "        return activated_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized weights after training: \n",
      "[[11.839]\n",
      " [11.839]\n",
      " [-19.048]]\n",
      "activated_output: [[0.996]\n",
      " [0.002]\n",
      " [0.002]\n",
      " [0.000]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.996],\n",
       "       [0.002],\n",
       "       [0.002],\n",
       "       [0.000]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs  = X\n",
    "weights = 2 * np.random.random((3, 1)) - 1\n",
    "bias    = 1\n",
    "correct_outputs = y\n",
    "perceptron = Perceptron(inputs, weights, bias, correct_outputs)\n",
    "perceptron.iterate(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "86HyRi8Osr3U"
   },
   "source": [
    "## 3) Implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. \n",
    "- Your network must have one hidden layer. \n",
    "- You do not have to update weights via gradient descent. You can use something like the derivative of the sigmoid function to update weights.\n",
    "- Train your model on the Heart Disease dataset from UCI:\n",
    "\n",
    "[Github Dataset](https://github.com/ryanleeallred/datasets/blob/master/heart.csv)\n",
    "\n",
    "[Raw File on Github](https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 14)\n",
      "age           int64\n",
      "sex           int64\n",
      "cp            int64\n",
      "trestbps      int64\n",
      "chol          int64\n",
      "fbs           int64\n",
      "restecg       int64\n",
      "thalach       int64\n",
      "exang         int64\n",
      "oldpeak     float64\n",
      "slope         int64\n",
      "ca            int64\n",
      "thal          int64\n",
      "target        int64\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv',\n",
    "                 header=0)\n",
    "print(df.shape)\n",
    "print(df.dtypes)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age         0\n",
       "sex         0\n",
       "cp          0\n",
       "trestbps    0\n",
       "chol        0\n",
       "fbs         0\n",
       "restecg     0\n",
       "thalach     0\n",
       "exang       0\n",
       "oldpeak     0\n",
       "slope       0\n",
       "ca          0\n",
       "thal        0\n",
       "target      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "  if df[col].dtype == 'int64':\n",
    "    df[col] = df[col].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age         float64\n",
       "sex         float64\n",
       "cp          float64\n",
       "trestbps    float64\n",
       "chol        float64\n",
       "fbs         float64\n",
       "restecg     float64\n",
       "thalach     float64\n",
       "exang       float64\n",
       "oldpeak     float64\n",
       "slope       float64\n",
       "ca          float64\n",
       "thal        float64\n",
       "target      float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((303, 13), (303,))"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_X = df.drop(['target'], axis=1)\n",
    "df_y = df['target']\n",
    "df_X.shape, df_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CNfiajv3v4Ed"
   },
   "outputs": [],
   "source": [
    "seed = 101\n",
    "np.random.seed(seed)\n",
    "\n",
    "class Neural_Network(object):\n",
    "    def __init__(self, num_input_nodes, num_hidden_nodes, num_output_nodes):\n",
    "        self.num_input_nodes  = num_input_nodes   # 13 input nodes\n",
    "        self.num_hidden_nodes = num_hidden_nodes  # 8 hidden nodes\n",
    "        self.num_output_nodes = num_output_nodes  # 1 output node\n",
    "        \n",
    "        self.L1_weights = np.random.randn(num_input_nodes, num_hidden_nodes)\n",
    "        self.L2_weights = np.random.randn(num_hidden_nodes, num_output_nodes)\n",
    "        \n",
    "    def feed_forward(self, X):\n",
    "        self.hidden_sum = np.dot(X, self.L1_weights)\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.L2_weights)\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "        return self.activated_output\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1 + np.exp(-s))\n",
    "    \n",
    "    def sigmoid_prime(self, s):\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def backward(self, X, y, o):\n",
    "\n",
    "        self.o_error = y - o\n",
    "        self.o_delta = self.o_error * self.sigmoid_prime(o)\n",
    "        \n",
    "        self.z2_error = self.o_delta.dot(self.L2_weights.T)\n",
    "        self.z2_delta = self.z2_error * self.sigmoid_prime(self.activated_hidden)\n",
    "        \n",
    "        # Adjust weights\n",
    "        self.L1_weights += X.T.dot(self.z2_delta)\n",
    "        self.L2_weights += self.activated_hidden.T.dot(self.o_delta)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        o = self.feed_forward(X)\n",
    "        self.backward(X, y, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_X.values\n",
    "y = df_y.values\n",
    "y_out = []\n",
    "for i in range(len(y)):\n",
    "    y_out.append([y[i]])\n",
    "y = y_out.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------- EPOCH 1 -----------+ Loss: 0.2501642263758063\n",
      "+---------- EPOCH 2 -----------+ Loss: 0.402341042572636\n",
      "+---------- EPOCH 3 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 4 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 5 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 50 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 100 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 150 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 200 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 250 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 300 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 350 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 400 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 450 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 500 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 550 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 600 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 650 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 700 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 750 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 800 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 850 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 900 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 950 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 1000 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 1050 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 1100 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 1150 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 1200 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 1250 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 1300 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 1350 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 1400 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 1450 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 1500 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 1550 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 1600 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 1650 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 1700 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 1750 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 1800 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 1850 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 1900 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 1950 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 2000 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 2050 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 2100 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 2150 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 2200 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 2250 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 2300 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 2350 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 2400 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 2450 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 2500 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 2550 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 2600 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 2650 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 2700 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 2750 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 2800 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 2850 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 2900 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 2950 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 3000 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 3050 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 3100 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 3150 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 3200 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 3250 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 3300 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 3350 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 3400 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 3450 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 3500 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 3550 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 3600 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 3650 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 3700 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 3750 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 3800 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 3850 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 3900 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 3950 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 4000 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 4050 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 4100 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 4150 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 4200 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 4250 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 4300 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 4350 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 4400 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 4450 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 4500 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 4550 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 4600 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 4650 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 4700 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 4750 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 4800 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 4850 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 4900 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 4950 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 5000 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 5050 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 5100 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 5150 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 5200 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 5250 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 5300 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 5350 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 5400 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 5450 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 5500 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 5550 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 5600 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 5650 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 5700 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 5750 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 5800 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 5850 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 5900 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 5950 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 6000 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 6050 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 6100 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 6150 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 6200 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 6250 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 6300 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 6350 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 6400 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 6450 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 6500 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 6550 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 6600 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 6650 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 6700 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 6750 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 6800 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 6850 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 6900 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 6950 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 7000 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 7050 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 7100 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 7150 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 7200 -----------+ Loss: 0.5445544554455445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------- EPOCH 7250 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 7300 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 7350 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 7400 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 7450 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 7500 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 7550 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 7600 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 7650 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 7700 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 7750 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 7800 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 7850 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 7900 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 7950 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 8000 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 8050 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 8100 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 8150 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 8200 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 8250 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 8300 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 8350 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 8400 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 8450 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 8500 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 8550 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 8600 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 8650 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 8700 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 8750 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 8800 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 8850 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 8900 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 8950 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 9000 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 9050 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 9100 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 9150 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 9200 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 9250 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 9300 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 9350 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 9400 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 9450 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 9500 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 9550 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 9600 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 9650 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 9700 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 9750 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 9800 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 9850 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 9900 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 9950 -----------+ Loss: 0.5445544554455445\n",
      "+---------- EPOCH 10000 -----------+ Loss: 0.5445544554455445\n"
     ]
    }
   ],
   "source": [
    "NN = Neural_Network(13, 8, 1)\n",
    "for i in range(10000): # trains the NN 1,000 times\n",
    "  if i+1 in [1,2,3,4,5] or (i+1) % 50 == 0:\n",
    "    print('+---------- EPOCH', i+1, '-----------+', end=' ')\n",
    "#     print(\"Input: \\n\", X) \n",
    "#     print(\"Actual Output: \\n\", y)  \n",
    "#     print(\"Predicted Output: \\n\" + str(NN.feed_forward(X))) \n",
    "    print(\"Loss: \" + str(np.mean(np.square(y - NN.feed_forward(X))))) # mean sum squared loss\n",
    "#     print(\"\\n\")\n",
    "  NN.train(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GGT1oRzXw3H9"
   },
   "source": [
    "## 4) Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy. \n",
    "\n",
    "- Use the Heart Disease Dataset (binary classification)\n",
    "- Use an appropriate loss function for a binary classification task\n",
    "- Use an appropriate activation function on the final layer of your network. \n",
    "- Train your model using verbose output for ease of grading.\n",
    "- Use GridSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
    "- When hyperparameter tuning, show you work by adding code cells for each new experiment. \n",
    "- Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
    "- You must hyperparameter tune at least 5 parameters in order to get a 3 on this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XWw4IYxLxKwH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 6.9056 - acc: 0.5455\n",
      "Epoch 2/150\n",
      "242/242 [==============================] - 0s 62us/step - loss: 6.1594 - acc: 0.5413\n",
      "Epoch 3/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 4.9671 - acc: 0.5744\n",
      "Epoch 4/150\n",
      "242/242 [==============================] - 0s 75us/step - loss: 3.9814 - acc: 0.5124\n",
      "Epoch 5/150\n",
      "242/242 [==============================] - 0s 72us/step - loss: 3.3002 - acc: 0.5124\n",
      "Epoch 6/150\n",
      "242/242 [==============================] - 0s 63us/step - loss: 2.9909 - acc: 0.4917\n",
      "Epoch 7/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 2.5253 - acc: 0.4917\n",
      "Epoch 8/150\n",
      "242/242 [==============================] - 0s 71us/step - loss: 2.0864 - acc: 0.5165\n",
      "Epoch 9/150\n",
      "242/242 [==============================] - 0s 64us/step - loss: 1.6528 - acc: 0.5744\n",
      "Epoch 10/150\n",
      "242/242 [==============================] - 0s 72us/step - loss: 1.2276 - acc: 0.6074\n",
      "Epoch 11/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.8600 - acc: 0.6612\n",
      "Epoch 12/150\n",
      "242/242 [==============================] - 0s 74us/step - loss: 0.7857 - acc: 0.6901\n",
      "Epoch 13/150\n",
      "242/242 [==============================] - 0s 73us/step - loss: 0.7412 - acc: 0.6901\n",
      "Epoch 14/150\n",
      "242/242 [==============================] - 0s 64us/step - loss: 0.7227 - acc: 0.6818\n",
      "Epoch 15/150\n",
      "242/242 [==============================] - 0s 69us/step - loss: 0.7068 - acc: 0.6860\n",
      "Epoch 16/150\n",
      "242/242 [==============================] - 0s 64us/step - loss: 0.6884 - acc: 0.6777\n",
      "Epoch 17/150\n",
      "242/242 [==============================] - 0s 82us/step - loss: 0.6769 - acc: 0.6777\n",
      "Epoch 18/150\n",
      "242/242 [==============================] - 0s 67us/step - loss: 0.6707 - acc: 0.6777\n",
      "Epoch 19/150\n",
      "242/242 [==============================] - 0s 77us/step - loss: 0.6575 - acc: 0.6818\n",
      "Epoch 20/150\n",
      "242/242 [==============================] - 0s 79us/step - loss: 0.6458 - acc: 0.6736\n",
      "Epoch 21/150\n",
      "242/242 [==============================] - 0s 66us/step - loss: 0.6368 - acc: 0.6777\n",
      "Epoch 22/150\n",
      "242/242 [==============================] - 0s 67us/step - loss: 0.6297 - acc: 0.6736\n",
      "Epoch 23/150\n",
      "242/242 [==============================] - 0s 74us/step - loss: 0.6211 - acc: 0.6736\n",
      "Epoch 24/150\n",
      "242/242 [==============================] - 0s 61us/step - loss: 0.6166 - acc: 0.6818\n",
      "Epoch 25/150\n",
      "242/242 [==============================] - 0s 74us/step - loss: 0.6115 - acc: 0.6818\n",
      "Epoch 26/150\n",
      "242/242 [==============================] - ETA: 0s - loss: 0.5468 - acc: 0.625 - 0s 70us/step - loss: 0.6096 - acc: 0.6736\n",
      "Epoch 27/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.6067 - acc: 0.6983\n",
      "Epoch 28/150\n",
      "242/242 [==============================] - 0s 69us/step - loss: 0.6003 - acc: 0.6901\n",
      "Epoch 29/150\n",
      "242/242 [==============================] - 0s 61us/step - loss: 0.5993 - acc: 0.6653\n",
      "Epoch 30/150\n",
      "242/242 [==============================] - 0s 75us/step - loss: 0.5944 - acc: 0.6901\n",
      "Epoch 31/150\n",
      "242/242 [==============================] - 0s 76us/step - loss: 0.5918 - acc: 0.7025\n",
      "Epoch 32/150\n",
      "242/242 [==============================] - 0s 53us/step - loss: 0.5913 - acc: 0.7066\n",
      "Epoch 33/150\n",
      "242/242 [==============================] - 0s 71us/step - loss: 0.5931 - acc: 0.6983\n",
      "Epoch 34/150\n",
      "242/242 [==============================] - 0s 65us/step - loss: 0.5933 - acc: 0.6901\n",
      "Epoch 35/150\n",
      "242/242 [==============================] - 0s 69us/step - loss: 0.5871 - acc: 0.7149\n",
      "Epoch 36/150\n",
      "242/242 [==============================] - 0s 77us/step - loss: 0.5855 - acc: 0.7149\n",
      "Epoch 37/150\n",
      "242/242 [==============================] - 0s 56us/step - loss: 0.5845 - acc: 0.6860\n",
      "Epoch 38/150\n",
      "242/242 [==============================] - 0s 72us/step - loss: 0.5826 - acc: 0.7025\n",
      "Epoch 39/150\n",
      "242/242 [==============================] - 0s 71us/step - loss: 0.5837 - acc: 0.7025\n",
      "Epoch 40/150\n",
      "242/242 [==============================] - 0s 53us/step - loss: 0.5783 - acc: 0.7066\n",
      "Epoch 41/150\n",
      "242/242 [==============================] - 0s 71us/step - loss: 0.5809 - acc: 0.7066\n",
      "Epoch 42/150\n",
      "242/242 [==============================] - 0s 51us/step - loss: 0.5775 - acc: 0.7149\n",
      "Epoch 43/150\n",
      "242/242 [==============================] - 0s 69us/step - loss: 0.5818 - acc: 0.6901\n",
      "Epoch 44/150\n",
      "242/242 [==============================] - 0s 55us/step - loss: 0.5806 - acc: 0.7149\n",
      "Epoch 45/150\n",
      "242/242 [==============================] - 0s 81us/step - loss: 0.5750 - acc: 0.7066\n",
      "Epoch 46/150\n",
      "242/242 [==============================] - 0s 63us/step - loss: 0.5767 - acc: 0.7025\n",
      "Epoch 47/150\n",
      "242/242 [==============================] - 0s 80us/step - loss: 0.5754 - acc: 0.7107\n",
      "Epoch 48/150\n",
      "242/242 [==============================] - 0s 72us/step - loss: 0.5763 - acc: 0.7149\n",
      "Epoch 49/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.5766 - acc: 0.7025\n",
      "Epoch 50/150\n",
      "242/242 [==============================] - 0s 73us/step - loss: 0.5770 - acc: 0.7149\n",
      "Epoch 51/150\n",
      "242/242 [==============================] - 0s 69us/step - loss: 0.5727 - acc: 0.7107\n",
      "Epoch 52/150\n",
      "242/242 [==============================] - 0s 67us/step - loss: 0.5719 - acc: 0.7190\n",
      "Epoch 53/150\n",
      "242/242 [==============================] - 0s 68us/step - loss: 0.5751 - acc: 0.7231\n",
      "Epoch 54/150\n",
      "242/242 [==============================] - 0s 66us/step - loss: 0.5695 - acc: 0.7149\n",
      "Epoch 55/150\n",
      "242/242 [==============================] - 0s 76us/step - loss: 0.5693 - acc: 0.7190\n",
      "Epoch 56/150\n",
      "242/242 [==============================] - 0s 75us/step - loss: 0.5682 - acc: 0.7149\n",
      "Epoch 57/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.5683 - acc: 0.7231\n",
      "Epoch 58/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.5660 - acc: 0.7149\n",
      "Epoch 59/150\n",
      "242/242 [==============================] - 0s 63us/step - loss: 0.5677 - acc: 0.7149\n",
      "Epoch 60/150\n",
      "242/242 [==============================] - 0s 66us/step - loss: 0.5673 - acc: 0.7149\n",
      "Epoch 61/150\n",
      "242/242 [==============================] - 0s 72us/step - loss: 0.5652 - acc: 0.7149\n",
      "Epoch 62/150\n",
      "242/242 [==============================] - 0s 83us/step - loss: 0.5648 - acc: 0.7231\n",
      "Epoch 63/150\n",
      "242/242 [==============================] - 0s 65us/step - loss: 0.5651 - acc: 0.7273\n",
      "Epoch 64/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.5626 - acc: 0.7190\n",
      "Epoch 65/150\n",
      "242/242 [==============================] - 0s 67us/step - loss: 0.5648 - acc: 0.7107\n",
      "Epoch 66/150\n",
      "242/242 [==============================] - 0s 66us/step - loss: 0.5611 - acc: 0.7149\n",
      "Epoch 67/150\n",
      "242/242 [==============================] - 0s 69us/step - loss: 0.5612 - acc: 0.7107\n",
      "Epoch 68/150\n",
      "242/242 [==============================] - 0s 66us/step - loss: 0.5591 - acc: 0.7190\n",
      "Epoch 69/150\n",
      "242/242 [==============================] - 0s 70us/step - loss: 0.5645 - acc: 0.7190\n",
      "Epoch 70/150\n",
      "242/242 [==============================] - 0s 70us/step - loss: 0.5627 - acc: 0.7273\n",
      "Epoch 71/150\n",
      "242/242 [==============================] - 0s 55us/step - loss: 0.5584 - acc: 0.7231\n",
      "Epoch 72/150\n",
      "242/242 [==============================] - 0s 65us/step - loss: 0.5644 - acc: 0.7149\n",
      "Epoch 73/150\n",
      "242/242 [==============================] - 0s 66us/step - loss: 0.5597 - acc: 0.7066\n",
      "Epoch 74/150\n",
      "242/242 [==============================] - 0s 63us/step - loss: 0.5551 - acc: 0.7190\n",
      "Epoch 75/150\n",
      "242/242 [==============================] - 0s 61us/step - loss: 0.5687 - acc: 0.7355\n",
      "Epoch 76/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.5572 - acc: 0.7066\n",
      "Epoch 77/150\n",
      "242/242 [==============================] - 0s 62us/step - loss: 0.5596 - acc: 0.7149\n",
      "Epoch 78/150\n",
      "242/242 [==============================] - 0s 62us/step - loss: 0.5635 - acc: 0.7397\n",
      "Epoch 79/150\n",
      "242/242 [==============================] - 0s 53us/step - loss: 0.5617 - acc: 0.6983\n",
      "Epoch 80/150\n",
      "242/242 [==============================] - 0s 62us/step - loss: 0.5520 - acc: 0.7190\n",
      "Epoch 81/150\n",
      "242/242 [==============================] - 0s 62us/step - loss: 0.5559 - acc: 0.7273\n",
      "Epoch 82/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.5590 - acc: 0.6983\n",
      "Epoch 83/150\n",
      "242/242 [==============================] - 0s 53us/step - loss: 0.5713 - acc: 0.7190\n",
      "Epoch 84/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 68us/step - loss: 0.5813 - acc: 0.6983\n",
      "Epoch 85/150\n",
      "242/242 [==============================] - 0s 55us/step - loss: 0.5581 - acc: 0.7314\n",
      "Epoch 86/150\n",
      "242/242 [==============================] - 0s 56us/step - loss: 0.5600 - acc: 0.7231\n",
      "Epoch 87/150\n",
      "242/242 [==============================] - 0s 69us/step - loss: 0.5496 - acc: 0.7190\n",
      "Epoch 88/150\n",
      "242/242 [==============================] - 0s 61us/step - loss: 0.5584 - acc: 0.7273\n",
      "Epoch 89/150\n",
      "242/242 [==============================] - 0s 54us/step - loss: 0.5522 - acc: 0.7355\n",
      "Epoch 90/150\n",
      "242/242 [==============================] - 0s 65us/step - loss: 0.5469 - acc: 0.7397\n",
      "Epoch 91/150\n",
      "242/242 [==============================] - 0s 73us/step - loss: 0.5503 - acc: 0.7066\n",
      "Epoch 92/150\n",
      "242/242 [==============================] - 0s 67us/step - loss: 0.5549 - acc: 0.7355\n",
      "Epoch 93/150\n",
      "242/242 [==============================] - 0s 56us/step - loss: 0.5466 - acc: 0.7149\n",
      "Epoch 94/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.5504 - acc: 0.7273\n",
      "Epoch 95/150\n",
      "242/242 [==============================] - 0s 53us/step - loss: 0.5455 - acc: 0.7314\n",
      "Epoch 96/150\n",
      "242/242 [==============================] - 0s 63us/step - loss: 0.5452 - acc: 0.7231\n",
      "Epoch 97/150\n",
      "242/242 [==============================] - 0s 64us/step - loss: 0.5436 - acc: 0.7273\n",
      "Epoch 98/150\n",
      "242/242 [==============================] - 0s 63us/step - loss: 0.5460 - acc: 0.7231\n",
      "Epoch 99/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.5441 - acc: 0.7355\n",
      "Epoch 100/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.5433 - acc: 0.7314\n",
      "Epoch 101/150\n",
      "242/242 [==============================] - 0s 61us/step - loss: 0.5414 - acc: 0.7190\n",
      "Epoch 102/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.5432 - acc: 0.7521\n",
      "Epoch 103/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.5664 - acc: 0.6777\n",
      "Epoch 104/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.5382 - acc: 0.7355\n",
      "Epoch 105/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.5437 - acc: 0.7479\n",
      "Epoch 106/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.5494 - acc: 0.7066\n",
      "Epoch 107/150\n",
      "242/242 [==============================] - 0s 56us/step - loss: 0.5320 - acc: 0.7355\n",
      "Epoch 108/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.5454 - acc: 0.7355\n",
      "Epoch 109/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.5347 - acc: 0.7355\n",
      "Epoch 110/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.5357 - acc: 0.7397\n",
      "Epoch 111/150\n",
      "242/242 [==============================] - 0s 53us/step - loss: 0.5363 - acc: 0.7190\n",
      "Epoch 112/150\n",
      "242/242 [==============================] - 0s 66us/step - loss: 0.5468 - acc: 0.7479\n",
      "Epoch 113/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.5383 - acc: 0.7231\n",
      "Epoch 114/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.5435 - acc: 0.7355\n",
      "Epoch 115/150\n",
      "242/242 [==============================] - 0s 64us/step - loss: 0.5316 - acc: 0.7314\n",
      "Epoch 116/150\n",
      "242/242 [==============================] - 0s 70us/step - loss: 0.5399 - acc: 0.7107\n",
      "Epoch 117/150\n",
      "242/242 [==============================] - 0s 76us/step - loss: 0.5310 - acc: 0.7355\n",
      "Epoch 118/150\n",
      "242/242 [==============================] - 0s 72us/step - loss: 0.5330 - acc: 0.7231\n",
      "Epoch 119/150\n",
      "242/242 [==============================] - 0s 62us/step - loss: 0.5422 - acc: 0.7521\n",
      "Epoch 120/150\n",
      "242/242 [==============================] - 0s 61us/step - loss: 0.5343 - acc: 0.7231\n",
      "Epoch 121/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.5331 - acc: 0.7314\n",
      "Epoch 122/150\n",
      "242/242 [==============================] - 0s 56us/step - loss: 0.5302 - acc: 0.7438\n",
      "Epoch 123/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.5437 - acc: 0.7438\n",
      "Epoch 124/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.5409 - acc: 0.7231\n",
      "Epoch 125/150\n",
      "242/242 [==============================] - 0s 55us/step - loss: 0.5292 - acc: 0.7479\n",
      "Epoch 126/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.5301 - acc: 0.7521\n",
      "Epoch 127/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.5296 - acc: 0.7149\n",
      "Epoch 128/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.5273 - acc: 0.7438\n",
      "Epoch 129/150\n",
      "242/242 [==============================] - 0s 65us/step - loss: 0.5221 - acc: 0.7562\n",
      "Epoch 130/150\n",
      "242/242 [==============================] - 0s 62us/step - loss: 0.5281 - acc: 0.7273\n",
      "Epoch 131/150\n",
      "242/242 [==============================] - 0s 65us/step - loss: 0.5217 - acc: 0.7314\n",
      "Epoch 132/150\n",
      "242/242 [==============================] - 0s 62us/step - loss: 0.5317 - acc: 0.7479\n",
      "Epoch 133/150\n",
      "242/242 [==============================] - 0s 64us/step - loss: 0.5170 - acc: 0.7314\n",
      "Epoch 134/150\n",
      "242/242 [==============================] - 0s 64us/step - loss: 0.5332 - acc: 0.7107\n",
      "Epoch 135/150\n",
      "242/242 [==============================] - 0s 64us/step - loss: 0.5338 - acc: 0.7397\n",
      "Epoch 136/150\n",
      "242/242 [==============================] - 0s 62us/step - loss: 0.5185 - acc: 0.7355\n",
      "Epoch 137/150\n",
      "242/242 [==============================] - 0s 62us/step - loss: 0.5208 - acc: 0.7314\n",
      "Epoch 138/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.5186 - acc: 0.7479\n",
      "Epoch 139/150\n",
      "242/242 [==============================] - 0s 61us/step - loss: 0.5174 - acc: 0.7438\n",
      "Epoch 140/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.5174 - acc: 0.7562\n",
      "Epoch 141/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.5179 - acc: 0.7521\n",
      "Epoch 142/150\n",
      "242/242 [==============================] - 0s 63us/step - loss: 0.5235 - acc: 0.7355\n",
      "Epoch 143/150\n",
      "242/242 [==============================] - 0s 64us/step - loss: 0.5132 - acc: 0.7562\n",
      "Epoch 144/150\n",
      "242/242 [==============================] - 0s 61us/step - loss: 0.5215 - acc: 0.7438\n",
      "Epoch 145/150\n",
      "242/242 [==============================] - 0s 64us/step - loss: 0.5114 - acc: 0.7562\n",
      "Epoch 146/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.5153 - acc: 0.7603\n",
      "Epoch 147/150\n",
      "242/242 [==============================] - 0s 70us/step - loss: 0.5124 - acc: 0.7397\n",
      "Epoch 148/150\n",
      "242/242 [==============================] - 0s 68us/step - loss: 0.5111 - acc: 0.7438\n",
      "Epoch 149/150\n",
      "242/242 [==============================] - 0s 63us/step - loss: 0.5110 - acc: 0.7562\n",
      "Epoch 150/150\n",
      "242/242 [==============================] - 0s 55us/step - loss: 0.5094 - acc: 0.7397\n",
      "61/61 [==============================] - 0s 6ms/step\n",
      "Epoch 1/150\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 4.8173 - acc: 0.4587\n",
      "Epoch 2/150\n",
      "242/242 [==============================] - 0s 56us/step - loss: 1.8192 - acc: 0.6157\n",
      "Epoch 3/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 1.6701 - acc: 0.6364\n",
      "Epoch 4/150\n",
      "242/242 [==============================] - 0s 62us/step - loss: 1.4092 - acc: 0.6488\n",
      "Epoch 5/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 1.1453 - acc: 0.6570\n",
      "Epoch 6/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.9976 - acc: 0.6777\n",
      "Epoch 7/150\n",
      "242/242 [==============================] - 0s 53us/step - loss: 0.9469 - acc: 0.6694\n",
      "Epoch 8/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.8395 - acc: 0.6860\n",
      "Epoch 9/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.7912 - acc: 0.6901\n",
      "Epoch 10/150\n",
      "242/242 [==============================] - 0s 53us/step - loss: 0.7640 - acc: 0.6942\n",
      "Epoch 11/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.7236 - acc: 0.7273\n",
      "Epoch 12/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.7083 - acc: 0.7066\n",
      "Epoch 13/150\n",
      "242/242 [==============================] - 0s 54us/step - loss: 0.6997 - acc: 0.6983\n",
      "Epoch 14/150\n",
      "242/242 [==============================] - 0s 54us/step - loss: 0.6987 - acc: 0.7149\n",
      "Epoch 15/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.6752 - acc: 0.7066\n",
      "Epoch 16/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 58us/step - loss: 0.6753 - acc: 0.6942\n",
      "Epoch 17/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.6606 - acc: 0.7149\n",
      "Epoch 18/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.6591 - acc: 0.7273\n",
      "Epoch 19/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.6528 - acc: 0.7107\n",
      "Epoch 20/150\n",
      "242/242 [==============================] - 0s 68us/step - loss: 0.6487 - acc: 0.7025\n",
      "Epoch 21/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.6496 - acc: 0.7107\n",
      "Epoch 22/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.6393 - acc: 0.7231\n",
      "Epoch 23/150\n",
      "242/242 [==============================] - 0s 54us/step - loss: 0.6363 - acc: 0.7231\n",
      "Epoch 24/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.6267 - acc: 0.7273\n",
      "Epoch 25/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.6415 - acc: 0.7025\n",
      "Epoch 26/150\n",
      "242/242 [==============================] - 0s 63us/step - loss: 0.6264 - acc: 0.7231\n",
      "Epoch 27/150\n",
      "242/242 [==============================] - 0s 61us/step - loss: 0.6157 - acc: 0.7438\n",
      "Epoch 28/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.6291 - acc: 0.7107\n",
      "Epoch 29/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.6284 - acc: 0.7149\n",
      "Epoch 30/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.6362 - acc: 0.7066\n",
      "Epoch 31/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.6209 - acc: 0.7107\n",
      "Epoch 32/150\n",
      "242/242 [==============================] - 0s 56us/step - loss: 0.6019 - acc: 0.7231\n",
      "Epoch 33/150\n",
      "242/242 [==============================] - 0s 55us/step - loss: 0.5828 - acc: 0.7521\n",
      "Epoch 34/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.5668 - acc: 0.7438\n",
      "Epoch 35/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.5553 - acc: 0.7521\n",
      "Epoch 36/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.5729 - acc: 0.7438\n",
      "Epoch 37/150\n",
      "242/242 [==============================] - 0s 56us/step - loss: 0.5736 - acc: 0.7190\n",
      "Epoch 38/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.5529 - acc: 0.7314\n",
      "Epoch 39/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.5298 - acc: 0.7479\n",
      "Epoch 40/150\n",
      "242/242 [==============================] - 0s 55us/step - loss: 0.5210 - acc: 0.7645\n",
      "Epoch 41/150\n",
      "242/242 [==============================] - 0s 55us/step - loss: 0.5183 - acc: 0.7603\n",
      "Epoch 42/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.5182 - acc: 0.7479\n",
      "Epoch 43/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.5015 - acc: 0.7645\n",
      "Epoch 44/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.5000 - acc: 0.7521\n",
      "Epoch 45/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.4994 - acc: 0.7603\n",
      "Epoch 46/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.5204 - acc: 0.7231\n",
      "Epoch 47/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.5202 - acc: 0.7521\n",
      "Epoch 48/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.5497 - acc: 0.7190\n",
      "Epoch 49/150\n",
      "242/242 [==============================] - 0s 56us/step - loss: 0.4805 - acc: 0.7686\n",
      "Epoch 50/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.4759 - acc: 0.7810\n",
      "Epoch 51/150\n",
      "242/242 [==============================] - 0s 56us/step - loss: 0.4721 - acc: 0.7893\n",
      "Epoch 52/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.4673 - acc: 0.7769\n",
      "Epoch 53/150\n",
      "242/242 [==============================] - 0s 61us/step - loss: 0.4743 - acc: 0.7851\n",
      "Epoch 54/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.4909 - acc: 0.7479\n",
      "Epoch 55/150\n",
      "242/242 [==============================] - 0s 56us/step - loss: 0.4737 - acc: 0.7769\n",
      "Epoch 56/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.4645 - acc: 0.7893\n",
      "Epoch 57/150\n",
      "242/242 [==============================] - 0s 63us/step - loss: 0.4582 - acc: 0.8017\n",
      "Epoch 58/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.4593 - acc: 0.7893\n",
      "Epoch 59/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.4569 - acc: 0.7851\n",
      "Epoch 60/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.4540 - acc: 0.7975\n",
      "Epoch 61/150\n",
      "242/242 [==============================] - 0s 56us/step - loss: 0.4547 - acc: 0.7893\n",
      "Epoch 62/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.4500 - acc: 0.7934\n",
      "Epoch 63/150\n",
      "242/242 [==============================] - 0s 61us/step - loss: 0.4538 - acc: 0.7893\n",
      "Epoch 64/150\n",
      "242/242 [==============================] - 0s 55us/step - loss: 0.4575 - acc: 0.7810\n",
      "Epoch 65/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.4465 - acc: 0.7934\n",
      "Epoch 66/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.4558 - acc: 0.7934\n",
      "Epoch 67/150\n",
      "242/242 [==============================] - 0s 62us/step - loss: 0.4464 - acc: 0.8058\n",
      "Epoch 68/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.4485 - acc: 0.7975\n",
      "Epoch 69/150\n",
      "242/242 [==============================] - 0s 62us/step - loss: 0.4458 - acc: 0.7810\n",
      "Epoch 70/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.4615 - acc: 0.8017\n",
      "Epoch 71/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.4552 - acc: 0.7851\n",
      "Epoch 72/150\n",
      "242/242 [==============================] - 0s 54us/step - loss: 0.4380 - acc: 0.8099\n",
      "Epoch 73/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.4336 - acc: 0.8058\n",
      "Epoch 74/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.4344 - acc: 0.7975\n",
      "Epoch 75/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.4382 - acc: 0.8017\n",
      "Epoch 76/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.4640 - acc: 0.7603\n",
      "Epoch 77/150\n",
      "242/242 [==============================] - 0s 61us/step - loss: 0.4537 - acc: 0.7727\n",
      "Epoch 78/150\n",
      "242/242 [==============================] - 0s 68us/step - loss: 0.4267 - acc: 0.8140\n",
      "Epoch 79/150\n",
      "242/242 [==============================] - 0s 62us/step - loss: 0.4489 - acc: 0.8058\n",
      "Epoch 80/150\n",
      "242/242 [==============================] - 0s 62us/step - loss: 0.4627 - acc: 0.7810\n",
      "Epoch 81/150\n",
      "242/242 [==============================] - 0s 56us/step - loss: 0.4437 - acc: 0.7851\n",
      "Epoch 82/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.4649 - acc: 0.7810\n",
      "Epoch 83/150\n",
      "242/242 [==============================] - 0s 54us/step - loss: 0.4486 - acc: 0.7686\n",
      "Epoch 84/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.4352 - acc: 0.7851\n",
      "Epoch 85/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.4276 - acc: 0.8140\n",
      "Epoch 86/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.4185 - acc: 0.8058\n",
      "Epoch 87/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.4231 - acc: 0.7893\n",
      "Epoch 88/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.4263 - acc: 0.8058\n",
      "Epoch 89/150\n",
      "242/242 [==============================] - 0s 61us/step - loss: 0.4258 - acc: 0.8058\n",
      "Epoch 90/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.4184 - acc: 0.8182\n",
      "Epoch 91/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.4180 - acc: 0.8058\n",
      "Epoch 92/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.4223 - acc: 0.8182\n",
      "Epoch 93/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.4167 - acc: 0.8099\n",
      "Epoch 94/150\n",
      "242/242 [==============================] - 0s 56us/step - loss: 0.4177 - acc: 0.8017\n",
      "Epoch 95/150\n",
      "242/242 [==============================] - 0s 55us/step - loss: 0.4220 - acc: 0.8223\n",
      "Epoch 96/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.4125 - acc: 0.8099\n",
      "Epoch 97/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.4178 - acc: 0.8264\n",
      "Epoch 98/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.4131 - acc: 0.8099\n",
      "Epoch 99/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 59us/step - loss: 0.4110 - acc: 0.8099\n",
      "Epoch 100/150\n",
      "242/242 [==============================] - 0s 52us/step - loss: 0.4169 - acc: 0.8182\n",
      "Epoch 101/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.4080 - acc: 0.8347\n",
      "Epoch 102/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.4089 - acc: 0.8223\n",
      "Epoch 103/150\n",
      "242/242 [==============================] - 0s 64us/step - loss: 0.4112 - acc: 0.8264\n",
      "Epoch 104/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.4336 - acc: 0.7975\n",
      "Epoch 105/150\n",
      "242/242 [==============================] - 0s 61us/step - loss: 0.4037 - acc: 0.8223\n",
      "Epoch 106/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.4052 - acc: 0.8347\n",
      "Epoch 107/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.4039 - acc: 0.8264\n",
      "Epoch 108/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.4056 - acc: 0.8306\n",
      "Epoch 109/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.4058 - acc: 0.8223\n",
      "Epoch 110/150\n",
      "242/242 [==============================] - 0s 52us/step - loss: 0.4010 - acc: 0.8223\n",
      "Epoch 111/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.4076 - acc: 0.8264\n",
      "Epoch 112/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.3987 - acc: 0.8347\n",
      "Epoch 113/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.3986 - acc: 0.8306\n",
      "Epoch 114/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.4033 - acc: 0.8264\n",
      "Epoch 115/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.4137 - acc: 0.8347\n",
      "Epoch 116/150\n",
      "242/242 [==============================] - 0s 54us/step - loss: 0.4135 - acc: 0.8099\n",
      "Epoch 117/150\n",
      "242/242 [==============================] - 0s 64us/step - loss: 0.4416 - acc: 0.7975\n",
      "Epoch 118/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.4279 - acc: 0.8140\n",
      "Epoch 119/150\n",
      "242/242 [==============================] - 0s 56us/step - loss: 0.3952 - acc: 0.8264\n",
      "Epoch 120/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.4206 - acc: 0.8058\n",
      "Epoch 121/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.3999 - acc: 0.8264\n",
      "Epoch 122/150\n",
      "242/242 [==============================] - 0s 62us/step - loss: 0.4055 - acc: 0.8140\n",
      "Epoch 123/150\n",
      "242/242 [==============================] - 0s 63us/step - loss: 0.4269 - acc: 0.7893\n",
      "Epoch 124/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.4170 - acc: 0.7810\n",
      "Epoch 125/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.4404 - acc: 0.7727\n",
      "Epoch 126/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.4137 - acc: 0.8347\n",
      "Epoch 127/150\n",
      "242/242 [==============================] - 0s 55us/step - loss: 0.3938 - acc: 0.8306\n",
      "Epoch 128/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.3915 - acc: 0.8306\n",
      "Epoch 129/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.4079 - acc: 0.8140\n",
      "Epoch 130/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.4060 - acc: 0.8182\n",
      "Epoch 131/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.3912 - acc: 0.8264\n",
      "Epoch 132/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.3952 - acc: 0.8306\n",
      "Epoch 133/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.3941 - acc: 0.8264\n",
      "Epoch 134/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.3950 - acc: 0.8306\n",
      "Epoch 135/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.4078 - acc: 0.8140\n",
      "Epoch 136/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.3952 - acc: 0.8306\n",
      "Epoch 137/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.3924 - acc: 0.8264\n",
      "Epoch 138/150\n",
      "242/242 [==============================] - 0s 54us/step - loss: 0.3878 - acc: 0.8430\n",
      "Epoch 139/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.3869 - acc: 0.8347\n",
      "Epoch 140/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.3885 - acc: 0.8306\n",
      "Epoch 141/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.3903 - acc: 0.8306\n",
      "Epoch 142/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.3888 - acc: 0.8182\n",
      "Epoch 143/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.3930 - acc: 0.8264\n",
      "Epoch 144/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.3904 - acc: 0.8347\n",
      "Epoch 145/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.3886 - acc: 0.8140\n",
      "Epoch 146/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.3833 - acc: 0.8347\n",
      "Epoch 147/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.3812 - acc: 0.8347\n",
      "Epoch 148/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.3921 - acc: 0.8347\n",
      "Epoch 149/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.4063 - acc: 0.8017\n",
      "Epoch 150/150\n",
      "242/242 [==============================] - 0s 64us/step - loss: 0.4052 - acc: 0.8058\n",
      "61/61 [==============================] - 0s 6ms/step\n",
      "Epoch 1/150\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 3.8059 - acc: 0.5455\n",
      "Epoch 2/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 1.0800 - acc: 0.6364\n",
      "Epoch 3/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 1.1337 - acc: 0.5455\n",
      "Epoch 4/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.7317 - acc: 0.6488\n",
      "Epoch 5/150\n",
      "242/242 [==============================] - 0s 55us/step - loss: 0.8213 - acc: 0.6322\n",
      "Epoch 6/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.7179 - acc: 0.6694\n",
      "Epoch 7/150\n",
      "242/242 [==============================] - 0s 56us/step - loss: 0.7221 - acc: 0.6570\n",
      "Epoch 8/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.6900 - acc: 0.6364\n",
      "Epoch 9/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.6639 - acc: 0.6529\n",
      "Epoch 10/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.6540 - acc: 0.6777\n",
      "Epoch 11/150\n",
      "242/242 [==============================] - 0s 61us/step - loss: 0.6451 - acc: 0.6736\n",
      "Epoch 12/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.6411 - acc: 0.6694\n",
      "Epoch 13/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.6368 - acc: 0.6694\n",
      "Epoch 14/150\n",
      "242/242 [==============================] - 0s 56us/step - loss: 0.6273 - acc: 0.6901\n",
      "Epoch 15/150\n",
      "242/242 [==============================] - 0s 56us/step - loss: 0.6276 - acc: 0.6860\n",
      "Epoch 16/150\n",
      "242/242 [==============================] - 0s 56us/step - loss: 0.6285 - acc: 0.6694\n",
      "Epoch 17/150\n",
      "242/242 [==============================] - 0s 56us/step - loss: 0.6183 - acc: 0.6983\n",
      "Epoch 18/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.6177 - acc: 0.6942\n",
      "Epoch 19/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.6216 - acc: 0.6529\n",
      "Epoch 20/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.6134 - acc: 0.7149\n",
      "Epoch 21/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.6107 - acc: 0.7107\n",
      "Epoch 22/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.6075 - acc: 0.6942\n",
      "Epoch 23/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.6036 - acc: 0.6942\n",
      "Epoch 24/150\n",
      "242/242 [==============================] - 0s 54us/step - loss: 0.6010 - acc: 0.6983\n",
      "Epoch 25/150\n",
      "242/242 [==============================] - 0s 56us/step - loss: 0.5948 - acc: 0.7066\n",
      "Epoch 26/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.5969 - acc: 0.6694\n",
      "Epoch 27/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.6014 - acc: 0.7107\n",
      "Epoch 28/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.5889 - acc: 0.7066\n",
      "Epoch 29/150\n",
      "242/242 [==============================] - 0s 61us/step - loss: 0.5981 - acc: 0.7066\n",
      "Epoch 30/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.5929 - acc: 0.6860\n",
      "Epoch 31/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 56us/step - loss: 0.5872 - acc: 0.7190\n",
      "Epoch 32/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.5817 - acc: 0.7107\n",
      "Epoch 33/150\n",
      "242/242 [==============================] - 0s 63us/step - loss: 0.5822 - acc: 0.7107\n",
      "Epoch 34/150\n",
      "242/242 [==============================] - 0s 68us/step - loss: 0.5765 - acc: 0.7107\n",
      "Epoch 35/150\n",
      "242/242 [==============================] - 0s 61us/step - loss: 0.5964 - acc: 0.7107\n",
      "Epoch 36/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.5974 - acc: 0.7066\n",
      "Epoch 37/150\n",
      "242/242 [==============================] - 0s 62us/step - loss: 0.5719 - acc: 0.7149\n",
      "Epoch 38/150\n",
      "242/242 [==============================] - 0s 64us/step - loss: 0.5790 - acc: 0.7066\n",
      "Epoch 39/150\n",
      "242/242 [==============================] - 0s 64us/step - loss: 0.5835 - acc: 0.7190\n",
      "Epoch 40/150\n",
      "242/242 [==============================] - 0s 61us/step - loss: 0.5640 - acc: 0.7107\n",
      "Epoch 41/150\n",
      "242/242 [==============================] - 0s 54us/step - loss: 0.5665 - acc: 0.7025\n",
      "Epoch 42/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.5675 - acc: 0.7231\n",
      "Epoch 43/150\n",
      "242/242 [==============================] - 0s 61us/step - loss: 0.5585 - acc: 0.7190\n",
      "Epoch 44/150\n",
      "242/242 [==============================] - 0s 62us/step - loss: 0.5571 - acc: 0.7107\n",
      "Epoch 45/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.5655 - acc: 0.7190\n",
      "Epoch 46/150\n",
      "242/242 [==============================] - 0s 65us/step - loss: 0.5554 - acc: 0.7273\n",
      "Epoch 47/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.5515 - acc: 0.7355\n",
      "Epoch 48/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.5490 - acc: 0.7231\n",
      "Epoch 49/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.5645 - acc: 0.7107\n",
      "Epoch 50/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.5537 - acc: 0.7190\n",
      "Epoch 51/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.5502 - acc: 0.7397\n",
      "Epoch 52/150\n",
      "242/242 [==============================] - 0s 62us/step - loss: 0.5476 - acc: 0.7397\n",
      "Epoch 53/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.5421 - acc: 0.7438\n",
      "Epoch 54/150\n",
      "242/242 [==============================] - 0s 64us/step - loss: 0.5578 - acc: 0.7231\n",
      "Epoch 55/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.5523 - acc: 0.7314\n",
      "Epoch 56/150\n",
      "242/242 [==============================] - 0s 61us/step - loss: 0.5450 - acc: 0.7314\n",
      "Epoch 57/150\n",
      "242/242 [==============================] - 0s 61us/step - loss: 0.5364 - acc: 0.7314\n",
      "Epoch 58/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.5366 - acc: 0.7273\n",
      "Epoch 59/150\n",
      "242/242 [==============================] - 0s 66us/step - loss: 0.5680 - acc: 0.7231\n",
      "Epoch 60/150\n",
      "242/242 [==============================] - 0s 63us/step - loss: 0.5318 - acc: 0.7397\n",
      "Epoch 61/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.5332 - acc: 0.7479\n",
      "Epoch 62/150\n",
      "242/242 [==============================] - 0s 63us/step - loss: 0.5328 - acc: 0.7438\n",
      "Epoch 63/150\n",
      "242/242 [==============================] - 0s 68us/step - loss: 0.5418 - acc: 0.7479\n",
      "Epoch 64/150\n",
      "242/242 [==============================] - 0s 62us/step - loss: 0.5411 - acc: 0.7397\n",
      "Epoch 65/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.5165 - acc: 0.7438\n",
      "Epoch 66/150\n",
      "242/242 [==============================] - 0s 61us/step - loss: 0.5231 - acc: 0.7438\n",
      "Epoch 67/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.5239 - acc: 0.7521\n",
      "Epoch 68/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.5367 - acc: 0.7066\n",
      "Epoch 69/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.5103 - acc: 0.7479\n",
      "Epoch 70/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.5121 - acc: 0.7562\n",
      "Epoch 71/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.5132 - acc: 0.7851\n",
      "Epoch 72/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.5132 - acc: 0.7727\n",
      "Epoch 73/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.5521 - acc: 0.7107\n",
      "Epoch 74/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.5168 - acc: 0.7521\n",
      "Epoch 75/150\n",
      "242/242 [==============================] - 0s 62us/step - loss: 0.5175 - acc: 0.7438\n",
      "Epoch 76/150\n",
      "242/242 [==============================] - 0s 64us/step - loss: 0.5066 - acc: 0.7562\n",
      "Epoch 77/150\n",
      "242/242 [==============================] - 0s 61us/step - loss: 0.5012 - acc: 0.7438\n",
      "Epoch 78/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.4976 - acc: 0.7727\n",
      "Epoch 79/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.4928 - acc: 0.7686\n",
      "Epoch 80/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.4930 - acc: 0.7727\n",
      "Epoch 81/150\n",
      "242/242 [==============================] - 0s 61us/step - loss: 0.4902 - acc: 0.7851\n",
      "Epoch 82/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.4894 - acc: 0.7603\n",
      "Epoch 83/150\n",
      "242/242 [==============================] - 0s 61us/step - loss: 0.4889 - acc: 0.7479\n",
      "Epoch 84/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.4873 - acc: 0.7562\n",
      "Epoch 85/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.4833 - acc: 0.7562\n",
      "Epoch 86/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.4880 - acc: 0.7727\n",
      "Epoch 87/150\n",
      "242/242 [==============================] - 0s 64us/step - loss: 0.5136 - acc: 0.7355\n",
      "Epoch 88/150\n",
      "242/242 [==============================] - 0s 62us/step - loss: 0.5183 - acc: 0.7603\n",
      "Epoch 89/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.5147 - acc: 0.7521\n",
      "Epoch 90/150\n",
      "242/242 [==============================] - 0s 63us/step - loss: 0.4854 - acc: 0.7562\n",
      "Epoch 91/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.4891 - acc: 0.7851\n",
      "Epoch 92/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.4785 - acc: 0.7727\n",
      "Epoch 93/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.4720 - acc: 0.7810\n",
      "Epoch 94/150\n",
      "242/242 [==============================] - 0s 70us/step - loss: 0.4656 - acc: 0.7686\n",
      "Epoch 95/150\n",
      "242/242 [==============================] - 0s 63us/step - loss: 0.4632 - acc: 0.7810\n",
      "Epoch 96/150\n",
      "242/242 [==============================] - 0s 61us/step - loss: 0.4626 - acc: 0.7727\n",
      "Epoch 97/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.4618 - acc: 0.7934\n",
      "Epoch 98/150\n",
      "242/242 [==============================] - 0s 50us/step - loss: 0.4643 - acc: 0.7934\n",
      "Epoch 99/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.4595 - acc: 0.7769\n",
      "Epoch 100/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.4603 - acc: 0.7893\n",
      "Epoch 101/150\n",
      "242/242 [==============================] - 0s 54us/step - loss: 0.4596 - acc: 0.7810\n",
      "Epoch 102/150\n",
      "242/242 [==============================] - 0s 56us/step - loss: 0.4641 - acc: 0.7769\n",
      "Epoch 103/150\n",
      "242/242 [==============================] - 0s 51us/step - loss: 0.4591 - acc: 0.7810\n",
      "Epoch 104/150\n",
      "242/242 [==============================] - 0s 52us/step - loss: 0.4780 - acc: 0.7934\n",
      "Epoch 105/150\n",
      "242/242 [==============================] - 0s 54us/step - loss: 0.4920 - acc: 0.7727\n",
      "Epoch 106/150\n",
      "242/242 [==============================] - 0s 53us/step - loss: 0.4716 - acc: 0.8140\n",
      "Epoch 107/150\n",
      "242/242 [==============================] - 0s 55us/step - loss: 0.4884 - acc: 0.7686\n",
      "Epoch 108/150\n",
      "242/242 [==============================] - 0s 50us/step - loss: 0.4676 - acc: 0.7810\n",
      "Epoch 109/150\n",
      "242/242 [==============================] - 0s 50us/step - loss: 0.4810 - acc: 0.7686\n",
      "Epoch 110/150\n",
      "242/242 [==============================] - 0s 52us/step - loss: 0.4697 - acc: 0.7686\n",
      "Epoch 111/150\n",
      "242/242 [==============================] - 0s 51us/step - loss: 0.4563 - acc: 0.8058\n",
      "Epoch 112/150\n",
      "242/242 [==============================] - 0s 50us/step - loss: 0.4430 - acc: 0.7893\n",
      "Epoch 113/150\n",
      "242/242 [==============================] - 0s 50us/step - loss: 0.4395 - acc: 0.7975\n",
      "Epoch 114/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 55us/step - loss: 0.4370 - acc: 0.8058\n",
      "Epoch 115/150\n",
      "242/242 [==============================] - 0s 52us/step - loss: 0.4346 - acc: 0.8017\n",
      "Epoch 116/150\n",
      "242/242 [==============================] - 0s 50us/step - loss: 0.4575 - acc: 0.7893\n",
      "Epoch 117/150\n",
      "242/242 [==============================] - 0s 63us/step - loss: 0.4418 - acc: 0.8058\n",
      "Epoch 118/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.4407 - acc: 0.8017\n",
      "Epoch 119/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.4325 - acc: 0.8058\n",
      "Epoch 120/150\n",
      "242/242 [==============================] - 0s 52us/step - loss: 0.4335 - acc: 0.8058\n",
      "Epoch 121/150\n",
      "242/242 [==============================] - 0s 56us/step - loss: 0.4287 - acc: 0.8140\n",
      "Epoch 122/150\n",
      "242/242 [==============================] - 0s 54us/step - loss: 0.4291 - acc: 0.8099\n",
      "Epoch 123/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.4279 - acc: 0.8017\n",
      "Epoch 124/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.4361 - acc: 0.7686\n",
      "Epoch 125/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.4404 - acc: 0.8058\n",
      "Epoch 126/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.4386 - acc: 0.7934\n",
      "Epoch 127/150\n",
      "242/242 [==============================] - 0s 61us/step - loss: 0.4662 - acc: 0.7851\n",
      "Epoch 128/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.4274 - acc: 0.8058\n",
      "Epoch 129/150\n",
      "242/242 [==============================] - 0s 60us/step - loss: 0.4227 - acc: 0.8223\n",
      "Epoch 130/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.4223 - acc: 0.8017\n",
      "Epoch 131/150\n",
      "242/242 [==============================] - 0s 55us/step - loss: 0.4244 - acc: 0.8017\n",
      "Epoch 132/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.4201 - acc: 0.8223\n",
      "Epoch 133/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.4245 - acc: 0.8182\n",
      "Epoch 134/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.4263 - acc: 0.7975\n",
      "Epoch 135/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.4178 - acc: 0.8099\n",
      "Epoch 136/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.4188 - acc: 0.8058\n",
      "Epoch 137/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.4148 - acc: 0.8182\n",
      "Epoch 138/150\n",
      "242/242 [==============================] - 0s 61us/step - loss: 0.4157 - acc: 0.8140\n",
      "Epoch 139/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.4215 - acc: 0.7893\n",
      "Epoch 140/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.4092 - acc: 0.8223\n",
      "Epoch 141/150\n",
      "242/242 [==============================] - 0s 54us/step - loss: 0.4166 - acc: 0.8099\n",
      "Epoch 142/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.4177 - acc: 0.8140\n",
      "Epoch 143/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.4300 - acc: 0.8017\n",
      "Epoch 144/150\n",
      "242/242 [==============================] - 0s 55us/step - loss: 0.4116 - acc: 0.8140\n",
      "Epoch 145/150\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.4111 - acc: 0.8264\n",
      "Epoch 146/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.4105 - acc: 0.8182\n",
      "Epoch 147/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.4391 - acc: 0.7934\n",
      "Epoch 148/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.4259 - acc: 0.8264\n",
      "Epoch 149/150\n",
      "242/242 [==============================] - 0s 58us/step - loss: 0.4009 - acc: 0.8264\n",
      "Epoch 150/150\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.4059 - acc: 0.8223\n",
      "61/61 [==============================] - 0s 7ms/step\n",
      "Epoch 1/150\n",
      "243/243 [==============================] - 1s 5ms/step - loss: 3.4014 - acc: 0.5802\n",
      "Epoch 2/150\n",
      "243/243 [==============================] - 0s 62us/step - loss: 1.5210 - acc: 0.6543\n",
      "Epoch 3/150\n",
      "243/243 [==============================] - 0s 57us/step - loss: 1.3790 - acc: 0.6214\n",
      "Epoch 4/150\n",
      "243/243 [==============================] - 0s 58us/step - loss: 1.0222 - acc: 0.6502\n",
      "Epoch 5/150\n",
      "243/243 [==============================] - 0s 57us/step - loss: 0.9381 - acc: 0.6996\n",
      "Epoch 6/150\n",
      "243/243 [==============================] - 0s 59us/step - loss: 0.9053 - acc: 0.6872\n",
      "Epoch 7/150\n",
      "243/243 [==============================] - 0s 60us/step - loss: 0.8516 - acc: 0.7078\n",
      "Epoch 8/150\n",
      "243/243 [==============================] - 0s 60us/step - loss: 0.8370 - acc: 0.7119\n",
      "Epoch 9/150\n",
      "243/243 [==============================] - 0s 57us/step - loss: 0.8049 - acc: 0.7037\n",
      "Epoch 10/150\n",
      "243/243 [==============================] - 0s 58us/step - loss: 0.7849 - acc: 0.7119\n",
      "Epoch 11/150\n",
      "243/243 [==============================] - 0s 59us/step - loss: 0.7743 - acc: 0.7202\n",
      "Epoch 12/150\n",
      "243/243 [==============================] - 0s 58us/step - loss: 0.7610 - acc: 0.7243\n",
      "Epoch 13/150\n",
      "243/243 [==============================] - 0s 60us/step - loss: 0.7427 - acc: 0.7243\n",
      "Epoch 14/150\n",
      "243/243 [==============================] - 0s 63us/step - loss: 0.7336 - acc: 0.7119\n",
      "Epoch 15/150\n",
      "243/243 [==============================] - 0s 60us/step - loss: 0.7146 - acc: 0.7284\n",
      "Epoch 16/150\n",
      "243/243 [==============================] - 0s 62us/step - loss: 0.7050 - acc: 0.7119\n",
      "Epoch 17/150\n",
      "243/243 [==============================] - 0s 61us/step - loss: 0.7093 - acc: 0.7243\n",
      "Epoch 18/150\n",
      "243/243 [==============================] - 0s 63us/step - loss: 0.7090 - acc: 0.6996\n",
      "Epoch 19/150\n",
      "243/243 [==============================] - 0s 62us/step - loss: 0.6754 - acc: 0.7037\n",
      "Epoch 20/150\n",
      "243/243 [==============================] - 0s 61us/step - loss: 0.6733 - acc: 0.7037\n",
      "Epoch 21/150\n",
      "243/243 [==============================] - 0s 60us/step - loss: 0.6706 - acc: 0.7078\n",
      "Epoch 22/150\n",
      "243/243 [==============================] - 0s 59us/step - loss: 0.6475 - acc: 0.7407\n",
      "Epoch 23/150\n",
      "243/243 [==============================] - 0s 66us/step - loss: 0.6555 - acc: 0.7078\n",
      "Epoch 24/150\n",
      "243/243 [==============================] - 0s 61us/step - loss: 0.6384 - acc: 0.7366\n",
      "Epoch 25/150\n",
      "243/243 [==============================] - 0s 60us/step - loss: 0.6395 - acc: 0.7202\n",
      "Epoch 26/150\n",
      "243/243 [==============================] - 0s 62us/step - loss: 0.6230 - acc: 0.7531\n",
      "Epoch 27/150\n",
      "243/243 [==============================] - 0s 59us/step - loss: 0.6082 - acc: 0.7490\n",
      "Epoch 28/150\n",
      "243/243 [==============================] - 0s 61us/step - loss: 0.6199 - acc: 0.7366\n",
      "Epoch 29/150\n",
      "243/243 [==============================] - 0s 62us/step - loss: 0.6054 - acc: 0.7407\n",
      "Epoch 30/150\n",
      "243/243 [==============================] - 0s 63us/step - loss: 0.5938 - acc: 0.7531\n",
      "Epoch 31/150\n",
      "243/243 [==============================] - 0s 61us/step - loss: 0.5918 - acc: 0.7613\n",
      "Epoch 32/150\n",
      "243/243 [==============================] - 0s 62us/step - loss: 0.5891 - acc: 0.7572\n",
      "Epoch 33/150\n",
      "243/243 [==============================] - 0s 66us/step - loss: 0.5935 - acc: 0.7654\n",
      "Epoch 34/150\n",
      "243/243 [==============================] - 0s 62us/step - loss: 0.5870 - acc: 0.7284\n",
      "Epoch 35/150\n",
      "243/243 [==============================] - 0s 62us/step - loss: 0.5915 - acc: 0.7695\n",
      "Epoch 36/150\n",
      "243/243 [==============================] - 0s 62us/step - loss: 0.6180 - acc: 0.7325\n",
      "Epoch 37/150\n",
      "243/243 [==============================] - 0s 61us/step - loss: 0.6121 - acc: 0.7572\n",
      "Epoch 38/150\n",
      "243/243 [==============================] - 0s 61us/step - loss: 0.5725 - acc: 0.7407\n",
      "Epoch 39/150\n",
      "243/243 [==============================] - 0s 59us/step - loss: 0.5682 - acc: 0.7737\n",
      "Epoch 40/150\n",
      "243/243 [==============================] - 0s 61us/step - loss: 0.5615 - acc: 0.7531\n",
      "Epoch 41/150\n",
      "243/243 [==============================] - 0s 57us/step - loss: 0.5580 - acc: 0.7572\n",
      "Epoch 42/150\n",
      "243/243 [==============================] - 0s 59us/step - loss: 0.5565 - acc: 0.7819\n",
      "Epoch 43/150\n",
      "243/243 [==============================] - 0s 59us/step - loss: 0.5675 - acc: 0.7490\n",
      "Epoch 44/150\n",
      "243/243 [==============================] - 0s 62us/step - loss: 0.5764 - acc: 0.7449\n",
      "Epoch 45/150\n",
      "243/243 [==============================] - 0s 59us/step - loss: 0.5715 - acc: 0.7531\n",
      "Epoch 46/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243/243 [==============================] - 0s 61us/step - loss: 0.5437 - acc: 0.7942\n",
      "Epoch 47/150\n",
      "243/243 [==============================] - 0s 61us/step - loss: 0.5465 - acc: 0.7654\n",
      "Epoch 48/150\n",
      "243/243 [==============================] - 0s 57us/step - loss: 0.5574 - acc: 0.7737\n",
      "Epoch 49/150\n",
      "243/243 [==============================] - 0s 74us/step - loss: 0.5363 - acc: 0.7819\n",
      "Epoch 50/150\n",
      "243/243 [==============================] - 0s 60us/step - loss: 0.5405 - acc: 0.7819\n",
      "Epoch 51/150\n",
      "243/243 [==============================] - 0s 59us/step - loss: 0.5380 - acc: 0.7819\n",
      "Epoch 52/150\n",
      "243/243 [==============================] - 0s 58us/step - loss: 0.5271 - acc: 0.7737\n",
      "Epoch 53/150\n",
      "243/243 [==============================] - 0s 58us/step - loss: 0.5240 - acc: 0.7860\n",
      "Epoch 54/150\n",
      "243/243 [==============================] - 0s 57us/step - loss: 0.5306 - acc: 0.7901\n",
      "Epoch 55/150\n",
      "243/243 [==============================] - 0s 57us/step - loss: 0.5293 - acc: 0.7778\n",
      "Epoch 56/150\n",
      "243/243 [==============================] - 0s 60us/step - loss: 0.5188 - acc: 0.7778\n",
      "Epoch 57/150\n",
      "243/243 [==============================] - 0s 64us/step - loss: 0.5162 - acc: 0.8025\n",
      "Epoch 58/150\n",
      "243/243 [==============================] - 0s 61us/step - loss: 0.5109 - acc: 0.7860\n",
      "Epoch 59/150\n",
      "243/243 [==============================] - 0s 72us/step - loss: 0.5081 - acc: 0.7819\n",
      "Epoch 60/150\n",
      "243/243 [==============================] - 0s 63us/step - loss: 0.5075 - acc: 0.7984\n",
      "Epoch 61/150\n",
      "243/243 [==============================] - 0s 61us/step - loss: 0.5199 - acc: 0.7819\n",
      "Epoch 62/150\n",
      "243/243 [==============================] - 0s 60us/step - loss: 0.5422 - acc: 0.7901\n",
      "Epoch 63/150\n",
      "243/243 [==============================] - 0s 56us/step - loss: 0.5496 - acc: 0.7860\n",
      "Epoch 64/150\n",
      "243/243 [==============================] - 0s 63us/step - loss: 0.5101 - acc: 0.7984\n",
      "Epoch 65/150\n",
      "243/243 [==============================] - 0s 63us/step - loss: 0.5068 - acc: 0.8025\n",
      "Epoch 66/150\n",
      "243/243 [==============================] - 0s 60us/step - loss: 0.5010 - acc: 0.7737\n",
      "Epoch 67/150\n",
      "243/243 [==============================] - 0s 54us/step - loss: 0.4943 - acc: 0.8025\n",
      "Epoch 68/150\n",
      "243/243 [==============================] - 0s 53us/step - loss: 0.4952 - acc: 0.7984\n",
      "Epoch 69/150\n",
      "243/243 [==============================] - 0s 57us/step - loss: 0.5031 - acc: 0.8107\n",
      "Epoch 70/150\n",
      "243/243 [==============================] - 0s 57us/step - loss: 0.5029 - acc: 0.7984\n",
      "Epoch 71/150\n",
      "243/243 [==============================] - 0s 54us/step - loss: 0.4923 - acc: 0.8066\n",
      "Epoch 72/150\n",
      "243/243 [==============================] - 0s 54us/step - loss: 0.4885 - acc: 0.8189\n",
      "Epoch 73/150\n",
      "243/243 [==============================] - 0s 53us/step - loss: 0.4869 - acc: 0.8025\n",
      "Epoch 74/150\n",
      "243/243 [==============================] - 0s 58us/step - loss: 0.4858 - acc: 0.8066\n",
      "Epoch 75/150\n",
      "243/243 [==============================] - 0s 51us/step - loss: 0.5001 - acc: 0.7819\n",
      "Epoch 76/150\n",
      "243/243 [==============================] - 0s 55us/step - loss: 0.5042 - acc: 0.8148\n",
      "Epoch 77/150\n",
      "243/243 [==============================] - 0s 50us/step - loss: 0.5518 - acc: 0.7490\n",
      "Epoch 78/150\n",
      "243/243 [==============================] - 0s 63us/step - loss: 0.5031 - acc: 0.8148\n",
      "Epoch 79/150\n",
      "243/243 [==============================] - 0s 62us/step - loss: 0.4851 - acc: 0.8025\n",
      "Epoch 80/150\n",
      "243/243 [==============================] - 0s 69us/step - loss: 0.4980 - acc: 0.8025\n",
      "Epoch 81/150\n",
      "243/243 [==============================] - 0s 76us/step - loss: 0.5149 - acc: 0.7984\n",
      "Epoch 82/150\n",
      "243/243 [==============================] - 0s 66us/step - loss: 0.5051 - acc: 0.8025\n",
      "Epoch 83/150\n",
      "243/243 [==============================] - 0s 64us/step - loss: 0.4756 - acc: 0.8313\n",
      "Epoch 84/150\n",
      "243/243 [==============================] - 0s 63us/step - loss: 0.4804 - acc: 0.8148\n",
      "Epoch 85/150\n",
      "243/243 [==============================] - 0s 62us/step - loss: 0.4587 - acc: 0.8025\n",
      "Epoch 86/150\n",
      "243/243 [==============================] - 0s 62us/step - loss: 0.4295 - acc: 0.8272\n",
      "Epoch 87/150\n",
      "243/243 [==============================] - 0s 61us/step - loss: 0.4171 - acc: 0.8148\n",
      "Epoch 88/150\n",
      "243/243 [==============================] - 0s 61us/step - loss: 0.4117 - acc: 0.7984\n",
      "Epoch 89/150\n",
      "243/243 [==============================] - 0s 59us/step - loss: 0.4057 - acc: 0.8230\n",
      "Epoch 90/150\n",
      "243/243 [==============================] - 0s 57us/step - loss: 0.4078 - acc: 0.8395\n",
      "Epoch 91/150\n",
      "243/243 [==============================] - 0s 63us/step - loss: 0.4137 - acc: 0.8025\n",
      "Epoch 92/150\n",
      "243/243 [==============================] - 0s 58us/step - loss: 0.4220 - acc: 0.7984\n",
      "Epoch 93/150\n",
      "243/243 [==============================] - 0s 60us/step - loss: 0.4078 - acc: 0.8189\n",
      "Epoch 94/150\n",
      "243/243 [==============================] - 0s 57us/step - loss: 0.3995 - acc: 0.7984\n",
      "Epoch 95/150\n",
      "243/243 [==============================] - 0s 59us/step - loss: 0.4009 - acc: 0.8066\n",
      "Epoch 96/150\n",
      "243/243 [==============================] - 0s 62us/step - loss: 0.4004 - acc: 0.8189\n",
      "Epoch 97/150\n",
      "243/243 [==============================] - 0s 65us/step - loss: 0.4395 - acc: 0.7942\n",
      "Epoch 98/150\n",
      "243/243 [==============================] - 0s 59us/step - loss: 0.4243 - acc: 0.8189\n",
      "Epoch 99/150\n",
      "243/243 [==============================] - 0s 66us/step - loss: 0.4135 - acc: 0.8148\n",
      "Epoch 100/150\n",
      "243/243 [==============================] - 0s 62us/step - loss: 0.4203 - acc: 0.8066\n",
      "Epoch 101/150\n",
      "243/243 [==============================] - 0s 66us/step - loss: 0.3992 - acc: 0.8272\n",
      "Epoch 102/150\n",
      "243/243 [==============================] - 0s 63us/step - loss: 0.4005 - acc: 0.8189\n",
      "Epoch 103/150\n",
      "243/243 [==============================] - 0s 62us/step - loss: 0.4002 - acc: 0.8354\n",
      "Epoch 104/150\n",
      "243/243 [==============================] - 0s 62us/step - loss: 0.4027 - acc: 0.8025\n",
      "Epoch 105/150\n",
      "243/243 [==============================] - 0s 61us/step - loss: 0.4062 - acc: 0.8148\n",
      "Epoch 106/150\n",
      "243/243 [==============================] - 0s 60us/step - loss: 0.3988 - acc: 0.8107\n",
      "Epoch 107/150\n",
      "243/243 [==============================] - 0s 62us/step - loss: 0.3911 - acc: 0.8230\n",
      "Epoch 108/150\n",
      "243/243 [==============================] - 0s 60us/step - loss: 0.4026 - acc: 0.8230\n",
      "Epoch 109/150\n",
      "243/243 [==============================] - 0s 58us/step - loss: 0.3904 - acc: 0.8189\n",
      "Epoch 110/150\n",
      "243/243 [==============================] - 0s 57us/step - loss: 0.3984 - acc: 0.8189\n",
      "Epoch 111/150\n",
      "243/243 [==============================] - 0s 58us/step - loss: 0.4013 - acc: 0.8107\n",
      "Epoch 112/150\n",
      "243/243 [==============================] - 0s 58us/step - loss: 0.3975 - acc: 0.8107\n",
      "Epoch 113/150\n",
      "243/243 [==============================] - 0s 61us/step - loss: 0.3921 - acc: 0.8272\n",
      "Epoch 114/150\n",
      "243/243 [==============================] - 0s 62us/step - loss: 0.3991 - acc: 0.8189\n",
      "Epoch 115/150\n",
      "243/243 [==============================] - 0s 64us/step - loss: 0.4012 - acc: 0.8189\n",
      "Epoch 116/150\n",
      "243/243 [==============================] - 0s 62us/step - loss: 0.4008 - acc: 0.8066\n",
      "Epoch 117/150\n",
      "243/243 [==============================] - 0s 58us/step - loss: 0.3994 - acc: 0.8230\n",
      "Epoch 118/150\n",
      "243/243 [==============================] - 0s 60us/step - loss: 0.3989 - acc: 0.8230\n",
      "Epoch 119/150\n",
      "243/243 [==============================] - 0s 61us/step - loss: 0.3887 - acc: 0.8189\n",
      "Epoch 120/150\n",
      "243/243 [==============================] - 0s 61us/step - loss: 0.3856 - acc: 0.8313\n",
      "Epoch 121/150\n",
      "243/243 [==============================] - 0s 59us/step - loss: 0.3872 - acc: 0.8148\n",
      "Epoch 122/150\n",
      "243/243 [==============================] - 0s 59us/step - loss: 0.3835 - acc: 0.8313\n",
      "Epoch 123/150\n",
      "243/243 [==============================] - 0s 58us/step - loss: 0.3928 - acc: 0.8354\n",
      "Epoch 124/150\n",
      "243/243 [==============================] - 0s 59us/step - loss: 0.3877 - acc: 0.8230\n",
      "Epoch 125/150\n",
      "243/243 [==============================] - 0s 66us/step - loss: 0.3924 - acc: 0.8313\n",
      "Epoch 126/150\n",
      "243/243 [==============================] - 0s 60us/step - loss: 0.3821 - acc: 0.8272\n",
      "Epoch 127/150\n",
      "243/243 [==============================] - 0s 61us/step - loss: 0.3908 - acc: 0.8230\n",
      "Epoch 128/150\n",
      "243/243 [==============================] - 0s 65us/step - loss: 0.3834 - acc: 0.8272\n",
      "Epoch 129/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243/243 [==============================] - 0s 60us/step - loss: 0.3943 - acc: 0.8107\n",
      "Epoch 130/150\n",
      "243/243 [==============================] - 0s 56us/step - loss: 0.3843 - acc: 0.8230\n",
      "Epoch 131/150\n",
      "243/243 [==============================] - 0s 59us/step - loss: 0.3806 - acc: 0.8313\n",
      "Epoch 132/150\n",
      "243/243 [==============================] - 0s 60us/step - loss: 0.3807 - acc: 0.8230\n",
      "Epoch 133/150\n",
      "243/243 [==============================] - 0s 60us/step - loss: 0.3922 - acc: 0.8395\n",
      "Epoch 134/150\n",
      "243/243 [==============================] - 0s 62us/step - loss: 0.3837 - acc: 0.8189\n",
      "Epoch 135/150\n",
      "243/243 [==============================] - 0s 59us/step - loss: 0.3896 - acc: 0.8230\n",
      "Epoch 136/150\n",
      "243/243 [==============================] - 0s 59us/step - loss: 0.3798 - acc: 0.8272\n",
      "Epoch 137/150\n",
      "243/243 [==============================] - 0s 61us/step - loss: 0.3834 - acc: 0.8107\n",
      "Epoch 138/150\n",
      "243/243 [==============================] - 0s 62us/step - loss: 0.3787 - acc: 0.8395\n",
      "Epoch 139/150\n",
      "243/243 [==============================] - 0s 62us/step - loss: 0.3814 - acc: 0.8395\n",
      "Epoch 140/150\n",
      "243/243 [==============================] - 0s 63us/step - loss: 0.3886 - acc: 0.8066\n",
      "Epoch 141/150\n",
      "243/243 [==============================] - 0s 63us/step - loss: 0.4032 - acc: 0.8230\n",
      "Epoch 142/150\n",
      "243/243 [==============================] - 0s 63us/step - loss: 0.4033 - acc: 0.8230\n",
      "Epoch 143/150\n",
      "243/243 [==============================] - 0s 65us/step - loss: 0.3956 - acc: 0.8148\n",
      "Epoch 144/150\n",
      "243/243 [==============================] - 0s 67us/step - loss: 0.4081 - acc: 0.7901\n",
      "Epoch 145/150\n",
      "243/243 [==============================] - 0s 64us/step - loss: 0.4040 - acc: 0.7860\n",
      "Epoch 146/150\n",
      "243/243 [==============================] - 0s 67us/step - loss: 0.3825 - acc: 0.8354\n",
      "Epoch 147/150\n",
      "243/243 [==============================] - 0s 63us/step - loss: 0.3865 - acc: 0.8436\n",
      "Epoch 148/150\n",
      "243/243 [==============================] - 0s 65us/step - loss: 0.3884 - acc: 0.8107\n",
      "Epoch 149/150\n",
      "243/243 [==============================] - 0s 60us/step - loss: 0.4015 - acc: 0.8189\n",
      "Epoch 150/150\n",
      "243/243 [==============================] - 0s 65us/step - loss: 0.3760 - acc: 0.8354\n",
      "60/60 [==============================] - 0s 7ms/step\n",
      "Epoch 1/150\n",
      "243/243 [==============================] - 1s 5ms/step - loss: 0.8777 - acc: 0.6584\n",
      "Epoch 2/150\n",
      "243/243 [==============================] - 0s 64us/step - loss: 0.8087 - acc: 0.6461\n",
      "Epoch 3/150\n",
      "243/243 [==============================] - 0s 67us/step - loss: 0.7612 - acc: 0.6626\n",
      "Epoch 4/150\n",
      "243/243 [==============================] - 0s 75us/step - loss: 0.7421 - acc: 0.6502\n",
      "Epoch 5/150\n",
      "243/243 [==============================] - 0s 75us/step - loss: 0.7152 - acc: 0.6502\n",
      "Epoch 6/150\n",
      "243/243 [==============================] - 0s 68us/step - loss: 0.7027 - acc: 0.6543\n",
      "Epoch 7/150\n",
      "243/243 [==============================] - 0s 67us/step - loss: 0.6932 - acc: 0.6379\n",
      "Epoch 8/150\n",
      "243/243 [==============================] - 0s 70us/step - loss: 0.6763 - acc: 0.6584\n",
      "Epoch 9/150\n",
      "243/243 [==============================] - 0s 67us/step - loss: 0.6738 - acc: 0.6461\n",
      "Epoch 10/150\n",
      "243/243 [==============================] - 0s 69us/step - loss: 0.6580 - acc: 0.6584\n",
      "Epoch 11/150\n",
      "243/243 [==============================] - 0s 77us/step - loss: 0.6601 - acc: 0.6420\n",
      "Epoch 12/150\n",
      "243/243 [==============================] - 0s 96us/step - loss: 0.6478 - acc: 0.6502\n",
      "Epoch 13/150\n",
      "243/243 [==============================] - 0s 92us/step - loss: 0.6403 - acc: 0.6543\n",
      "Epoch 14/150\n",
      "243/243 [==============================] - 0s 81us/step - loss: 0.6380 - acc: 0.6667\n",
      "Epoch 15/150\n",
      "243/243 [==============================] - 0s 80us/step - loss: 0.6470 - acc: 0.6543\n",
      "Epoch 16/150\n",
      "243/243 [==============================] - 0s 79us/step - loss: 0.6276 - acc: 0.6667\n",
      "Epoch 17/150\n",
      "243/243 [==============================] - 0s 81us/step - loss: 0.6248 - acc: 0.6749\n",
      "Epoch 18/150\n",
      "243/243 [==============================] - 0s 81us/step - loss: 0.6234 - acc: 0.6543\n",
      "Epoch 19/150\n",
      "243/243 [==============================] - 0s 93us/step - loss: 0.6166 - acc: 0.6749\n",
      "Epoch 20/150\n",
      "243/243 [==============================] - 0s 79us/step - loss: 0.6147 - acc: 0.6667\n",
      "Epoch 21/150\n",
      "243/243 [==============================] - 0s 72us/step - loss: 0.6086 - acc: 0.6790\n",
      "Epoch 22/150\n",
      "243/243 [==============================] - 0s 76us/step - loss: 0.6069 - acc: 0.6790\n",
      "Epoch 23/150\n",
      "243/243 [==============================] - 0s 81us/step - loss: 0.6037 - acc: 0.6872\n",
      "Epoch 24/150\n",
      "243/243 [==============================] - 0s 75us/step - loss: 0.6009 - acc: 0.6626\n",
      "Epoch 25/150\n",
      "243/243 [==============================] - 0s 81us/step - loss: 0.5999 - acc: 0.6667\n",
      "Epoch 26/150\n",
      "243/243 [==============================] - 0s 83us/step - loss: 0.5915 - acc: 0.6790\n",
      "Epoch 27/150\n",
      "243/243 [==============================] - 0s 72us/step - loss: 0.5948 - acc: 0.6749\n",
      "Epoch 28/150\n",
      "243/243 [==============================] - 0s 87us/step - loss: 0.5876 - acc: 0.6872\n",
      "Epoch 29/150\n",
      "243/243 [==============================] - 0s 68us/step - loss: 0.5921 - acc: 0.6872\n",
      "Epoch 30/150\n",
      "243/243 [==============================] - 0s 69us/step - loss: 0.5840 - acc: 0.6708\n",
      "Epoch 31/150\n",
      "243/243 [==============================] - 0s 73us/step - loss: 0.5793 - acc: 0.6831\n",
      "Epoch 32/150\n",
      "243/243 [==============================] - 0s 68us/step - loss: 0.5832 - acc: 0.6790\n",
      "Epoch 33/150\n",
      "243/243 [==============================] - 0s 65us/step - loss: 0.5810 - acc: 0.6790\n",
      "Epoch 34/150\n",
      "243/243 [==============================] - 0s 61us/step - loss: 0.5827 - acc: 0.6708\n",
      "Epoch 35/150\n",
      "243/243 [==============================] - 0s 59us/step - loss: 0.6066 - acc: 0.6708\n",
      "Epoch 36/150\n",
      "243/243 [==============================] - 0s 62us/step - loss: 0.5763 - acc: 0.7037\n",
      "Epoch 37/150\n",
      "243/243 [==============================] - 0s 59us/step - loss: 0.5668 - acc: 0.6667\n",
      "Epoch 38/150\n",
      "243/243 [==============================] - 0s 63us/step - loss: 0.5690 - acc: 0.6831\n",
      "Epoch 39/150\n",
      "243/243 [==============================] - 0s 63us/step - loss: 0.5635 - acc: 0.6996\n",
      "Epoch 40/150\n",
      "243/243 [==============================] - 0s 59us/step - loss: 0.5670 - acc: 0.6872\n",
      "Epoch 41/150\n",
      "243/243 [==============================] - 0s 63us/step - loss: 0.5587 - acc: 0.6955\n",
      "Epoch 42/150\n",
      "243/243 [==============================] - 0s 60us/step - loss: 0.5609 - acc: 0.6872\n",
      "Epoch 43/150\n",
      "243/243 [==============================] - 0s 59us/step - loss: 0.5569 - acc: 0.7037\n",
      "Epoch 44/150\n",
      "243/243 [==============================] - 0s 61us/step - loss: 0.5545 - acc: 0.7078\n",
      "Epoch 45/150\n",
      "243/243 [==============================] - 0s 62us/step - loss: 0.5522 - acc: 0.6955\n",
      "Epoch 46/150\n",
      "243/243 [==============================] - 0s 62us/step - loss: 0.5546 - acc: 0.6914\n",
      "Epoch 47/150\n",
      "243/243 [==============================] - 0s 62us/step - loss: 0.5574 - acc: 0.7037\n",
      "Epoch 48/150\n",
      "243/243 [==============================] - 0s 64us/step - loss: 0.5473 - acc: 0.7078\n",
      "Epoch 49/150\n",
      "243/243 [==============================] - 0s 65us/step - loss: 0.5480 - acc: 0.6996\n",
      "Epoch 50/150\n",
      "243/243 [==============================] - 0s 60us/step - loss: 0.5477 - acc: 0.7119\n",
      "Epoch 51/150\n",
      "243/243 [==============================] - 0s 61us/step - loss: 0.5527 - acc: 0.7243\n",
      "Epoch 52/150\n",
      "243/243 [==============================] - 0s 64us/step - loss: 0.5478 - acc: 0.6872\n",
      "Epoch 53/150\n",
      "243/243 [==============================] - 0s 65us/step - loss: 0.5502 - acc: 0.7160\n",
      "Epoch 54/150\n",
      "243/243 [==============================] - 0s 60us/step - loss: 0.5338 - acc: 0.7243\n",
      "Epoch 55/150\n",
      "243/243 [==============================] - 0s 63us/step - loss: 0.5394 - acc: 0.7078\n",
      "Epoch 56/150\n",
      "243/243 [==============================] - 0s 65us/step - loss: 0.5337 - acc: 0.7243\n",
      "Epoch 57/150\n",
      "243/243 [==============================] - 0s 73us/step - loss: 0.5319 - acc: 0.7366\n",
      "Epoch 58/150\n",
      "243/243 [==============================] - 0s 76us/step - loss: 0.5327 - acc: 0.7202\n",
      "Epoch 59/150\n",
      "243/243 [==============================] - 0s 78us/step - loss: 0.5309 - acc: 0.7202\n",
      "Epoch 60/150\n",
      "243/243 [==============================] - 0s 73us/step - loss: 0.5279 - acc: 0.7284\n",
      "Epoch 61/150\n",
      "243/243 [==============================] - 0s 82us/step - loss: 0.5330 - acc: 0.7366\n",
      "Epoch 62/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243/243 [==============================] - 0s 70us/step - loss: 0.5381 - acc: 0.7119\n",
      "Epoch 63/150\n",
      "243/243 [==============================] - 0s 80us/step - loss: 0.5456 - acc: 0.7449\n",
      "Epoch 64/150\n",
      "243/243 [==============================] - 0s 76us/step - loss: 0.5425 - acc: 0.7202\n",
      "Epoch 65/150\n",
      "243/243 [==============================] - 0s 77us/step - loss: 0.5299 - acc: 0.7490\n",
      "Epoch 66/150\n",
      "243/243 [==============================] - 0s 81us/step - loss: 0.5178 - acc: 0.7284\n",
      "Epoch 67/150\n",
      "243/243 [==============================] - 0s 86us/step - loss: 0.5254 - acc: 0.7119\n",
      "Epoch 68/150\n",
      "243/243 [==============================] - 0s 79us/step - loss: 0.5283 - acc: 0.7407\n",
      "Epoch 69/150\n",
      "243/243 [==============================] - 0s 82us/step - loss: 0.5130 - acc: 0.7325\n",
      "Epoch 70/150\n",
      "243/243 [==============================] - 0s 81us/step - loss: 0.5143 - acc: 0.7407\n",
      "Epoch 71/150\n",
      "243/243 [==============================] - 0s 72us/step - loss: 0.5085 - acc: 0.7407\n",
      "Epoch 72/150\n",
      "243/243 [==============================] - 0s 67us/step - loss: 0.5068 - acc: 0.7325\n",
      "Epoch 73/150\n",
      "243/243 [==============================] - 0s 67us/step - loss: 0.5077 - acc: 0.7449\n",
      "Epoch 74/150\n",
      "243/243 [==============================] - 0s 67us/step - loss: 0.5048 - acc: 0.7449\n",
      "Epoch 75/150\n",
      "243/243 [==============================] - 0s 62us/step - loss: 0.5082 - acc: 0.7407\n",
      "Epoch 76/150\n",
      "243/243 [==============================] - 0s 69us/step - loss: 0.5066 - acc: 0.7490\n",
      "Epoch 77/150\n",
      "243/243 [==============================] - 0s 75us/step - loss: 0.5057 - acc: 0.7284\n",
      "Epoch 78/150\n",
      "243/243 [==============================] - 0s 68us/step - loss: 0.4982 - acc: 0.7490\n",
      "Epoch 79/150\n",
      "243/243 [==============================] - 0s 65us/step - loss: 0.4956 - acc: 0.7449\n",
      "Epoch 80/150\n",
      "243/243 [==============================] - 0s 64us/step - loss: 0.4954 - acc: 0.7531\n",
      "Epoch 81/150\n",
      "243/243 [==============================] - 0s 64us/step - loss: 0.4985 - acc: 0.7449\n",
      "Epoch 82/150\n",
      "243/243 [==============================] - 0s 63us/step - loss: 0.5125 - acc: 0.7366\n",
      "Epoch 83/150\n",
      "243/243 [==============================] - 0s 62us/step - loss: 0.5159 - acc: 0.7490\n",
      "Epoch 84/150\n",
      "243/243 [==============================] - 0s 62us/step - loss: 0.4846 - acc: 0.7449\n",
      "Epoch 85/150\n",
      "243/243 [==============================] - 0s 64us/step - loss: 0.4911 - acc: 0.7572\n",
      "Epoch 86/150\n",
      "243/243 [==============================] - 0s 65us/step - loss: 0.4904 - acc: 0.7531\n",
      "Epoch 87/150\n",
      "243/243 [==============================] - 0s 64us/step - loss: 0.4857 - acc: 0.7654\n",
      "Epoch 88/150\n",
      "243/243 [==============================] - 0s 64us/step - loss: 0.4847 - acc: 0.7572\n",
      "Epoch 89/150\n",
      "243/243 [==============================] - 0s 63us/step - loss: 0.4829 - acc: 0.7572\n",
      "Epoch 90/150\n",
      "243/243 [==============================] - 0s 64us/step - loss: 0.4888 - acc: 0.7613\n",
      "Epoch 91/150\n",
      "243/243 [==============================] - 0s 66us/step - loss: 0.4991 - acc: 0.7531\n",
      "Epoch 92/150\n",
      "243/243 [==============================] - 0s 63us/step - loss: 0.4947 - acc: 0.7654\n",
      "Epoch 93/150\n",
      "243/243 [==============================] - 0s 61us/step - loss: 0.4772 - acc: 0.7490\n",
      "Epoch 94/150\n",
      "243/243 [==============================] - 0s 60us/step - loss: 0.4847 - acc: 0.7449\n",
      "Epoch 95/150\n",
      "243/243 [==============================] - 0s 58us/step - loss: 0.4736 - acc: 0.7778\n",
      "Epoch 96/150\n",
      "243/243 [==============================] - 0s 57us/step - loss: 0.4740 - acc: 0.7572\n",
      "Epoch 97/150\n",
      "243/243 [==============================] - 0s 59us/step - loss: 0.4718 - acc: 0.7654\n",
      "Epoch 98/150\n",
      "243/243 [==============================] - 0s 63us/step - loss: 0.4727 - acc: 0.7778\n",
      "Epoch 99/150\n",
      "243/243 [==============================] - 0s 61us/step - loss: 0.4701 - acc: 0.7737\n",
      "Epoch 100/150\n",
      "243/243 [==============================] - 0s 61us/step - loss: 0.4698 - acc: 0.7695\n",
      "Epoch 101/150\n",
      "243/243 [==============================] - 0s 63us/step - loss: 0.4666 - acc: 0.7654\n",
      "Epoch 102/150\n",
      "243/243 [==============================] - 0s 64us/step - loss: 0.4672 - acc: 0.7695\n",
      "Epoch 103/150\n",
      "243/243 [==============================] - 0s 61us/step - loss: 0.4669 - acc: 0.7654\n",
      "Epoch 104/150\n",
      "243/243 [==============================] - 0s 61us/step - loss: 0.4660 - acc: 0.7737\n",
      "Epoch 105/150\n",
      "243/243 [==============================] - 0s 62us/step - loss: 0.4625 - acc: 0.7737\n",
      "Epoch 106/150\n",
      "243/243 [==============================] - 0s 60us/step - loss: 0.4593 - acc: 0.7778\n",
      "Epoch 107/150\n",
      "243/243 [==============================] - 0s 63us/step - loss: 0.4636 - acc: 0.7654\n",
      "Epoch 108/150\n",
      "243/243 [==============================] - 0s 63us/step - loss: 0.4811 - acc: 0.7778\n",
      "Epoch 109/150\n",
      "243/243 [==============================] - 0s 61us/step - loss: 0.4730 - acc: 0.7572\n",
      "Epoch 110/150\n",
      "243/243 [==============================] - 0s 61us/step - loss: 0.4630 - acc: 0.7901\n",
      "Epoch 111/150\n",
      "243/243 [==============================] - 0s 63us/step - loss: 0.4579 - acc: 0.7778\n",
      "Epoch 112/150\n",
      "243/243 [==============================] - 0s 62us/step - loss: 0.4537 - acc: 0.7778\n",
      "Epoch 113/150\n",
      "243/243 [==============================] - 0s 58us/step - loss: 0.4511 - acc: 0.7860\n",
      "Epoch 114/150\n",
      "243/243 [==============================] - 0s 63us/step - loss: 0.4509 - acc: 0.7860\n",
      "Epoch 115/150\n",
      "243/243 [==============================] - 0s 63us/step - loss: 0.4496 - acc: 0.7778\n",
      "Epoch 116/150\n",
      "243/243 [==============================] - 0s 61us/step - loss: 0.4490 - acc: 0.7860\n",
      "Epoch 117/150\n",
      "243/243 [==============================] - 0s 60us/step - loss: 0.4465 - acc: 0.7819\n",
      "Epoch 118/150\n",
      "243/243 [==============================] - 0s 63us/step - loss: 0.4518 - acc: 0.7695\n",
      "Epoch 119/150\n",
      "243/243 [==============================] - 0s 64us/step - loss: 0.4432 - acc: 0.7860\n",
      "Epoch 120/150\n",
      "243/243 [==============================] - 0s 61us/step - loss: 0.4421 - acc: 0.7984\n",
      "Epoch 121/150\n",
      "243/243 [==============================] - 0s 69us/step - loss: 0.4480 - acc: 0.7778\n",
      "Epoch 122/150\n",
      "243/243 [==============================] - 0s 67us/step - loss: 0.4374 - acc: 0.7942\n",
      "Epoch 123/150\n",
      "243/243 [==============================] - 0s 69us/step - loss: 0.4475 - acc: 0.7819\n",
      "Epoch 124/150\n",
      "243/243 [==============================] - 0s 68us/step - loss: 0.4559 - acc: 0.7737\n",
      "Epoch 125/150\n",
      "243/243 [==============================] - 0s 71us/step - loss: 0.4345 - acc: 0.7942\n",
      "Epoch 126/150\n",
      "243/243 [==============================] - 0s 72us/step - loss: 0.4306 - acc: 0.8066\n",
      "Epoch 127/150\n",
      "243/243 [==============================] - 0s 77us/step - loss: 0.4275 - acc: 0.8066\n",
      "Epoch 128/150\n",
      "243/243 [==============================] - 0s 72us/step - loss: 0.4337 - acc: 0.7984\n",
      "Epoch 129/150\n",
      "243/243 [==============================] - 0s 73us/step - loss: 0.4345 - acc: 0.7942\n",
      "Epoch 130/150\n",
      "243/243 [==============================] - 0s 72us/step - loss: 0.4237 - acc: 0.8066\n",
      "Epoch 131/150\n",
      "243/243 [==============================] - 0s 69us/step - loss: 0.4208 - acc: 0.8025\n",
      "Epoch 132/150\n",
      "243/243 [==============================] - 0s 71us/step - loss: 0.4228 - acc: 0.8025\n",
      "Epoch 133/150\n",
      "243/243 [==============================] - 0s 68us/step - loss: 0.4204 - acc: 0.8107\n",
      "Epoch 134/150\n",
      "243/243 [==============================] - 0s 75us/step - loss: 0.4180 - acc: 0.8189\n",
      "Epoch 135/150\n",
      "243/243 [==============================] - 0s 68us/step - loss: 0.4140 - acc: 0.8066\n",
      "Epoch 136/150\n",
      "243/243 [==============================] - 0s 70us/step - loss: 0.4226 - acc: 0.8025\n",
      "Epoch 137/150\n",
      "243/243 [==============================] - 0s 66us/step - loss: 0.4147 - acc: 0.8107\n",
      "Epoch 138/150\n",
      "243/243 [==============================] - 0s 65us/step - loss: 0.4138 - acc: 0.8148\n",
      "Epoch 139/150\n",
      "243/243 [==============================] - 0s 67us/step - loss: 0.4111 - acc: 0.7942\n",
      "Epoch 140/150\n",
      "243/243 [==============================] - 0s 69us/step - loss: 0.4110 - acc: 0.8107\n",
      "Epoch 141/150\n",
      "243/243 [==============================] - 0s 67us/step - loss: 0.4161 - acc: 0.8107\n",
      "Epoch 142/150\n",
      "243/243 [==============================] - 0s 58us/step - loss: 0.4080 - acc: 0.8148\n",
      "Epoch 143/150\n",
      "243/243 [==============================] - 0s 69us/step - loss: 0.4062 - acc: 0.8148\n",
      "Epoch 144/150\n",
      "243/243 [==============================] - 0s 62us/step - loss: 0.4061 - acc: 0.8066\n",
      "Epoch 145/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243/243 [==============================] - 0s 60us/step - loss: 0.4013 - acc: 0.8066\n",
      "Epoch 146/150\n",
      "243/243 [==============================] - 0s 58us/step - loss: 0.4111 - acc: 0.8189\n",
      "Epoch 147/150\n",
      "243/243 [==============================] - 0s 65us/step - loss: 0.3992 - acc: 0.8107\n",
      "Epoch 148/150\n",
      "243/243 [==============================] - 0s 65us/step - loss: 0.4063 - acc: 0.8148\n",
      "Epoch 149/150\n",
      "243/243 [==============================] - 0s 68us/step - loss: 0.3986 - acc: 0.8066\n",
      "Epoch 150/150\n",
      "243/243 [==============================] - 0s 59us/step - loss: 0.4021 - acc: 0.8066\n",
      "60/60 [==============================] - 0s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "def baseline_model():\n",
    "  model = Sequential()\n",
    "  model.add(Dense(13, input_dim=X.shape[1], activation='relu'))\n",
    "  model.add(Dense(8, activation='relu'))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "seed = 101\n",
    "np.random.seed(seed)\n",
    "estimator = KerasClassifier(build_fn=baseline_model, \n",
    "                                     epochs=150,\n",
    "                                     batch_size=32,\n",
    "                                     verbose=1)\n",
    "kfold = StratifiedKFold(n_splits=5, random_state=seed)\n",
    "results = cross_val_score(estimator, X, y, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: 0.77, 0.09\n"
     ]
    }
   ],
   "source": [
    "print(f'Results: {results.mean():.2f}, {results.std():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our mean accuracy is 0.77 with a std-dev of 0.09. That's not bad for a first shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating grid search\n",
      "Fitting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.8151815132339402 using {'batch_size': 64, 'epochs': 180}\n",
      "Means: 0.5445544549537571, Stdev: 0.32498273026560126 with: {'batch_size': 32, 'epochs': 20}\n",
      "Means: 0.6633663321092184, Stdev: 0.3000055946246005 with: {'batch_size': 32, 'epochs': 40}\n",
      "Means: 0.570957094922711, Stdev: 0.3273572743345168 with: {'batch_size': 32, 'epochs': 60}\n",
      "Means: 0.6666666638142992, Stdev: 0.3654260837339631 with: {'batch_size': 32, 'epochs': 80}\n",
      "Means: 0.21122111876805624, Stdev: 0.2956329437494221 with: {'batch_size': 32, 'epochs': 100}\n",
      "Means: 0.6105610582694756, Stdev: 0.34514733472950465 with: {'batch_size': 32, 'epochs': 120}\n",
      "Means: 0.45874587409567125, Stdev: 0.39785922778490446 with: {'batch_size': 32, 'epochs': 140}\n",
      "Means: 0.39273927422246524, Stdev: 0.32194238291140614 with: {'batch_size': 32, 'epochs': 160}\n",
      "Means: 0.7458745815572959, Stdev: 0.08978753422069291 with: {'batch_size': 32, 'epochs': 180}\n",
      "Means: 0.5841584187923091, Stdev: 0.27156821591288166 with: {'batch_size': 64, 'epochs': 20}\n",
      "Means: 0.2640264036361337, Stdev: 0.38625628340825996 with: {'batch_size': 64, 'epochs': 40}\n",
      "Means: 0.4983498277050434, Stdev: 0.3460004318320958 with: {'batch_size': 64, 'epochs': 60}\n",
      "Means: 0.5577557747906977, Stdev: 0.3930285538257894 with: {'batch_size': 64, 'epochs': 80}\n",
      "Means: 0.6072607302036223, Stdev: 0.32714365867878714 with: {'batch_size': 64, 'epochs': 100}\n",
      "Means: 0.5973597359735974, Stdev: 0.34450005828793745 with: {'batch_size': 64, 'epochs': 120}\n",
      "Means: 0.5379537914452379, Stdev: 0.33155469414138 with: {'batch_size': 64, 'epochs': 140}\n",
      "Means: 0.5412541232486763, Stdev: 0.27859653319484795 with: {'batch_size': 64, 'epochs': 160}\n",
      "Means: 0.8151815132339402, Stdev: 0.17906995353507357 with: {'batch_size': 64, 'epochs': 180}\n",
      "Means: 0.6732673208312233, Stdev: 0.36476377273789495 with: {'batch_size': 96, 'epochs': 20}\n",
      "Means: 0.3597359665156198, Stdev: 0.29362577634238013 with: {'batch_size': 96, 'epochs': 40}\n",
      "Means: 0.34653464677703655, Stdev: 0.18991823771080094 with: {'batch_size': 96, 'epochs': 60}\n",
      "Means: 0.34983498230576515, Stdev: 0.42318151828182166 with: {'batch_size': 96, 'epochs': 80}\n",
      "Means: 0.6435643584027936, Stdev: 0.24369100679491376 with: {'batch_size': 96, 'epochs': 100}\n",
      "Means: 0.4686468589817337, Stdev: 0.28467384043978783 with: {'batch_size': 96, 'epochs': 120}\n",
      "Means: 0.42904290409371404, Stdev: 0.26210592086856094 with: {'batch_size': 96, 'epochs': 140}\n",
      "Means: 0.6303630321726154, Stdev: 0.3555586475338533 with: {'batch_size': 96, 'epochs': 160}\n",
      "Means: 0.4521452155050271, Stdev: 0.36795446864800235 with: {'batch_size': 96, 'epochs': 180}\n"
     ]
    }
   ],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 101\n",
    "np.random.seed(seed)\n",
    "\n",
    "def create_model():\n",
    "  model = Sequential()\n",
    "  model.add(Dense(13, input_dim=X.shape[1], activation='relu'))\n",
    "  model.add(Dense(8, activation='relu'))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "param_grid = {'batch_size': range(32, 128, 32),\n",
    "              'epochs':     range(20, 200, 20)}\n",
    "\n",
    "print('Creating grid search')\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=5)\n",
    "\n",
    "print('Fitting')\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's getting better - 0.81 accuracy with 180 epochs and batch_size of 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating grid search\n",
      "Fitting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.8580858097611481 using {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.2, 'momentum': 0.8}\n",
      "Means: 0.528052807837823, Stdev: 0.4503243332742807 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.001, 'momentum': 0.0}\n",
      "Means: 0.1782178276836282, Stdev: 0.23886222553010986 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.001, 'momentum': 0.2}\n",
      "Means: 0.34323432225205325, Stdev: 0.42842525954602934 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.001, 'momentum': 0.4}\n",
      "Means: 0.4587458757677487, Stdev: 0.45433663507435285 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.001, 'momentum': 0.6}\n",
      "Means: 0.23102310290037603, Stdev: 0.2844536638898839 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.001, 'momentum': 0.8}\n",
      "Means: 0.11881188472898879, Stdev: 0.14572282646488985 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.001, 'momentum': 0.9}\n",
      "Means: 0.5412541242322513, Stdev: 0.45433663507435285 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.01, 'momentum': 0.0}\n",
      "Means: 0.8547854716628298, Stdev: 0.1817044780743494 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.01, 'momentum': 0.2}\n",
      "Means: 0.5445544542652545, Stdev: 0.4540248686382509 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.01, 'momentum': 0.4}\n",
      "Means: 0.33993399221904996, Stdev: 0.4272031763574851 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.01, 'momentum': 0.6}\n",
      "Means: 0.4587458757677487, Stdev: 0.45433663507435285 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.01, 'momentum': 0.8}\n",
      "Means: 0.66006600778095, Stdev: 0.4272031763574851 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.01, 'momentum': 0.9}\n",
      "Means: 0.2607260737875507, Stdev: 0.38842202431342 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.1, 'momentum': 0.0}\n",
      "Means: 0.66006600778095, Stdev: 0.4272031763574851 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.1, 'momentum': 0.2}\n",
      "Means: 0.6567656777479468, Stdev: 0.4284252595460293 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.1, 'momentum': 0.4}\n",
      "Means: 0.34323432225205325, Stdev: 0.42842525954602934 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.1, 'momentum': 0.6}\n",
      "Means: 0.46204620580075206, Stdev: 0.45462422971365796 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.1, 'momentum': 0.8}\n",
      "Means: 0.66006600778095, Stdev: 0.4272031763574851 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.1, 'momentum': 0.9}\n",
      "Means: 0.4587458757677487, Stdev: 0.45433663507435285 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.2, 'momentum': 0.0}\n",
      "Means: 0.5445544542652545, Stdev: 0.4540248686382509 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.2, 'momentum': 0.2}\n",
      "Means: 0.4587458757677487, Stdev: 0.45433663507435285 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.2, 'momentum': 0.4}\n",
      "Means: 0.0594059417743494, Stdev: 0.11832394767378107 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.2, 'momentum': 0.6}\n",
      "Means: 0.8580858097611481, Stdev: 0.2826627559204759 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.2, 'momentum': 0.8}\n",
      "Means: 0.1419141902388519, Stdev: 0.2826627559204759 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.2, 'momentum': 0.9}\n",
      "Means: 0.6567656777479468, Stdev: 0.4284252595460293 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.3, 'momentum': 0.0}\n",
      "Means: 0.5445544542652545, Stdev: 0.4540248686382509 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.3, 'momentum': 0.2}\n",
      "Means: 0.33993399221904996, Stdev: 0.4272031763574851 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.3, 'momentum': 0.4}\n",
      "Means: 0.4587458757677487, Stdev: 0.45433663507435285 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.3, 'momentum': 0.6}\n",
      "Means: 0.33993399221904996, Stdev: 0.4272031763574851 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.3, 'momentum': 0.8}\n",
      "Means: 0.0594059417743494, Stdev: 0.11832394767378107 with: {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.3, 'momentum': 0.9}\n"
     ]
    }
   ],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 101\n",
    "np.random.seed(seed)\n",
    "\n",
    "def create_model(learn_rate=0.01, momentum=0):\n",
    "  model = Sequential()\n",
    "  model.add(Dense(13, input_dim=X.shape[1], activation='relu'))\n",
    "  model.add(Dense(8, activation='relu'))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "  optimizer = SGD(lr=learn_rate, momentum=momentum)\n",
    "  model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "param_grid = {'batch_size': [64],\n",
    "              'epochs':     [180],\n",
    "              'learn_rate': [0.001, 0.01, 0.1, 0.2, 0.3],\n",
    "              'momentum':   [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]}\n",
    "\n",
    "print('Creating grid search')\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=5)\n",
    "\n",
    "print('Fitting')\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best: 0.8580858097611481 using {'batch_size': 64, 'epochs': 180, 'learn_rate': 0.2, 'momentum': 0.8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating grid search\n",
      "Fitting\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-183-591a98b96cda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Fitting'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mgrid_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Best: {grid_result.best_score_} using {grid_result.best_params_}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    709\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 711\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 101\n",
    "np.random.seed(seed)\n",
    "\n",
    "def create_model(learn_rate=0.01, momentum=0, init_mode='uniform'):\n",
    "  model = Sequential()\n",
    "  model.add(Dense(13, input_dim=X.shape[1], kernel_initializer=init_mode, activation='relu'))\n",
    "  model.add(Dense(8, kernel_initializer=init_mode, activation='relu'))\n",
    "  model.add(Dense(1, kernel_initializer=init_mode, activation='sigmoid'))\n",
    "  optimizer = SGD(lr=learn_rate, momentum=momentum)\n",
    "  model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "param_grid = {'batch_size': [32, 64, 128],\n",
    "              'epochs':     [160, 180, 200],\n",
    "              'learn_rate': [0.1, 0.2, 0.3],\n",
    "              'momentum':   [0.7, 0.8, 0.9],\n",
    "              'init_mode': ['uniform', 'lecun_uniform', 'normal', 'zero', \n",
    "                             'glorot_normal', 'glorot_uniform', 'he_normal', \n",
    "                             'he_uniform']}\n",
    "\n",
    "print('Creating grid search')\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=5)\n",
    "\n",
    "print('Fitting')\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Too late - couldn't finish this run - took too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "DS43SC.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
