{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of DS_Unit_4_Sprint_3_Neural_Nets_Spring_Challenge.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/macscheffer/DS-Unit-4-Sprint-3-Neural-Networks/blob/master/Copy_of_DS_Unit_4_Sprint_3_Neural_Nets_Spring_Challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6SKlgYrpcym",
        "colab_type": "text"
      },
      "source": [
        "# Neural Networks Sprint Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrEbRrjVphPM",
        "colab_type": "text"
      },
      "source": [
        "## 1) Define the following terms:\n",
        "\n",
        "- Neuron\n",
        "- Input Layer\n",
        "- Hidden Layer\n",
        "- Output Layer\n",
        "- Activation\n",
        "- Backpropagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5EksLqnp4oB",
        "colab_type": "text"
      },
      "source": [
        "A neuron receives information from previous nodes in the network, transforms that input into an output that it then sends on to other nodes with an activation function.\n",
        "\n",
        "An input layer is the first layer in a standard feed forward neural network, ie it is being fed information from the initial features of the data.\n",
        "\n",
        "Hidden layer is any layer between the input layer and output layer, kind of like the way the question is formatted. Hidden layers are what make neural networks so powerful when trying to find nonlinear relationships.\n",
        "\n",
        "the output layer transforms information from the final hidden layer into output that is the prediction.\n",
        "\n",
        "Activation is a way to filter what data gets passed from layer to layer in the neural network. \n",
        "\n",
        "Backpropagation is the messenger telling the network whether or not the network made a mistake when it made a prediction. It is the algorithm that updates weights and bias parameters of neural networks. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ri_gRA2Jp728",
        "colab_type": "text"
      },
      "source": [
        "## 2) Create a perceptron class that can model the behavior of an AND gate. You can use the following table as your training data:\n",
        "\n",
        "| x1 | x2 | x3 | y |\n",
        "|----|----|----|---|\n",
        "| 1  | 1  | 1  | 1 |\n",
        "| 1  | 0  | 1  | 0 |\n",
        "| 0  | 1  | 1  | 0 |\n",
        "| 0  | 0  | 1  | 0 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cJ3otqPZ2WE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ig6ZTH8tpQ19",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Perceptron():\n",
        "  \n",
        "  def __init__(self, learning_rate=0.1, iterations=100, tolerance=0.000001):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.iterations = iterations\n",
        "        self.tolerance = tolerance\n",
        "        \n",
        "  \n",
        "  def fit(self, X, y):\n",
        "        # initialize weights\n",
        "        self.weights_ = np.random.uniform(-0.01, 0.01, X.shape[1] + 1)\n",
        "        \n",
        "        self.costs_ = []\n",
        "        for i in range(self.iterations):\n",
        "            preds = self.predict_proba(X)\n",
        "            errors = preds - y\n",
        "            cost = np.sum(errors ** 2)\n",
        "            self.costs_.append(cost)\n",
        "            gradient = np.dot(X.T, errors)\n",
        "            self.weights_[1:] -= self.learning_rate * gradient\n",
        "            self.weights_[0] -= np.mean(errors)\n",
        " \n",
        "        return self\n",
        "      \n",
        "  def predict_proba(self, X):\n",
        "    return 1. / (1. + np.exp(-(np.dot(X, self.weights_[1:]) + self.weights_[0])))\n",
        "      \n",
        "  def predict(self, X):\n",
        "    return np.where(self.predict_proba(X)>=0.5, 1, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XPjvRZEZ9wL",
        "colab_type": "code",
        "outputId": "03b5bee2-dec0-4929-9a96-6a31270074f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "X = np.array([[1,0,1],\n",
        "            [1,1,1],\n",
        "            [0,0,1],\n",
        "            [0,1,1]])\n",
        "y = np.array([1,1,0,0])\n",
        "\n",
        "perc = Perceptron()\n",
        "perc.fit(X, y)\n",
        "perc.predict(X)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86HyRi8Osr3U",
        "colab_type": "text"
      },
      "source": [
        "## 3) Implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. \n",
        "- Your network must have one hidden layer. \n",
        "- You do not have to update weights via gradient descent. You can use something like the derivative of the sigmoid function to update weights.\n",
        "- Train your model on the Heart Disease dataset from UCI:\n",
        "\n",
        "[Github Dataset](https://github.com/ryanleeallred/datasets/blob/master/heart.csv)\n",
        "\n",
        "[Raw File on Github](https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNfiajv3v4Ed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IL7W-7samAr",
        "colab_type": "code",
        "outputId": "664b4050-638f-429e-9f34-14638e2826cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>63</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>145</td>\n",
              "      <td>233</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>0</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>37</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>130</td>\n",
              "      <td>250</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>187</td>\n",
              "      <td>0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>41</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>130</td>\n",
              "      <td>204</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>172</td>\n",
              "      <td>0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>120</td>\n",
              "      <td>236</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>178</td>\n",
              "      <td>0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>57</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>120</td>\n",
              "      <td>354</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>163</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
              "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
              "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
              "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
              "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
              "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
              "\n",
              "   ca  thal  target  \n",
              "0   0     1       1  \n",
              "1   0     2       1  \n",
              "2   0     2       1  \n",
              "3   0     2       1  \n",
              "4   0     2       1  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIcX-2ShHYx1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "b659e1a1-6d11-4fd5-b781-44416deebad6"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X = df.drop(columns=['target'])\n",
        "y = df['target']\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "X = X.reshape(-1,13)\n",
        "y = np.array(y).reshape(-1,1)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
            "  return self.partial_fit(X, y)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
            "  return self.fit(X, **fit_params).transform(X)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUiOOHksbYzS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuralNetwork(object):\n",
        "  def __init__(self):\n",
        "    self.inputs = 13\n",
        "    self.hiddenNodes = 7\n",
        "    self.outputNodes = 1\n",
        "\n",
        "    # Initialize Weights:\n",
        "    self.L1_weights = np.random.randn(self.inputs, self.hiddenNodes) \n",
        "    self.L2_weights = np.random.randn(self.hiddenNodes, self.outputNodes)\n",
        "    \n",
        "    \n",
        "  def feed_forward(self, X):\n",
        "    \n",
        "    self.hidden_sum = np.dot(X, self.L1_weights)\n",
        "    self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
        "    self.output_sum = np.dot(self.activated_hidden, self.L2_weights)\n",
        "    self.activated_output = self.sigmoid(self.output_sum)\n",
        "    \n",
        "    return self.activated_output\n",
        "    \n",
        "  def sigmoid(self, s):\n",
        "    return 1/(1+np.exp(-s))\n",
        "  \n",
        "  \n",
        "  def sigmoidPrime(self, s):\n",
        "    return s * (1 - s)\n",
        "  \n",
        "  \n",
        "  def backward(self, X, y, output):\n",
        "    self.output_error = y - output\n",
        "    \n",
        "    self.output_delta = self.output_error * self.sigmoidPrime(output) \n",
        "    \n",
        "    self.z2_error = self.output_delta.dot(self.L2_weights.T)\n",
        "    \n",
        "    self.z2_delta = self.z2_error*self.sigmoidPrime(self.activated_hidden) \n",
        "    \n",
        "    self.L1_weights += X.T.dot(self.z2_delta) \n",
        "\n",
        "    self.L2_weights += self.activated_hidden.T.dot(self.output_delta) \n",
        "    \n",
        "  def train (self, X, y):\n",
        "    output = self.feed_forward(X)\n",
        "    self.backward(X, y, output)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJ9heyKvGiOH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 13277
        },
        "outputId": "f618231f-69a6-4ea1-f188-2ecf1934c374"
      },
      "source": [
        "nn = NeuralNetwork()\n",
        "for i in range(1000): \n",
        "  if i+1 in [1,2,3,4,5] or (i+1) % 5 == 0:\n",
        "    print('EPOCH', i+1)   \n",
        "    print(\"Loss:\" + str(np.mean(np.square(y - nn.feed_forward(X))))) # mean sum squared loss\n",
        "    print(\"\\n\")\n",
        "  nn.train(X, y)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCH 1\n",
            "Loss:0.4534137786347697\n",
            "\n",
            "\n",
            "EPOCH 2\n",
            "Loss:0.4554282424419276\n",
            "\n",
            "\n",
            "EPOCH 3\n",
            "Loss:0.45542758324679405\n",
            "\n",
            "\n",
            "EPOCH 4\n",
            "Loss:0.4554268574112785\n",
            "\n",
            "\n",
            "EPOCH 5\n",
            "Loss:0.45542605435603867\n",
            "\n",
            "\n",
            "EPOCH 10\n",
            "Loss:0.4554203064755986\n",
            "\n",
            "\n",
            "EPOCH 15\n",
            "Loss:0.45540836041407157\n",
            "\n",
            "\n",
            "EPOCH 20\n",
            "Loss:0.45537350765551265\n",
            "\n",
            "\n",
            "EPOCH 25\n",
            "Loss:0.45517759983494843\n",
            "\n",
            "\n",
            "EPOCH 30\n",
            "Loss:0.44965445000995297\n",
            "\n",
            "\n",
            "EPOCH 35\n",
            "Loss:0.4369969998515835\n",
            "\n",
            "\n",
            "EPOCH 40\n",
            "Loss:0.270539826360468\n",
            "\n",
            "\n",
            "EPOCH 45\n",
            "Loss:0.1561323644455471\n",
            "\n",
            "\n",
            "EPOCH 50\n",
            "Loss:0.1485655545751396\n",
            "\n",
            "\n",
            "EPOCH 55\n",
            "Loss:0.1474686854339892\n",
            "\n",
            "\n",
            "EPOCH 60\n",
            "Loss:0.14535264150199675\n",
            "\n",
            "\n",
            "EPOCH 65\n",
            "Loss:0.13799153710082607\n",
            "\n",
            "\n",
            "EPOCH 70\n",
            "Loss:0.13091284202782674\n",
            "\n",
            "\n",
            "EPOCH 75\n",
            "Loss:0.12299070278336763\n",
            "\n",
            "\n",
            "EPOCH 80\n",
            "Loss:0.11486872235285266\n",
            "\n",
            "\n",
            "EPOCH 85\n",
            "Loss:0.10350591324458301\n",
            "\n",
            "\n",
            "EPOCH 90\n",
            "Loss:0.09801089107164311\n",
            "\n",
            "\n",
            "EPOCH 95\n",
            "Loss:0.09512950902469022\n",
            "\n",
            "\n",
            "EPOCH 100\n",
            "Loss:0.09404588877922151\n",
            "\n",
            "\n",
            "EPOCH 105\n",
            "Loss:0.0939190983477646\n",
            "\n",
            "\n",
            "EPOCH 110\n",
            "Loss:0.09382574933681245\n",
            "\n",
            "\n",
            "EPOCH 115\n",
            "Loss:0.09374687346116428\n",
            "\n",
            "\n",
            "EPOCH 120\n",
            "Loss:0.093670035989256\n",
            "\n",
            "\n",
            "EPOCH 125\n",
            "Loss:0.09357349224564082\n",
            "\n",
            "\n",
            "EPOCH 130\n",
            "Loss:0.093373090058636\n",
            "\n",
            "\n",
            "EPOCH 135\n",
            "Loss:0.09196019306402527\n",
            "\n",
            "\n",
            "EPOCH 140\n",
            "Loss:0.09534186328797069\n",
            "\n",
            "\n",
            "EPOCH 145\n",
            "Loss:0.08031054950228596\n",
            "\n",
            "\n",
            "EPOCH 150\n",
            "Loss:0.07435592692075153\n",
            "\n",
            "\n",
            "EPOCH 155\n",
            "Loss:0.07071379897310244\n",
            "\n",
            "\n",
            "EPOCH 160\n",
            "Loss:0.06989472969311442\n",
            "\n",
            "\n",
            "EPOCH 165\n",
            "Loss:0.06620654398552865\n",
            "\n",
            "\n",
            "EPOCH 170\n",
            "Loss:0.06630576715423644\n",
            "\n",
            "\n",
            "EPOCH 175\n",
            "Loss:0.06966593475807853\n",
            "\n",
            "\n",
            "EPOCH 180\n",
            "Loss:0.06601994991699887\n",
            "\n",
            "\n",
            "EPOCH 185\n",
            "Loss:0.06112932718565822\n",
            "\n",
            "\n",
            "EPOCH 190\n",
            "Loss:0.06043715296167357\n",
            "\n",
            "\n",
            "EPOCH 195\n",
            "Loss:0.05977747492411423\n",
            "\n",
            "\n",
            "EPOCH 200\n",
            "Loss:0.059494392985362826\n",
            "\n",
            "\n",
            "EPOCH 205\n",
            "Loss:0.059406067197992865\n",
            "\n",
            "\n",
            "EPOCH 210\n",
            "Loss:0.059344572616538076\n",
            "\n",
            "\n",
            "EPOCH 215\n",
            "Loss:0.05929576430571673\n",
            "\n",
            "\n",
            "EPOCH 220\n",
            "Loss:0.059255152168352684\n",
            "\n",
            "\n",
            "EPOCH 225\n",
            "Loss:0.0592203661786635\n",
            "\n",
            "\n",
            "EPOCH 230\n",
            "Loss:0.05918992249520176\n",
            "\n",
            "\n",
            "EPOCH 235\n",
            "Loss:0.05916279792178434\n",
            "\n",
            "\n",
            "EPOCH 240\n",
            "Loss:0.05913820134778588\n",
            "\n",
            "\n",
            "EPOCH 245\n",
            "Loss:0.059115374125756265\n",
            "\n",
            "\n",
            "EPOCH 250\n",
            "Loss:0.05909315699800416\n",
            "\n",
            "\n",
            "EPOCH 255\n",
            "Loss:0.05906729709824112\n",
            "\n",
            "\n",
            "EPOCH 260\n",
            "Loss:0.058868728764967675\n",
            "\n",
            "\n",
            "EPOCH 265\n",
            "Loss:0.0790642409488885\n",
            "\n",
            "\n",
            "EPOCH 270\n",
            "Loss:0.05892906020028978\n",
            "\n",
            "\n",
            "EPOCH 275\n",
            "Loss:0.07086430037567122\n",
            "\n",
            "\n",
            "EPOCH 280\n",
            "Loss:0.05962150229004322\n",
            "\n",
            "\n",
            "EPOCH 285\n",
            "Loss:0.05751649704098592\n",
            "\n",
            "\n",
            "EPOCH 290\n",
            "Loss:0.055963907753619827\n",
            "\n",
            "\n",
            "EPOCH 295\n",
            "Loss:0.0558630697444461\n",
            "\n",
            "\n",
            "EPOCH 300\n",
            "Loss:0.05580412476550546\n",
            "\n",
            "\n",
            "EPOCH 305\n",
            "Loss:0.055761801473132235\n",
            "\n",
            "\n",
            "EPOCH 310\n",
            "Loss:0.05572736188331065\n",
            "\n",
            "\n",
            "EPOCH 315\n",
            "Loss:0.05569568919280294\n",
            "\n",
            "\n",
            "EPOCH 320\n",
            "Loss:0.055664272752633186\n",
            "\n",
            "\n",
            "EPOCH 325\n",
            "Loss:0.055635172680360424\n",
            "\n",
            "\n",
            "EPOCH 330\n",
            "Loss:0.055611266733403344\n",
            "\n",
            "\n",
            "EPOCH 335\n",
            "Loss:0.05559183762405573\n",
            "\n",
            "\n",
            "EPOCH 340\n",
            "Loss:0.055575270414131066\n",
            "\n",
            "\n",
            "EPOCH 345\n",
            "Loss:0.055560613071588605\n",
            "\n",
            "\n",
            "EPOCH 350\n",
            "Loss:0.0555473779078643\n",
            "\n",
            "\n",
            "EPOCH 355\n",
            "Loss:0.055535282334758115\n",
            "\n",
            "\n",
            "EPOCH 360\n",
            "Loss:0.055524137079573335\n",
            "\n",
            "\n",
            "EPOCH 365\n",
            "Loss:0.055513802524315925\n",
            "\n",
            "\n",
            "EPOCH 370\n",
            "Loss:0.055504169633597215\n",
            "\n",
            "\n",
            "EPOCH 375\n",
            "Loss:0.0554951501175657\n",
            "\n",
            "\n",
            "EPOCH 380\n",
            "Loss:0.05548667056996038\n",
            "\n",
            "\n",
            "EPOCH 385\n",
            "Loss:0.05547866859910199\n",
            "\n",
            "\n",
            "EPOCH 390\n",
            "Loss:0.055471090087178924\n",
            "\n",
            "\n",
            "EPOCH 395\n",
            "Loss:0.055463887137351335\n",
            "\n",
            "\n",
            "EPOCH 400\n",
            "Loss:0.05545701645038906\n",
            "\n",
            "\n",
            "EPOCH 405\n",
            "Loss:0.055450437958701304\n",
            "\n",
            "\n",
            "EPOCH 410\n",
            "Loss:0.05544411358622672\n",
            "\n",
            "\n",
            "EPOCH 415\n",
            "Loss:0.05543800601556647\n",
            "\n",
            "\n",
            "EPOCH 420\n",
            "Loss:0.055432077333241306\n",
            "\n",
            "\n",
            "EPOCH 425\n",
            "Loss:0.05542628738568622\n",
            "\n",
            "\n",
            "EPOCH 430\n",
            "Loss:0.05542059159814896\n",
            "\n",
            "\n",
            "EPOCH 435\n",
            "Loss:0.05541493785393566\n",
            "\n",
            "\n",
            "EPOCH 440\n",
            "Loss:0.0554092617334913\n",
            "\n",
            "\n",
            "EPOCH 445\n",
            "Loss:0.055403478818654844\n",
            "\n",
            "\n",
            "EPOCH 450\n",
            "Loss:0.05539747151792005\n",
            "\n",
            "\n",
            "EPOCH 455\n",
            "Loss:0.055391065055599334\n",
            "\n",
            "\n",
            "EPOCH 460\n",
            "Loss:0.05538398037490151\n",
            "\n",
            "\n",
            "EPOCH 465\n",
            "Loss:0.055375732909216935\n",
            "\n",
            "\n",
            "EPOCH 470\n",
            "Loss:0.055365387286885545\n",
            "\n",
            "\n",
            "EPOCH 475\n",
            "Loss:0.05535085511181322\n",
            "\n",
            "\n",
            "EPOCH 480\n",
            "Loss:0.05532630990108826\n",
            "\n",
            "\n",
            "EPOCH 485\n",
            "Loss:0.05526751479910837\n",
            "\n",
            "\n",
            "EPOCH 490\n",
            "Loss:0.05492911791838123\n",
            "\n",
            "\n",
            "EPOCH 495\n",
            "Loss:0.05373143449132751\n",
            "\n",
            "\n",
            "EPOCH 500\n",
            "Loss:0.05345125344337106\n",
            "\n",
            "\n",
            "EPOCH 505\n",
            "Loss:0.05334684009155062\n",
            "\n",
            "\n",
            "EPOCH 510\n",
            "Loss:0.05329022800423119\n",
            "\n",
            "\n",
            "EPOCH 515\n",
            "Loss:0.053253896093825394\n",
            "\n",
            "\n",
            "EPOCH 520\n",
            "Loss:0.053228064017638355\n",
            "\n",
            "\n",
            "EPOCH 525\n",
            "Loss:0.05320833350026026\n",
            "\n",
            "\n",
            "EPOCH 530\n",
            "Loss:0.05319244165092937\n",
            "\n",
            "\n",
            "EPOCH 535\n",
            "Loss:0.05317911716695156\n",
            "\n",
            "\n",
            "EPOCH 540\n",
            "Loss:0.05316759982055341\n",
            "\n",
            "\n",
            "EPOCH 545\n",
            "Loss:0.053157412508969736\n",
            "\n",
            "\n",
            "EPOCH 550\n",
            "Loss:0.05314824322615384\n",
            "\n",
            "\n",
            "EPOCH 555\n",
            "Loss:0.05313988005618655\n",
            "\n",
            "\n",
            "EPOCH 560\n",
            "Loss:0.05313217380047112\n",
            "\n",
            "\n",
            "EPOCH 565\n",
            "Loss:0.053125015799656686\n",
            "\n",
            "\n",
            "EPOCH 570\n",
            "Loss:0.05311832441405961\n",
            "\n",
            "\n",
            "EPOCH 575\n",
            "Loss:0.05311203656706867\n",
            "\n",
            "\n",
            "EPOCH 580\n",
            "Loss:0.053106102315310245\n",
            "\n",
            "\n",
            "EPOCH 585\n",
            "Loss:0.053100481267990884\n",
            "\n",
            "\n",
            "EPOCH 590\n",
            "Loss:0.05309514016169173\n",
            "\n",
            "\n",
            "EPOCH 595\n",
            "Loss:0.05309005117422396\n",
            "\n",
            "\n",
            "EPOCH 600\n",
            "Loss:0.05308519072263576\n",
            "\n",
            "\n",
            "EPOCH 605\n",
            "Loss:0.053080538586044126\n",
            "\n",
            "\n",
            "EPOCH 610\n",
            "Loss:0.05307607725154274\n",
            "\n",
            "\n",
            "EPOCH 615\n",
            "Loss:0.05307179141677692\n",
            "\n",
            "\n",
            "EPOCH 620\n",
            "Loss:0.053067667604890296\n",
            "\n",
            "\n",
            "EPOCH 625\n",
            "Loss:0.05306369386166296\n",
            "\n",
            "\n",
            "EPOCH 630\n",
            "Loss:0.05305985951385144\n",
            "\n",
            "\n",
            "EPOCH 635\n",
            "Loss:0.053056154973843486\n",
            "\n",
            "\n",
            "EPOCH 640\n",
            "Loss:0.05305257157987213\n",
            "\n",
            "\n",
            "EPOCH 645\n",
            "Loss:0.053049101463883384\n",
            "\n",
            "\n",
            "EPOCH 650\n",
            "Loss:0.05304573744115201\n",
            "\n",
            "\n",
            "EPOCH 655\n",
            "Loss:0.05304247291717028\n",
            "\n",
            "\n",
            "EPOCH 660\n",
            "Loss:0.05303930180837169\n",
            "\n",
            "\n",
            "EPOCH 665\n",
            "Loss:0.05303621847401964\n",
            "\n",
            "\n",
            "EPOCH 670\n",
            "Loss:0.05303321765716593\n",
            "\n",
            "\n",
            "EPOCH 675\n",
            "Loss:0.0530302944330264\n",
            "\n",
            "\n",
            "EPOCH 680\n",
            "Loss:0.05302744416346594\n",
            "\n",
            "\n",
            "EPOCH 685\n",
            "Loss:0.05302466245656479\n",
            "\n",
            "\n",
            "EPOCH 690\n",
            "Loss:0.05302194513047365\n",
            "\n",
            "\n",
            "EPOCH 695\n",
            "Loss:0.05301928818097339\n",
            "\n",
            "\n",
            "EPOCH 700\n",
            "Loss:0.05301668775235199\n",
            "\n",
            "\n",
            "EPOCH 705\n",
            "Loss:0.05301414011141255\n",
            "\n",
            "\n",
            "EPOCH 710\n",
            "Loss:0.053011641624639456\n",
            "\n",
            "\n",
            "EPOCH 715\n",
            "Loss:0.05300918873879553\n",
            "\n",
            "\n",
            "EPOCH 720\n",
            "Loss:0.0530067779655021\n",
            "\n",
            "\n",
            "EPOCH 725\n",
            "Loss:0.05300440587068304\n",
            "\n",
            "\n",
            "EPOCH 730\n",
            "Loss:0.053002069070126376\n",
            "\n",
            "\n",
            "EPOCH 735\n",
            "Loss:0.05299976423282156\n",
            "\n",
            "\n",
            "EPOCH 740\n",
            "Loss:0.052997488094131975\n",
            "\n",
            "\n",
            "EPOCH 745\n",
            "Loss:0.052995237481186286\n",
            "\n",
            "\n",
            "EPOCH 750\n",
            "Loss:0.05299300935300033\n",
            "\n",
            "\n",
            "EPOCH 755\n",
            "Loss:0.05299080085759135\n",
            "\n",
            "\n",
            "EPOCH 760\n",
            "Loss:0.052988609407479056\n",
            "\n",
            "\n",
            "EPOCH 765\n",
            "Loss:0.052986432773213146\n",
            "\n",
            "\n",
            "EPOCH 770\n",
            "Loss:0.05298426919170587\n",
            "\n",
            "\n",
            "EPOCH 775\n",
            "Loss:0.05298211748214984\n",
            "\n",
            "\n",
            "EPOCH 780\n",
            "Loss:0.05297997715752415\n",
            "\n",
            "\n",
            "EPOCH 785\n",
            "Loss:0.052977848515073624\n",
            "\n",
            "\n",
            "EPOCH 790\n",
            "Loss:0.05297573268624078\n",
            "\n",
            "\n",
            "EPOCH 795\n",
            "Loss:0.05297363162723971\n",
            "\n",
            "\n",
            "EPOCH 800\n",
            "Loss:0.052971548037302\n",
            "\n",
            "\n",
            "EPOCH 805\n",
            "Loss:0.052969485202740516\n",
            "\n",
            "\n",
            "EPOCH 810\n",
            "Loss:0.05296744677934853\n",
            "\n",
            "\n",
            "EPOCH 815\n",
            "Loss:0.052965436539252744\n",
            "\n",
            "\n",
            "EPOCH 820\n",
            "Loss:0.0529634581165464\n",
            "\n",
            "\n",
            "EPOCH 825\n",
            "Loss:0.052961514785770984\n",
            "\n",
            "\n",
            "EPOCH 830\n",
            "Loss:0.05295960929870264\n",
            "\n",
            "\n",
            "EPOCH 835\n",
            "Loss:0.05295774379119864\n",
            "\n",
            "\n",
            "EPOCH 840\n",
            "Loss:0.052955919757799765\n",
            "\n",
            "\n",
            "EPOCH 845\n",
            "Loss:0.05295413808135049\n",
            "\n",
            "\n",
            "EPOCH 850\n",
            "Loss:0.052952399099891925\n",
            "\n",
            "\n",
            "EPOCH 855\n",
            "Loss:0.05295070269308055\n",
            "\n",
            "\n",
            "EPOCH 860\n",
            "Loss:0.05294904837364213\n",
            "\n",
            "\n",
            "EPOCH 865\n",
            "Loss:0.05294743537391435\n",
            "\n",
            "\n",
            "EPOCH 870\n",
            "Loss:0.052945862721882626\n",
            "\n",
            "\n",
            "EPOCH 875\n",
            "Loss:0.05294432930450435\n",
            "\n",
            "\n",
            "EPOCH 880\n",
            "Loss:0.052942833918352236\n",
            "\n",
            "\n",
            "EPOCH 885\n",
            "Loss:0.052941375308846565\n",
            "\n",
            "\n",
            "EPOCH 890\n",
            "Loss:0.05293995219986196\n",
            "\n",
            "\n",
            "EPOCH 895\n",
            "Loss:0.05293856331556353\n",
            "\n",
            "\n",
            "EPOCH 900\n",
            "Loss:0.052937207396161014\n",
            "\n",
            "\n",
            "EPOCH 905\n",
            "Loss:0.05293588320900548\n",
            "\n",
            "\n",
            "EPOCH 910\n",
            "Loss:0.05293458955617714\n",
            "\n",
            "\n",
            "EPOCH 915\n",
            "Loss:0.05293332527945961\n",
            "\n",
            "\n",
            "EPOCH 920\n",
            "Loss:0.052932089263385634\n",
            "\n",
            "\n",
            "EPOCH 925\n",
            "Loss:0.052930880436869394\n",
            "\n",
            "\n",
            "EPOCH 930\n",
            "Loss:0.05292969777381127\n",
            "\n",
            "\n",
            "EPOCH 935\n",
            "Loss:0.052928540292960284\n",
            "\n",
            "\n",
            "EPOCH 940\n",
            "Loss:0.05292740705724652\n",
            "\n",
            "\n",
            "EPOCH 945\n",
            "Loss:0.05292629717273991\n",
            "\n",
            "\n",
            "EPOCH 950\n",
            "Loss:0.05292520978735136\n",
            "\n",
            "\n",
            "EPOCH 955\n",
            "Loss:0.05292414408936188\n",
            "\n",
            "\n",
            "EPOCH 960\n",
            "Loss:0.052923099305843636\n",
            "\n",
            "\n",
            "EPOCH 965\n",
            "Loss:0.05292207470101994\n",
            "\n",
            "\n",
            "EPOCH 970\n",
            "Loss:0.0529210695745993\n",
            "\n",
            "\n",
            "EPOCH 975\n",
            "Loss:0.05292008326010963\n",
            "\n",
            "\n",
            "EPOCH 980\n",
            "Loss:0.05291911512325184\n",
            "\n",
            "\n",
            "EPOCH 985\n",
            "Loss:0.05291816456028683\n",
            "\n",
            "\n",
            "EPOCH 990\n",
            "Loss:0.052917230996466405\n",
            "\n",
            "\n",
            "EPOCH 995\n",
            "Loss:0.05291631388451525\n",
            "\n",
            "\n",
            "EPOCH 1000\n",
            "Loss:0.05291541270316894\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGT1oRzXw3H9",
        "colab_type": "text"
      },
      "source": [
        "## 4) Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy. \n",
        "\n",
        "- Use the Heart Disease Dataset (binary classification)\n",
        "- Use an appropriate loss function for a binary classification task\n",
        "- Use an appropriate activation function on the final layer of your network. \n",
        "- Train your model using verbose output for ease of grading.\n",
        "- Use GridSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
        "- When hyperparameter tuning, show you work by adding code cells for each new experiment. \n",
        "- Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
        "- You must hyperparameter tune at least 5 parameters in order to get a 3 on this section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWw4IYxLxKwH",
        "colab_type": "code",
        "outputId": "fba402f7-830f-4832-8e13-fe1bbfd78773",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17455
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "def baseline(optimizer='adam', init='uniform', activation='relu'):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(13, input_dim=13, activation=activation))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "model = KerasClassifier(build_fn=baseline, epochs=100, batch_size=5, verbose=1)\n",
        "kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "results = cross_val_score(model, X_train, y_train, cv=kfold)\n",
        "print(\"Results: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "193/193 [==============================] - 1s 3ms/step - loss: 0.5755 - acc: 0.7098\n",
            "Epoch 2/100\n",
            "193/193 [==============================] - 0s 265us/step - loss: 0.5185 - acc: 0.7617\n",
            "Epoch 3/100\n",
            "193/193 [==============================] - 0s 257us/step - loss: 0.4779 - acc: 0.7824\n",
            "Epoch 4/100\n",
            "193/193 [==============================] - 0s 271us/step - loss: 0.4473 - acc: 0.8083\n",
            "Epoch 5/100\n",
            "193/193 [==============================] - 0s 248us/step - loss: 0.4213 - acc: 0.8290\n",
            "Epoch 6/100\n",
            "193/193 [==============================] - 0s 246us/step - loss: 0.4013 - acc: 0.8653\n",
            "Epoch 7/100\n",
            "193/193 [==============================] - 0s 265us/step - loss: 0.3847 - acc: 0.8705\n",
            "Epoch 8/100\n",
            "193/193 [==============================] - 0s 277us/step - loss: 0.3705 - acc: 0.8756\n",
            "Epoch 9/100\n",
            "193/193 [==============================] - 0s 254us/step - loss: 0.3590 - acc: 0.8705\n",
            "Epoch 10/100\n",
            "193/193 [==============================] - 0s 253us/step - loss: 0.3488 - acc: 0.8705\n",
            "Epoch 11/100\n",
            "193/193 [==============================] - 0s 258us/step - loss: 0.3404 - acc: 0.8808\n",
            "Epoch 12/100\n",
            "193/193 [==============================] - 0s 253us/step - loss: 0.3330 - acc: 0.8808\n",
            "Epoch 13/100\n",
            "193/193 [==============================] - 0s 254us/step - loss: 0.3256 - acc: 0.8808\n",
            "Epoch 14/100\n",
            "193/193 [==============================] - 0s 244us/step - loss: 0.3199 - acc: 0.8808\n",
            "Epoch 15/100\n",
            "193/193 [==============================] - 0s 248us/step - loss: 0.3136 - acc: 0.8705\n",
            "Epoch 16/100\n",
            "193/193 [==============================] - 0s 268us/step - loss: 0.3082 - acc: 0.8705\n",
            "Epoch 17/100\n",
            "193/193 [==============================] - 0s 259us/step - loss: 0.3026 - acc: 0.8756\n",
            "Epoch 18/100\n",
            "193/193 [==============================] - 0s 272us/step - loss: 0.2974 - acc: 0.8756\n",
            "Epoch 19/100\n",
            "193/193 [==============================] - 0s 251us/step - loss: 0.2936 - acc: 0.8860\n",
            "Epoch 20/100\n",
            "193/193 [==============================] - 0s 251us/step - loss: 0.2897 - acc: 0.8756\n",
            "Epoch 21/100\n",
            "193/193 [==============================] - 0s 251us/step - loss: 0.2866 - acc: 0.8860\n",
            "Epoch 22/100\n",
            "193/193 [==============================] - 0s 240us/step - loss: 0.2826 - acc: 0.8860\n",
            "Epoch 23/100\n",
            "193/193 [==============================] - 0s 237us/step - loss: 0.2792 - acc: 0.9016\n",
            "Epoch 24/100\n",
            "193/193 [==============================] - 0s 268us/step - loss: 0.2753 - acc: 0.8964\n",
            "Epoch 25/100\n",
            "193/193 [==============================] - 0s 249us/step - loss: 0.2720 - acc: 0.8964\n",
            "Epoch 26/100\n",
            "193/193 [==============================] - 0s 247us/step - loss: 0.2693 - acc: 0.8912\n",
            "Epoch 27/100\n",
            "193/193 [==============================] - 0s 250us/step - loss: 0.2657 - acc: 0.8964\n",
            "Epoch 28/100\n",
            "193/193 [==============================] - 0s 259us/step - loss: 0.2629 - acc: 0.9067\n",
            "Epoch 29/100\n",
            "193/193 [==============================] - 0s 248us/step - loss: 0.2598 - acc: 0.9119\n",
            "Epoch 30/100\n",
            "193/193 [==============================] - 0s 258us/step - loss: 0.2574 - acc: 0.9067\n",
            "Epoch 31/100\n",
            "193/193 [==============================] - 0s 254us/step - loss: 0.2557 - acc: 0.9119\n",
            "Epoch 32/100\n",
            "193/193 [==============================] - 0s 253us/step - loss: 0.2537 - acc: 0.9119\n",
            "Epoch 33/100\n",
            "193/193 [==============================] - 0s 259us/step - loss: 0.2514 - acc: 0.9067\n",
            "Epoch 34/100\n",
            "193/193 [==============================] - 0s 250us/step - loss: 0.2492 - acc: 0.9119\n",
            "Epoch 35/100\n",
            "193/193 [==============================] - 0s 262us/step - loss: 0.2465 - acc: 0.9119\n",
            "Epoch 36/100\n",
            "193/193 [==============================] - 0s 265us/step - loss: 0.2439 - acc: 0.9119\n",
            "Epoch 37/100\n",
            "193/193 [==============================] - 0s 263us/step - loss: 0.2423 - acc: 0.9067\n",
            "Epoch 38/100\n",
            "193/193 [==============================] - 0s 264us/step - loss: 0.2403 - acc: 0.9067\n",
            "Epoch 39/100\n",
            "193/193 [==============================] - 0s 253us/step - loss: 0.2373 - acc: 0.9067\n",
            "Epoch 40/100\n",
            "193/193 [==============================] - 0s 265us/step - loss: 0.2367 - acc: 0.9067\n",
            "Epoch 41/100\n",
            "193/193 [==============================] - 0s 250us/step - loss: 0.2338 - acc: 0.9067\n",
            "Epoch 42/100\n",
            "193/193 [==============================] - 0s 266us/step - loss: 0.2321 - acc: 0.9067\n",
            "Epoch 43/100\n",
            "193/193 [==============================] - 0s 247us/step - loss: 0.2298 - acc: 0.9067\n",
            "Epoch 44/100\n",
            "193/193 [==============================] - 0s 255us/step - loss: 0.2278 - acc: 0.9067\n",
            "Epoch 45/100\n",
            "193/193 [==============================] - 0s 235us/step - loss: 0.2266 - acc: 0.9067\n",
            "Epoch 46/100\n",
            "193/193 [==============================] - 0s 252us/step - loss: 0.2245 - acc: 0.9067\n",
            "Epoch 47/100\n",
            "193/193 [==============================] - 0s 251us/step - loss: 0.2226 - acc: 0.9119\n",
            "Epoch 48/100\n",
            "193/193 [==============================] - 0s 252us/step - loss: 0.2212 - acc: 0.9119\n",
            "Epoch 49/100\n",
            "193/193 [==============================] - 0s 273us/step - loss: 0.2194 - acc: 0.9171\n",
            "Epoch 50/100\n",
            "193/193 [==============================] - 0s 251us/step - loss: 0.2184 - acc: 0.9171\n",
            "Epoch 51/100\n",
            "193/193 [==============================] - 0s 258us/step - loss: 0.2162 - acc: 0.9171\n",
            "Epoch 52/100\n",
            "193/193 [==============================] - 0s 252us/step - loss: 0.2149 - acc: 0.9171\n",
            "Epoch 53/100\n",
            "193/193 [==============================] - 0s 292us/step - loss: 0.2141 - acc: 0.9171\n",
            "Epoch 54/100\n",
            "193/193 [==============================] - 0s 247us/step - loss: 0.2128 - acc: 0.9171\n",
            "Epoch 55/100\n",
            "193/193 [==============================] - 0s 248us/step - loss: 0.2106 - acc: 0.9171\n",
            "Epoch 56/100\n",
            "193/193 [==============================] - 0s 252us/step - loss: 0.2088 - acc: 0.9171\n",
            "Epoch 57/100\n",
            "193/193 [==============================] - 0s 259us/step - loss: 0.2077 - acc: 0.9171\n",
            "Epoch 58/100\n",
            "193/193 [==============================] - 0s 249us/step - loss: 0.2079 - acc: 0.9171\n",
            "Epoch 59/100\n",
            "193/193 [==============================] - 0s 248us/step - loss: 0.2055 - acc: 0.9171\n",
            "Epoch 60/100\n",
            "193/193 [==============================] - 0s 265us/step - loss: 0.2031 - acc: 0.9171\n",
            "Epoch 61/100\n",
            "193/193 [==============================] - 0s 253us/step - loss: 0.2024 - acc: 0.9171\n",
            "Epoch 62/100\n",
            "193/193 [==============================] - 0s 264us/step - loss: 0.2007 - acc: 0.9171\n",
            "Epoch 63/100\n",
            "193/193 [==============================] - 0s 251us/step - loss: 0.1991 - acc: 0.9119\n",
            "Epoch 64/100\n",
            "193/193 [==============================] - 0s 263us/step - loss: 0.1984 - acc: 0.9223\n",
            "Epoch 65/100\n",
            "193/193 [==============================] - 0s 256us/step - loss: 0.1964 - acc: 0.9275\n",
            "Epoch 66/100\n",
            "193/193 [==============================] - 0s 258us/step - loss: 0.1955 - acc: 0.9171\n",
            "Epoch 67/100\n",
            "193/193 [==============================] - 0s 254us/step - loss: 0.1942 - acc: 0.9223\n",
            "Epoch 68/100\n",
            "193/193 [==============================] - 0s 270us/step - loss: 0.1928 - acc: 0.9223\n",
            "Epoch 69/100\n",
            "193/193 [==============================] - 0s 260us/step - loss: 0.1919 - acc: 0.9171\n",
            "Epoch 70/100\n",
            "193/193 [==============================] - 0s 258us/step - loss: 0.1904 - acc: 0.9171\n",
            "Epoch 71/100\n",
            "193/193 [==============================] - 0s 237us/step - loss: 0.1893 - acc: 0.9275\n",
            "Epoch 72/100\n",
            "193/193 [==============================] - 0s 264us/step - loss: 0.1878 - acc: 0.9275\n",
            "Epoch 73/100\n",
            "193/193 [==============================] - 0s 269us/step - loss: 0.1863 - acc: 0.9171\n",
            "Epoch 74/100\n",
            "193/193 [==============================] - 0s 253us/step - loss: 0.1852 - acc: 0.9326\n",
            "Epoch 75/100\n",
            "193/193 [==============================] - 0s 261us/step - loss: 0.1842 - acc: 0.9326\n",
            "Epoch 76/100\n",
            "193/193 [==============================] - 0s 258us/step - loss: 0.1830 - acc: 0.9326\n",
            "Epoch 77/100\n",
            "193/193 [==============================] - 0s 248us/step - loss: 0.1815 - acc: 0.9275\n",
            "Epoch 78/100\n",
            "193/193 [==============================] - 0s 269us/step - loss: 0.1803 - acc: 0.9326\n",
            "Epoch 79/100\n",
            "193/193 [==============================] - 0s 261us/step - loss: 0.1792 - acc: 0.9326\n",
            "Epoch 80/100\n",
            "193/193 [==============================] - 0s 258us/step - loss: 0.1777 - acc: 0.9326\n",
            "Epoch 81/100\n",
            "193/193 [==============================] - 0s 260us/step - loss: 0.1771 - acc: 0.9326\n",
            "Epoch 82/100\n",
            "193/193 [==============================] - 0s 264us/step - loss: 0.1755 - acc: 0.9326\n",
            "Epoch 83/100\n",
            "193/193 [==============================] - 0s 255us/step - loss: 0.1743 - acc: 0.9326\n",
            "Epoch 84/100\n",
            "193/193 [==============================] - 0s 246us/step - loss: 0.1732 - acc: 0.9326\n",
            "Epoch 85/100\n",
            "193/193 [==============================] - 0s 253us/step - loss: 0.1719 - acc: 0.9326\n",
            "Epoch 86/100\n",
            "193/193 [==============================] - 0s 250us/step - loss: 0.1716 - acc: 0.9378\n",
            "Epoch 87/100\n",
            "193/193 [==============================] - 0s 269us/step - loss: 0.1701 - acc: 0.9326\n",
            "Epoch 88/100\n",
            "193/193 [==============================] - 0s 234us/step - loss: 0.1695 - acc: 0.9378\n",
            "Epoch 89/100\n",
            "193/193 [==============================] - 0s 256us/step - loss: 0.1673 - acc: 0.9378\n",
            "Epoch 90/100\n",
            "193/193 [==============================] - 0s 243us/step - loss: 0.1662 - acc: 0.9326\n",
            "Epoch 91/100\n",
            "193/193 [==============================] - 0s 249us/step - loss: 0.1647 - acc: 0.9326\n",
            "Epoch 92/100\n",
            "193/193 [==============================] - 0s 270us/step - loss: 0.1644 - acc: 0.9326\n",
            "Epoch 93/100\n",
            "193/193 [==============================] - 0s 265us/step - loss: 0.1621 - acc: 0.9378\n",
            "Epoch 94/100\n",
            "193/193 [==============================] - 0s 246us/step - loss: 0.1615 - acc: 0.9378\n",
            "Epoch 95/100\n",
            "193/193 [==============================] - 0s 268us/step - loss: 0.1605 - acc: 0.9378\n",
            "Epoch 96/100\n",
            "193/193 [==============================] - 0s 263us/step - loss: 0.1589 - acc: 0.9378\n",
            "Epoch 97/100\n",
            "193/193 [==============================] - 0s 243us/step - loss: 0.1583 - acc: 0.9326\n",
            "Epoch 98/100\n",
            "193/193 [==============================] - 0s 260us/step - loss: 0.1573 - acc: 0.9378\n",
            "Epoch 99/100\n",
            "193/193 [==============================] - 0s 239us/step - loss: 0.1564 - acc: 0.9430\n",
            "Epoch 100/100\n",
            "193/193 [==============================] - 0s 240us/step - loss: 0.1553 - acc: 0.9378\n",
            "49/49 [==============================] - 0s 4ms/step\n",
            "Epoch 1/100\n",
            "193/193 [==============================] - 1s 3ms/step - loss: 0.6515 - acc: 0.5959\n",
            "Epoch 2/100\n",
            "193/193 [==============================] - 0s 238us/step - loss: 0.5747 - acc: 0.6943\n",
            "Epoch 3/100\n",
            "193/193 [==============================] - 0s 268us/step - loss: 0.5179 - acc: 0.7617\n",
            "Epoch 4/100\n",
            "193/193 [==============================] - 0s 258us/step - loss: 0.4740 - acc: 0.8135\n",
            "Epoch 5/100\n",
            "193/193 [==============================] - 0s 261us/step - loss: 0.4391 - acc: 0.8497\n",
            "Epoch 6/100\n",
            "193/193 [==============================] - 0s 252us/step - loss: 0.4118 - acc: 0.8549\n",
            "Epoch 7/100\n",
            "193/193 [==============================] - 0s 259us/step - loss: 0.3893 - acc: 0.8549\n",
            "Epoch 8/100\n",
            "193/193 [==============================] - 0s 249us/step - loss: 0.3704 - acc: 0.8497\n",
            "Epoch 9/100\n",
            "193/193 [==============================] - 0s 269us/step - loss: 0.3559 - acc: 0.8653\n",
            "Epoch 10/100\n",
            "193/193 [==============================] - 0s 283us/step - loss: 0.3436 - acc: 0.8705\n",
            "Epoch 11/100\n",
            "193/193 [==============================] - 0s 257us/step - loss: 0.3331 - acc: 0.8653\n",
            "Epoch 12/100\n",
            "193/193 [==============================] - 0s 256us/step - loss: 0.3234 - acc: 0.8653\n",
            "Epoch 13/100\n",
            "193/193 [==============================] - 0s 264us/step - loss: 0.3159 - acc: 0.8705\n",
            "Epoch 14/100\n",
            "193/193 [==============================] - 0s 267us/step - loss: 0.3085 - acc: 0.8705\n",
            "Epoch 15/100\n",
            "193/193 [==============================] - 0s 262us/step - loss: 0.3022 - acc: 0.8756\n",
            "Epoch 16/100\n",
            "193/193 [==============================] - 0s 256us/step - loss: 0.2966 - acc: 0.8756\n",
            "Epoch 17/100\n",
            "193/193 [==============================] - 0s 253us/step - loss: 0.2914 - acc: 0.8756\n",
            "Epoch 18/100\n",
            "193/193 [==============================] - 0s 256us/step - loss: 0.2868 - acc: 0.8756\n",
            "Epoch 19/100\n",
            "193/193 [==============================] - 0s 260us/step - loss: 0.2823 - acc: 0.8756\n",
            "Epoch 20/100\n",
            "193/193 [==============================] - 0s 253us/step - loss: 0.2788 - acc: 0.8756\n",
            "Epoch 21/100\n",
            "193/193 [==============================] - 0s 242us/step - loss: 0.2744 - acc: 0.8808\n",
            "Epoch 22/100\n",
            "193/193 [==============================] - 0s 276us/step - loss: 0.2698 - acc: 0.8860\n",
            "Epoch 23/100\n",
            "193/193 [==============================] - 0s 260us/step - loss: 0.2667 - acc: 0.8860\n",
            "Epoch 24/100\n",
            "193/193 [==============================] - 0s 357us/step - loss: 0.2637 - acc: 0.8860\n",
            "Epoch 25/100\n",
            "193/193 [==============================] - 0s 256us/step - loss: 0.2599 - acc: 0.8912\n",
            "Epoch 26/100\n",
            "193/193 [==============================] - 0s 239us/step - loss: 0.2570 - acc: 0.8860\n",
            "Epoch 27/100\n",
            "193/193 [==============================] - 0s 240us/step - loss: 0.2545 - acc: 0.9016\n",
            "Epoch 28/100\n",
            "193/193 [==============================] - 0s 266us/step - loss: 0.2513 - acc: 0.8964\n",
            "Epoch 29/100\n",
            "193/193 [==============================] - 0s 272us/step - loss: 0.2488 - acc: 0.9067\n",
            "Epoch 30/100\n",
            "193/193 [==============================] - 0s 268us/step - loss: 0.2457 - acc: 0.9016\n",
            "Epoch 31/100\n",
            "193/193 [==============================] - 0s 247us/step - loss: 0.2434 - acc: 0.9067\n",
            "Epoch 32/100\n",
            "193/193 [==============================] - 0s 264us/step - loss: 0.2402 - acc: 0.9067\n",
            "Epoch 33/100\n",
            "193/193 [==============================] - 0s 254us/step - loss: 0.2381 - acc: 0.9067\n",
            "Epoch 34/100\n",
            "193/193 [==============================] - 0s 255us/step - loss: 0.2355 - acc: 0.9067\n",
            "Epoch 35/100\n",
            "193/193 [==============================] - 0s 255us/step - loss: 0.2329 - acc: 0.9067\n",
            "Epoch 36/100\n",
            "193/193 [==============================] - 0s 259us/step - loss: 0.2309 - acc: 0.9067\n",
            "Epoch 37/100\n",
            "193/193 [==============================] - 0s 245us/step - loss: 0.2288 - acc: 0.9067\n",
            "Epoch 38/100\n",
            "193/193 [==============================] - 0s 260us/step - loss: 0.2258 - acc: 0.9067\n",
            "Epoch 39/100\n",
            "193/193 [==============================] - 0s 254us/step - loss: 0.2239 - acc: 0.9067\n",
            "Epoch 40/100\n",
            "193/193 [==============================] - 0s 273us/step - loss: 0.2217 - acc: 0.9171\n",
            "Epoch 41/100\n",
            "193/193 [==============================] - 0s 263us/step - loss: 0.2191 - acc: 0.9119\n",
            "Epoch 42/100\n",
            "193/193 [==============================] - 0s 255us/step - loss: 0.2174 - acc: 0.9119\n",
            "Epoch 43/100\n",
            "193/193 [==============================] - 0s 259us/step - loss: 0.2151 - acc: 0.9119\n",
            "Epoch 44/100\n",
            "193/193 [==============================] - 0s 255us/step - loss: 0.2136 - acc: 0.9119\n",
            "Epoch 45/100\n",
            "193/193 [==============================] - 0s 252us/step - loss: 0.2110 - acc: 0.9171\n",
            "Epoch 46/100\n",
            "193/193 [==============================] - 0s 251us/step - loss: 0.2089 - acc: 0.9171\n",
            "Epoch 47/100\n",
            "193/193 [==============================] - 0s 272us/step - loss: 0.2069 - acc: 0.9223\n",
            "Epoch 48/100\n",
            "193/193 [==============================] - 0s 252us/step - loss: 0.2048 - acc: 0.9223\n",
            "Epoch 49/100\n",
            "193/193 [==============================] - 0s 260us/step - loss: 0.2029 - acc: 0.9223\n",
            "Epoch 50/100\n",
            "193/193 [==============================] - 0s 261us/step - loss: 0.2012 - acc: 0.9171\n",
            "Epoch 51/100\n",
            "193/193 [==============================] - 0s 255us/step - loss: 0.1992 - acc: 0.9171\n",
            "Epoch 52/100\n",
            "193/193 [==============================] - 0s 260us/step - loss: 0.1971 - acc: 0.9223\n",
            "Epoch 53/100\n",
            "193/193 [==============================] - 0s 259us/step - loss: 0.1960 - acc: 0.9171\n",
            "Epoch 54/100\n",
            "193/193 [==============================] - 0s 263us/step - loss: 0.1943 - acc: 0.9171\n",
            "Epoch 55/100\n",
            "193/193 [==============================] - 0s 247us/step - loss: 0.1913 - acc: 0.9275\n",
            "Epoch 56/100\n",
            "193/193 [==============================] - 0s 273us/step - loss: 0.1903 - acc: 0.9223\n",
            "Epoch 57/100\n",
            "193/193 [==============================] - 0s 256us/step - loss: 0.1880 - acc: 0.9223\n",
            "Epoch 58/100\n",
            "193/193 [==============================] - 0s 273us/step - loss: 0.1868 - acc: 0.9275\n",
            "Epoch 59/100\n",
            "193/193 [==============================] - 0s 258us/step - loss: 0.1846 - acc: 0.9223\n",
            "Epoch 60/100\n",
            "193/193 [==============================] - 0s 250us/step - loss: 0.1833 - acc: 0.9223\n",
            "Epoch 61/100\n",
            "193/193 [==============================] - 0s 250us/step - loss: 0.1819 - acc: 0.9378\n",
            "Epoch 62/100\n",
            "193/193 [==============================] - 0s 244us/step - loss: 0.1803 - acc: 0.9223\n",
            "Epoch 63/100\n",
            "193/193 [==============================] - 0s 266us/step - loss: 0.1792 - acc: 0.9326\n",
            "Epoch 64/100\n",
            "193/193 [==============================] - 0s 246us/step - loss: 0.1771 - acc: 0.9326\n",
            "Epoch 65/100\n",
            "193/193 [==============================] - 0s 248us/step - loss: 0.1750 - acc: 0.9275\n",
            "Epoch 66/100\n",
            "193/193 [==============================] - 0s 271us/step - loss: 0.1738 - acc: 0.9378\n",
            "Epoch 67/100\n",
            "193/193 [==============================] - 0s 262us/step - loss: 0.1718 - acc: 0.9378\n",
            "Epoch 68/100\n",
            "193/193 [==============================] - 0s 261us/step - loss: 0.1717 - acc: 0.9378\n",
            "Epoch 69/100\n",
            "193/193 [==============================] - 0s 255us/step - loss: 0.1695 - acc: 0.9326\n",
            "Epoch 70/100\n",
            "193/193 [==============================] - 0s 244us/step - loss: 0.1679 - acc: 0.9275\n",
            "Epoch 71/100\n",
            "193/193 [==============================] - 0s 271us/step - loss: 0.1671 - acc: 0.9326\n",
            "Epoch 72/100\n",
            "193/193 [==============================] - 0s 253us/step - loss: 0.1652 - acc: 0.9430\n",
            "Epoch 73/100\n",
            "193/193 [==============================] - 0s 269us/step - loss: 0.1656 - acc: 0.9378\n",
            "Epoch 74/100\n",
            "193/193 [==============================] - 0s 254us/step - loss: 0.1628 - acc: 0.9378\n",
            "Epoch 75/100\n",
            "193/193 [==============================] - 0s 266us/step - loss: 0.1615 - acc: 0.9378\n",
            "Epoch 76/100\n",
            "193/193 [==============================] - 0s 253us/step - loss: 0.1609 - acc: 0.9430\n",
            "Epoch 77/100\n",
            "193/193 [==============================] - 0s 247us/step - loss: 0.1598 - acc: 0.9430\n",
            "Epoch 78/100\n",
            "193/193 [==============================] - 0s 263us/step - loss: 0.1577 - acc: 0.9430\n",
            "Epoch 79/100\n",
            "193/193 [==============================] - 0s 260us/step - loss: 0.1575 - acc: 0.9378\n",
            "Epoch 80/100\n",
            "193/193 [==============================] - 0s 256us/step - loss: 0.1552 - acc: 0.9430\n",
            "Epoch 81/100\n",
            "193/193 [==============================] - 0s 260us/step - loss: 0.1538 - acc: 0.9430\n",
            "Epoch 82/100\n",
            "193/193 [==============================] - 0s 257us/step - loss: 0.1528 - acc: 0.9430\n",
            "Epoch 83/100\n",
            "193/193 [==============================] - 0s 246us/step - loss: 0.1511 - acc: 0.9430\n",
            "Epoch 84/100\n",
            "193/193 [==============================] - 0s 261us/step - loss: 0.1508 - acc: 0.9430\n",
            "Epoch 85/100\n",
            "193/193 [==============================] - 0s 248us/step - loss: 0.1494 - acc: 0.9430\n",
            "Epoch 86/100\n",
            "193/193 [==============================] - 0s 267us/step - loss: 0.1479 - acc: 0.9430\n",
            "Epoch 87/100\n",
            "193/193 [==============================] - 0s 288us/step - loss: 0.1471 - acc: 0.9430\n",
            "Epoch 88/100\n",
            "193/193 [==============================] - 0s 266us/step - loss: 0.1457 - acc: 0.9430\n",
            "Epoch 89/100\n",
            "193/193 [==============================] - 0s 260us/step - loss: 0.1450 - acc: 0.9430\n",
            "Epoch 90/100\n",
            "193/193 [==============================] - 0s 261us/step - loss: 0.1441 - acc: 0.9430\n",
            "Epoch 91/100\n",
            "193/193 [==============================] - 0s 247us/step - loss: 0.1418 - acc: 0.9430\n",
            "Epoch 92/100\n",
            "193/193 [==============================] - 0s 247us/step - loss: 0.1413 - acc: 0.9482\n",
            "Epoch 93/100\n",
            "193/193 [==============================] - 0s 243us/step - loss: 0.1400 - acc: 0.9482\n",
            "Epoch 94/100\n",
            "193/193 [==============================] - 0s 278us/step - loss: 0.1385 - acc: 0.9482\n",
            "Epoch 95/100\n",
            "193/193 [==============================] - 0s 251us/step - loss: 0.1372 - acc: 0.9482\n",
            "Epoch 96/100\n",
            "193/193 [==============================] - 0s 259us/step - loss: 0.1371 - acc: 0.9482\n",
            "Epoch 97/100\n",
            "193/193 [==============================] - 0s 258us/step - loss: 0.1359 - acc: 0.9482\n",
            "Epoch 98/100\n",
            "193/193 [==============================] - 0s 244us/step - loss: 0.1341 - acc: 0.9482\n",
            "Epoch 99/100\n",
            "193/193 [==============================] - 0s 265us/step - loss: 0.1325 - acc: 0.9482\n",
            "Epoch 100/100\n",
            "193/193 [==============================] - 0s 261us/step - loss: 0.1319 - acc: 0.9482\n",
            "49/49 [==============================] - 0s 4ms/step\n",
            "Epoch 1/100\n",
            "193/193 [==============================] - 1s 3ms/step - loss: 0.7195 - acc: 0.5337\n",
            "Epoch 2/100\n",
            "193/193 [==============================] - 0s 250us/step - loss: 0.6400 - acc: 0.6373\n",
            "Epoch 3/100\n",
            "193/193 [==============================] - 0s 262us/step - loss: 0.5796 - acc: 0.6995\n",
            "Epoch 4/100\n",
            "193/193 [==============================] - 0s 290us/step - loss: 0.5316 - acc: 0.7513\n",
            "Epoch 5/100\n",
            "193/193 [==============================] - 0s 266us/step - loss: 0.4922 - acc: 0.8031\n",
            "Epoch 6/100\n",
            "193/193 [==============================] - 0s 272us/step - loss: 0.4601 - acc: 0.8083\n",
            "Epoch 7/100\n",
            "193/193 [==============================] - 0s 258us/step - loss: 0.4349 - acc: 0.8187\n",
            "Epoch 8/100\n",
            "193/193 [==============================] - 0s 273us/step - loss: 0.4144 - acc: 0.8238\n",
            "Epoch 9/100\n",
            "193/193 [==============================] - 0s 263us/step - loss: 0.3967 - acc: 0.8290\n",
            "Epoch 10/100\n",
            "193/193 [==============================] - 0s 259us/step - loss: 0.3824 - acc: 0.8446\n",
            "Epoch 11/100\n",
            "193/193 [==============================] - 0s 251us/step - loss: 0.3702 - acc: 0.8549\n",
            "Epoch 12/100\n",
            "193/193 [==============================] - 0s 260us/step - loss: 0.3605 - acc: 0.8601\n",
            "Epoch 13/100\n",
            "193/193 [==============================] - 0s 251us/step - loss: 0.3508 - acc: 0.8601\n",
            "Epoch 14/100\n",
            "193/193 [==============================] - 0s 267us/step - loss: 0.3431 - acc: 0.8653\n",
            "Epoch 15/100\n",
            "193/193 [==============================] - 0s 262us/step - loss: 0.3358 - acc: 0.8601\n",
            "Epoch 16/100\n",
            "193/193 [==============================] - 0s 265us/step - loss: 0.3299 - acc: 0.8653\n",
            "Epoch 17/100\n",
            "193/193 [==============================] - 0s 261us/step - loss: 0.3239 - acc: 0.8601\n",
            "Epoch 18/100\n",
            "193/193 [==============================] - 0s 256us/step - loss: 0.3190 - acc: 0.8653\n",
            "Epoch 19/100\n",
            "193/193 [==============================] - 0s 266us/step - loss: 0.3142 - acc: 0.8653\n",
            "Epoch 20/100\n",
            "193/193 [==============================] - 0s 257us/step - loss: 0.3099 - acc: 0.8705\n",
            "Epoch 21/100\n",
            "193/193 [==============================] - 0s 258us/step - loss: 0.3057 - acc: 0.8705\n",
            "Epoch 22/100\n",
            "193/193 [==============================] - 0s 277us/step - loss: 0.3032 - acc: 0.8705\n",
            "Epoch 23/100\n",
            "193/193 [==============================] - 0s 298us/step - loss: 0.2988 - acc: 0.8808\n",
            "Epoch 24/100\n",
            "193/193 [==============================] - 0s 266us/step - loss: 0.2961 - acc: 0.8808\n",
            "Epoch 25/100\n",
            "193/193 [==============================] - 0s 257us/step - loss: 0.2933 - acc: 0.8808\n",
            "Epoch 26/100\n",
            "193/193 [==============================] - 0s 269us/step - loss: 0.2901 - acc: 0.8808\n",
            "Epoch 27/100\n",
            "193/193 [==============================] - 0s 265us/step - loss: 0.2873 - acc: 0.8808\n",
            "Epoch 28/100\n",
            "193/193 [==============================] - 0s 258us/step - loss: 0.2838 - acc: 0.8808\n",
            "Epoch 29/100\n",
            "193/193 [==============================] - 0s 263us/step - loss: 0.2821 - acc: 0.8756\n",
            "Epoch 30/100\n",
            "193/193 [==============================] - 0s 275us/step - loss: 0.2792 - acc: 0.8808\n",
            "Epoch 31/100\n",
            "193/193 [==============================] - 0s 246us/step - loss: 0.2762 - acc: 0.8808\n",
            "Epoch 32/100\n",
            "193/193 [==============================] - 0s 257us/step - loss: 0.2743 - acc: 0.8808\n",
            "Epoch 33/100\n",
            "193/193 [==============================] - 0s 267us/step - loss: 0.2729 - acc: 0.8808\n",
            "Epoch 34/100\n",
            "193/193 [==============================] - 0s 260us/step - loss: 0.2697 - acc: 0.8808\n",
            "Epoch 35/100\n",
            "193/193 [==============================] - 0s 276us/step - loss: 0.2681 - acc: 0.8860\n",
            "Epoch 36/100\n",
            "193/193 [==============================] - 0s 261us/step - loss: 0.2664 - acc: 0.8860\n",
            "Epoch 37/100\n",
            "193/193 [==============================] - 0s 257us/step - loss: 0.2637 - acc: 0.8912\n",
            "Epoch 38/100\n",
            "193/193 [==============================] - 0s 243us/step - loss: 0.2623 - acc: 0.8912\n",
            "Epoch 39/100\n",
            "193/193 [==============================] - 0s 254us/step - loss: 0.2608 - acc: 0.8912\n",
            "Epoch 40/100\n",
            "193/193 [==============================] - 0s 260us/step - loss: 0.2589 - acc: 0.8912\n",
            "Epoch 41/100\n",
            "193/193 [==============================] - 0s 264us/step - loss: 0.2571 - acc: 0.8912\n",
            "Epoch 42/100\n",
            "193/193 [==============================] - 0s 257us/step - loss: 0.2549 - acc: 0.8912\n",
            "Epoch 43/100\n",
            "193/193 [==============================] - 0s 254us/step - loss: 0.2527 - acc: 0.8912\n",
            "Epoch 44/100\n",
            "193/193 [==============================] - 0s 268us/step - loss: 0.2519 - acc: 0.8912\n",
            "Epoch 45/100\n",
            "193/193 [==============================] - 0s 253us/step - loss: 0.2505 - acc: 0.8912\n",
            "Epoch 46/100\n",
            "193/193 [==============================] - 0s 265us/step - loss: 0.2482 - acc: 0.8912\n",
            "Epoch 47/100\n",
            "193/193 [==============================] - 0s 245us/step - loss: 0.2464 - acc: 0.9016\n",
            "Epoch 48/100\n",
            "193/193 [==============================] - 0s 262us/step - loss: 0.2446 - acc: 0.8964\n",
            "Epoch 49/100\n",
            "193/193 [==============================] - 0s 256us/step - loss: 0.2427 - acc: 0.8964\n",
            "Epoch 50/100\n",
            "193/193 [==============================] - 0s 260us/step - loss: 0.2421 - acc: 0.9016\n",
            "Epoch 51/100\n",
            "193/193 [==============================] - 0s 255us/step - loss: 0.2396 - acc: 0.9016\n",
            "Epoch 52/100\n",
            "193/193 [==============================] - 0s 252us/step - loss: 0.2384 - acc: 0.8964\n",
            "Epoch 53/100\n",
            "193/193 [==============================] - 0s 268us/step - loss: 0.2377 - acc: 0.9016\n",
            "Epoch 54/100\n",
            "193/193 [==============================] - 0s 268us/step - loss: 0.2342 - acc: 0.9016\n",
            "Epoch 55/100\n",
            "193/193 [==============================] - 0s 269us/step - loss: 0.2329 - acc: 0.8964\n",
            "Epoch 56/100\n",
            "193/193 [==============================] - 0s 246us/step - loss: 0.2315 - acc: 0.9016\n",
            "Epoch 57/100\n",
            "193/193 [==============================] - 0s 251us/step - loss: 0.2306 - acc: 0.9067\n",
            "Epoch 58/100\n",
            "193/193 [==============================] - 0s 260us/step - loss: 0.2282 - acc: 0.9119\n",
            "Epoch 59/100\n",
            "193/193 [==============================] - 0s 235us/step - loss: 0.2266 - acc: 0.9171\n",
            "Epoch 60/100\n",
            "193/193 [==============================] - 0s 255us/step - loss: 0.2258 - acc: 0.9171\n",
            "Epoch 61/100\n",
            "193/193 [==============================] - 0s 254us/step - loss: 0.2239 - acc: 0.9119\n",
            "Epoch 62/100\n",
            "193/193 [==============================] - 0s 244us/step - loss: 0.2218 - acc: 0.9119\n",
            "Epoch 63/100\n",
            "193/193 [==============================] - 0s 280us/step - loss: 0.2199 - acc: 0.9223\n",
            "Epoch 64/100\n",
            "193/193 [==============================] - 0s 256us/step - loss: 0.2189 - acc: 0.9171\n",
            "Epoch 65/100\n",
            "193/193 [==============================] - 0s 246us/step - loss: 0.2163 - acc: 0.9223\n",
            "Epoch 66/100\n",
            "193/193 [==============================] - 0s 236us/step - loss: 0.2164 - acc: 0.9223\n",
            "Epoch 67/100\n",
            "193/193 [==============================] - 0s 269us/step - loss: 0.2140 - acc: 0.9171\n",
            "Epoch 68/100\n",
            "193/193 [==============================] - 0s 280us/step - loss: 0.2125 - acc: 0.9119\n",
            "Epoch 69/100\n",
            "193/193 [==============================] - 0s 257us/step - loss: 0.2109 - acc: 0.9275\n",
            "Epoch 70/100\n",
            "193/193 [==============================] - 0s 256us/step - loss: 0.2095 - acc: 0.9275\n",
            "Epoch 71/100\n",
            "193/193 [==============================] - 0s 266us/step - loss: 0.2083 - acc: 0.9223\n",
            "Epoch 72/100\n",
            "193/193 [==============================] - 0s 260us/step - loss: 0.2076 - acc: 0.9326\n",
            "Epoch 73/100\n",
            "193/193 [==============================] - 0s 257us/step - loss: 0.2046 - acc: 0.9275\n",
            "Epoch 74/100\n",
            "193/193 [==============================] - 0s 261us/step - loss: 0.2036 - acc: 0.9326\n",
            "Epoch 75/100\n",
            "193/193 [==============================] - 0s 247us/step - loss: 0.2020 - acc: 0.9275\n",
            "Epoch 76/100\n",
            "193/193 [==============================] - 0s 270us/step - loss: 0.2000 - acc: 0.9223\n",
            "Epoch 77/100\n",
            "193/193 [==============================] - 0s 261us/step - loss: 0.1994 - acc: 0.9223\n",
            "Epoch 78/100\n",
            "193/193 [==============================] - 0s 241us/step - loss: 0.1971 - acc: 0.9275\n",
            "Epoch 79/100\n",
            "193/193 [==============================] - 0s 254us/step - loss: 0.1958 - acc: 0.9275\n",
            "Epoch 80/100\n",
            "193/193 [==============================] - 0s 257us/step - loss: 0.1941 - acc: 0.9275\n",
            "Epoch 81/100\n",
            "193/193 [==============================] - 0s 273us/step - loss: 0.1929 - acc: 0.9223\n",
            "Epoch 82/100\n",
            "193/193 [==============================] - 0s 262us/step - loss: 0.1912 - acc: 0.9275\n",
            "Epoch 83/100\n",
            "193/193 [==============================] - 0s 243us/step - loss: 0.1898 - acc: 0.9275\n",
            "Epoch 84/100\n",
            "193/193 [==============================] - 0s 256us/step - loss: 0.1882 - acc: 0.9275\n",
            "Epoch 85/100\n",
            "193/193 [==============================] - 0s 253us/step - loss: 0.1870 - acc: 0.9326\n",
            "Epoch 86/100\n",
            "193/193 [==============================] - 0s 250us/step - loss: 0.1864 - acc: 0.9326\n",
            "Epoch 87/100\n",
            "193/193 [==============================] - 0s 261us/step - loss: 0.1845 - acc: 0.9326\n",
            "Epoch 88/100\n",
            "193/193 [==============================] - 0s 256us/step - loss: 0.1830 - acc: 0.9326\n",
            "Epoch 89/100\n",
            "193/193 [==============================] - 0s 261us/step - loss: 0.1812 - acc: 0.9326\n",
            "Epoch 90/100\n",
            "193/193 [==============================] - 0s 257us/step - loss: 0.1800 - acc: 0.9326\n",
            "Epoch 91/100\n",
            "193/193 [==============================] - 0s 258us/step - loss: 0.1786 - acc: 0.9326\n",
            "Epoch 92/100\n",
            "193/193 [==============================] - 0s 240us/step - loss: 0.1770 - acc: 0.9326\n",
            "Epoch 93/100\n",
            "193/193 [==============================] - 0s 269us/step - loss: 0.1761 - acc: 0.9326\n",
            "Epoch 94/100\n",
            "193/193 [==============================] - 0s 268us/step - loss: 0.1740 - acc: 0.9326\n",
            "Epoch 95/100\n",
            "193/193 [==============================] - 0s 258us/step - loss: 0.1732 - acc: 0.9326\n",
            "Epoch 96/100\n",
            "193/193 [==============================] - 0s 252us/step - loss: 0.1715 - acc: 0.9378\n",
            "Epoch 97/100\n",
            "193/193 [==============================] - 0s 258us/step - loss: 0.1709 - acc: 0.9378\n",
            "Epoch 98/100\n",
            "193/193 [==============================] - 0s 322us/step - loss: 0.1684 - acc: 0.9430\n",
            "Epoch 99/100\n",
            "193/193 [==============================] - 0s 331us/step - loss: 0.1675 - acc: 0.9430\n",
            "Epoch 100/100\n",
            "193/193 [==============================] - 0s 261us/step - loss: 0.1655 - acc: 0.9378\n",
            "49/49 [==============================] - 0s 4ms/step\n",
            "Epoch 1/100\n",
            "194/194 [==============================] - 1s 3ms/step - loss: 0.7046 - acc: 0.5464\n",
            "Epoch 2/100\n",
            "194/194 [==============================] - 0s 252us/step - loss: 0.6194 - acc: 0.6546\n",
            "Epoch 3/100\n",
            "194/194 [==============================] - 0s 258us/step - loss: 0.5586 - acc: 0.7268\n",
            "Epoch 4/100\n",
            "194/194 [==============================] - 0s 267us/step - loss: 0.5125 - acc: 0.7474\n",
            "Epoch 5/100\n",
            "194/194 [==============================] - 0s 249us/step - loss: 0.4762 - acc: 0.7680\n",
            "Epoch 6/100\n",
            "194/194 [==============================] - 0s 272us/step - loss: 0.4483 - acc: 0.7887\n",
            "Epoch 7/100\n",
            "194/194 [==============================] - 0s 262us/step - loss: 0.4264 - acc: 0.7835\n",
            "Epoch 8/100\n",
            "194/194 [==============================] - 0s 279us/step - loss: 0.4095 - acc: 0.8041\n",
            "Epoch 9/100\n",
            "194/194 [==============================] - 0s 267us/step - loss: 0.3956 - acc: 0.8144\n",
            "Epoch 10/100\n",
            "194/194 [==============================] - 0s 269us/step - loss: 0.3842 - acc: 0.8093\n",
            "Epoch 11/100\n",
            "194/194 [==============================] - 0s 271us/step - loss: 0.3747 - acc: 0.8093\n",
            "Epoch 12/100\n",
            "194/194 [==============================] - 0s 262us/step - loss: 0.3664 - acc: 0.8196\n",
            "Epoch 13/100\n",
            "194/194 [==============================] - 0s 276us/step - loss: 0.3581 - acc: 0.8247\n",
            "Epoch 14/100\n",
            "194/194 [==============================] - 0s 263us/step - loss: 0.3519 - acc: 0.8351\n",
            "Epoch 15/100\n",
            "194/194 [==============================] - 0s 268us/step - loss: 0.3453 - acc: 0.8351\n",
            "Epoch 16/100\n",
            "194/194 [==============================] - 0s 261us/step - loss: 0.3399 - acc: 0.8402\n",
            "Epoch 17/100\n",
            "194/194 [==============================] - 0s 260us/step - loss: 0.3343 - acc: 0.8402\n",
            "Epoch 18/100\n",
            "194/194 [==============================] - 0s 252us/step - loss: 0.3288 - acc: 0.8505\n",
            "Epoch 19/100\n",
            "194/194 [==============================] - 0s 256us/step - loss: 0.3241 - acc: 0.8660\n",
            "Epoch 20/100\n",
            "194/194 [==============================] - 0s 254us/step - loss: 0.3195 - acc: 0.8660\n",
            "Epoch 21/100\n",
            "194/194 [==============================] - 0s 251us/step - loss: 0.3147 - acc: 0.8608\n",
            "Epoch 22/100\n",
            "194/194 [==============================] - 0s 240us/step - loss: 0.3107 - acc: 0.8711\n",
            "Epoch 23/100\n",
            "194/194 [==============================] - 0s 266us/step - loss: 0.3062 - acc: 0.8763\n",
            "Epoch 24/100\n",
            "194/194 [==============================] - 0s 265us/step - loss: 0.3019 - acc: 0.8763\n",
            "Epoch 25/100\n",
            "194/194 [==============================] - 0s 256us/step - loss: 0.2983 - acc: 0.8763\n",
            "Epoch 26/100\n",
            "194/194 [==============================] - 0s 262us/step - loss: 0.2945 - acc: 0.8814\n",
            "Epoch 27/100\n",
            "194/194 [==============================] - 0s 229us/step - loss: 0.2905 - acc: 0.8814\n",
            "Epoch 28/100\n",
            "194/194 [==============================] - 0s 265us/step - loss: 0.2878 - acc: 0.8866\n",
            "Epoch 29/100\n",
            "194/194 [==============================] - 0s 248us/step - loss: 0.2829 - acc: 0.8866\n",
            "Epoch 30/100\n",
            "194/194 [==============================] - 0s 269us/step - loss: 0.2791 - acc: 0.8918\n",
            "Epoch 31/100\n",
            "194/194 [==============================] - 0s 270us/step - loss: 0.2757 - acc: 0.8918\n",
            "Epoch 32/100\n",
            "194/194 [==============================] - 0s 265us/step - loss: 0.2726 - acc: 0.8918\n",
            "Epoch 33/100\n",
            "194/194 [==============================] - 0s 264us/step - loss: 0.2697 - acc: 0.8918\n",
            "Epoch 34/100\n",
            "194/194 [==============================] - 0s 258us/step - loss: 0.2661 - acc: 0.8918\n",
            "Epoch 35/100\n",
            "194/194 [==============================] - 0s 253us/step - loss: 0.2635 - acc: 0.8969\n",
            "Epoch 36/100\n",
            "194/194 [==============================] - 0s 263us/step - loss: 0.2599 - acc: 0.9021\n",
            "Epoch 37/100\n",
            "194/194 [==============================] - 0s 251us/step - loss: 0.2570 - acc: 0.9021\n",
            "Epoch 38/100\n",
            "194/194 [==============================] - 0s 259us/step - loss: 0.2542 - acc: 0.9021\n",
            "Epoch 39/100\n",
            "194/194 [==============================] - 0s 247us/step - loss: 0.2515 - acc: 0.9021\n",
            "Epoch 40/100\n",
            "194/194 [==============================] - 0s 254us/step - loss: 0.2485 - acc: 0.9021\n",
            "Epoch 41/100\n",
            "194/194 [==============================] - 0s 268us/step - loss: 0.2462 - acc: 0.9021\n",
            "Epoch 42/100\n",
            "194/194 [==============================] - 0s 258us/step - loss: 0.2435 - acc: 0.9021\n",
            "Epoch 43/100\n",
            "194/194 [==============================] - 0s 264us/step - loss: 0.2407 - acc: 0.9021\n",
            "Epoch 44/100\n",
            "194/194 [==============================] - 0s 266us/step - loss: 0.2386 - acc: 0.9021\n",
            "Epoch 45/100\n",
            "194/194 [==============================] - 0s 247us/step - loss: 0.2361 - acc: 0.9072\n",
            "Epoch 46/100\n",
            "194/194 [==============================] - 0s 297us/step - loss: 0.2332 - acc: 0.9124\n",
            "Epoch 47/100\n",
            "194/194 [==============================] - 0s 255us/step - loss: 0.2307 - acc: 0.9072\n",
            "Epoch 48/100\n",
            "194/194 [==============================] - 0s 267us/step - loss: 0.2286 - acc: 0.9124\n",
            "Epoch 49/100\n",
            "194/194 [==============================] - 0s 265us/step - loss: 0.2262 - acc: 0.9124\n",
            "Epoch 50/100\n",
            "194/194 [==============================] - 0s 270us/step - loss: 0.2242 - acc: 0.9124\n",
            "Epoch 51/100\n",
            "194/194 [==============================] - 0s 253us/step - loss: 0.2222 - acc: 0.9124\n",
            "Epoch 52/100\n",
            "194/194 [==============================] - 0s 269us/step - loss: 0.2198 - acc: 0.9124\n",
            "Epoch 53/100\n",
            "194/194 [==============================] - 0s 261us/step - loss: 0.2187 - acc: 0.9072\n",
            "Epoch 54/100\n",
            "194/194 [==============================] - 0s 255us/step - loss: 0.2150 - acc: 0.9124\n",
            "Epoch 55/100\n",
            "194/194 [==============================] - 0s 259us/step - loss: 0.2138 - acc: 0.9124\n",
            "Epoch 56/100\n",
            "194/194 [==============================] - 0s 250us/step - loss: 0.2114 - acc: 0.9124\n",
            "Epoch 57/100\n",
            "194/194 [==============================] - 0s 262us/step - loss: 0.2093 - acc: 0.9124\n",
            "Epoch 58/100\n",
            "194/194 [==============================] - 0s 261us/step - loss: 0.2069 - acc: 0.9124\n",
            "Epoch 59/100\n",
            "194/194 [==============================] - 0s 246us/step - loss: 0.2049 - acc: 0.9124\n",
            "Epoch 60/100\n",
            "194/194 [==============================] - 0s 258us/step - loss: 0.2029 - acc: 0.9124\n",
            "Epoch 61/100\n",
            "194/194 [==============================] - 0s 249us/step - loss: 0.2014 - acc: 0.9124\n",
            "Epoch 62/100\n",
            "194/194 [==============================] - 0s 270us/step - loss: 0.1993 - acc: 0.9175\n",
            "Epoch 63/100\n",
            "194/194 [==============================] - 0s 250us/step - loss: 0.1972 - acc: 0.9124\n",
            "Epoch 64/100\n",
            "194/194 [==============================] - 0s 265us/step - loss: 0.1953 - acc: 0.9175\n",
            "Epoch 65/100\n",
            "194/194 [==============================] - 0s 272us/step - loss: 0.1936 - acc: 0.9124\n",
            "Epoch 66/100\n",
            "194/194 [==============================] - 0s 260us/step - loss: 0.1911 - acc: 0.9175\n",
            "Epoch 67/100\n",
            "194/194 [==============================] - 0s 253us/step - loss: 0.1893 - acc: 0.9227\n",
            "Epoch 68/100\n",
            "194/194 [==============================] - 0s 271us/step - loss: 0.1881 - acc: 0.9175\n",
            "Epoch 69/100\n",
            "194/194 [==============================] - 0s 253us/step - loss: 0.1850 - acc: 0.9227\n",
            "Epoch 70/100\n",
            "194/194 [==============================] - 0s 269us/step - loss: 0.1836 - acc: 0.9175\n",
            "Epoch 71/100\n",
            "194/194 [==============================] - 0s 253us/step - loss: 0.1821 - acc: 0.9227\n",
            "Epoch 72/100\n",
            "194/194 [==============================] - 0s 266us/step - loss: 0.1800 - acc: 0.9227\n",
            "Epoch 73/100\n",
            "194/194 [==============================] - 0s 255us/step - loss: 0.1791 - acc: 0.9175\n",
            "Epoch 74/100\n",
            "194/194 [==============================] - 0s 250us/step - loss: 0.1771 - acc: 0.9227\n",
            "Epoch 75/100\n",
            "194/194 [==============================] - 0s 258us/step - loss: 0.1750 - acc: 0.9227\n",
            "Epoch 76/100\n",
            "194/194 [==============================] - 0s 265us/step - loss: 0.1734 - acc: 0.9227\n",
            "Epoch 77/100\n",
            "194/194 [==============================] - 0s 250us/step - loss: 0.1719 - acc: 0.9227\n",
            "Epoch 78/100\n",
            "194/194 [==============================] - 0s 258us/step - loss: 0.1701 - acc: 0.9227\n",
            "Epoch 79/100\n",
            "194/194 [==============================] - 0s 263us/step - loss: 0.1682 - acc: 0.9227\n",
            "Epoch 80/100\n",
            "194/194 [==============================] - 0s 255us/step - loss: 0.1666 - acc: 0.9227\n",
            "Epoch 81/100\n",
            "194/194 [==============================] - 0s 254us/step - loss: 0.1665 - acc: 0.9227\n",
            "Epoch 82/100\n",
            "194/194 [==============================] - 0s 255us/step - loss: 0.1637 - acc: 0.9330\n",
            "Epoch 83/100\n",
            "194/194 [==============================] - 0s 268us/step - loss: 0.1619 - acc: 0.9330\n",
            "Epoch 84/100\n",
            "194/194 [==============================] - 0s 250us/step - loss: 0.1602 - acc: 0.9330\n",
            "Epoch 85/100\n",
            "194/194 [==============================] - 0s 302us/step - loss: 0.1597 - acc: 0.9278\n",
            "Epoch 86/100\n",
            "194/194 [==============================] - 0s 262us/step - loss: 0.1572 - acc: 0.9330\n",
            "Epoch 87/100\n",
            "194/194 [==============================] - 0s 265us/step - loss: 0.1561 - acc: 0.9330\n",
            "Epoch 88/100\n",
            "194/194 [==============================] - 0s 249us/step - loss: 0.1541 - acc: 0.9381\n",
            "Epoch 89/100\n",
            "194/194 [==============================] - 0s 277us/step - loss: 0.1527 - acc: 0.9485\n",
            "Epoch 90/100\n",
            "194/194 [==============================] - 0s 255us/step - loss: 0.1512 - acc: 0.9433\n",
            "Epoch 91/100\n",
            "194/194 [==============================] - 0s 271us/step - loss: 0.1495 - acc: 0.9536\n",
            "Epoch 92/100\n",
            "194/194 [==============================] - 0s 254us/step - loss: 0.1479 - acc: 0.9536\n",
            "Epoch 93/100\n",
            "194/194 [==============================] - 0s 263us/step - loss: 0.1475 - acc: 0.9433\n",
            "Epoch 94/100\n",
            "194/194 [==============================] - 0s 263us/step - loss: 0.1450 - acc: 0.9536\n",
            "Epoch 95/100\n",
            "194/194 [==============================] - 0s 264us/step - loss: 0.1437 - acc: 0.9588\n",
            "Epoch 96/100\n",
            "194/194 [==============================] - 0s 255us/step - loss: 0.1422 - acc: 0.9588\n",
            "Epoch 97/100\n",
            "194/194 [==============================] - 0s 266us/step - loss: 0.1412 - acc: 0.9536\n",
            "Epoch 98/100\n",
            "194/194 [==============================] - 0s 266us/step - loss: 0.1393 - acc: 0.9588\n",
            "Epoch 99/100\n",
            "194/194 [==============================] - 0s 268us/step - loss: 0.1385 - acc: 0.9588\n",
            "Epoch 100/100\n",
            "194/194 [==============================] - 0s 260us/step - loss: 0.1371 - acc: 0.9639\n",
            "48/48 [==============================] - 0s 4ms/step\n",
            "Epoch 1/100\n",
            "195/195 [==============================] - 1s 3ms/step - loss: 0.6811 - acc: 0.6359\n",
            "Epoch 2/100\n",
            "195/195 [==============================] - 0s 262us/step - loss: 0.6016 - acc: 0.6769\n",
            "Epoch 3/100\n",
            "195/195 [==============================] - 0s 252us/step - loss: 0.5431 - acc: 0.7744\n",
            "Epoch 4/100\n",
            "195/195 [==============================] - 0s 264us/step - loss: 0.4973 - acc: 0.7949\n",
            "Epoch 5/100\n",
            "195/195 [==============================] - 0s 256us/step - loss: 0.4595 - acc: 0.8308\n",
            "Epoch 6/100\n",
            "195/195 [==============================] - 0s 262us/step - loss: 0.4292 - acc: 0.8359\n",
            "Epoch 7/100\n",
            "195/195 [==============================] - 0s 253us/step - loss: 0.4038 - acc: 0.8410\n",
            "Epoch 8/100\n",
            "195/195 [==============================] - 0s 248us/step - loss: 0.3824 - acc: 0.8564\n",
            "Epoch 9/100\n",
            "195/195 [==============================] - 0s 249us/step - loss: 0.3637 - acc: 0.8615\n",
            "Epoch 10/100\n",
            "195/195 [==============================] - 0s 273us/step - loss: 0.3479 - acc: 0.8718\n",
            "Epoch 11/100\n",
            "195/195 [==============================] - 0s 255us/step - loss: 0.3336 - acc: 0.8769\n",
            "Epoch 12/100\n",
            "195/195 [==============================] - 0s 242us/step - loss: 0.3226 - acc: 0.8769\n",
            "Epoch 13/100\n",
            "195/195 [==============================] - 0s 280us/step - loss: 0.3121 - acc: 0.8769\n",
            "Epoch 14/100\n",
            "195/195 [==============================] - 0s 280us/step - loss: 0.3034 - acc: 0.8769\n",
            "Epoch 15/100\n",
            "195/195 [==============================] - 0s 261us/step - loss: 0.2950 - acc: 0.8872\n",
            "Epoch 16/100\n",
            "195/195 [==============================] - 0s 272us/step - loss: 0.2888 - acc: 0.8872\n",
            "Epoch 17/100\n",
            "195/195 [==============================] - 0s 258us/step - loss: 0.2818 - acc: 0.8872\n",
            "Epoch 18/100\n",
            "195/195 [==============================] - 0s 261us/step - loss: 0.2766 - acc: 0.8872\n",
            "Epoch 19/100\n",
            "195/195 [==============================] - 0s 250us/step - loss: 0.2711 - acc: 0.8974\n",
            "Epoch 20/100\n",
            "195/195 [==============================] - 0s 247us/step - loss: 0.2665 - acc: 0.8974\n",
            "Epoch 21/100\n",
            "195/195 [==============================] - 0s 264us/step - loss: 0.2617 - acc: 0.9077\n",
            "Epoch 22/100\n",
            "195/195 [==============================] - 0s 274us/step - loss: 0.2574 - acc: 0.9077\n",
            "Epoch 23/100\n",
            "195/195 [==============================] - 0s 269us/step - loss: 0.2534 - acc: 0.9077\n",
            "Epoch 24/100\n",
            "195/195 [==============================] - 0s 241us/step - loss: 0.2502 - acc: 0.9077\n",
            "Epoch 25/100\n",
            "195/195 [==============================] - 0s 258us/step - loss: 0.2466 - acc: 0.9077\n",
            "Epoch 26/100\n",
            "195/195 [==============================] - 0s 240us/step - loss: 0.2435 - acc: 0.9077\n",
            "Epoch 27/100\n",
            "195/195 [==============================] - 0s 270us/step - loss: 0.2402 - acc: 0.9077\n",
            "Epoch 28/100\n",
            "195/195 [==============================] - 0s 256us/step - loss: 0.2372 - acc: 0.9128\n",
            "Epoch 29/100\n",
            "195/195 [==============================] - 0s 272us/step - loss: 0.2345 - acc: 0.9128\n",
            "Epoch 30/100\n",
            "195/195 [==============================] - 0s 263us/step - loss: 0.2323 - acc: 0.9179\n",
            "Epoch 31/100\n",
            "195/195 [==============================] - 0s 270us/step - loss: 0.2298 - acc: 0.9231\n",
            "Epoch 32/100\n",
            "195/195 [==============================] - 0s 261us/step - loss: 0.2268 - acc: 0.9231\n",
            "Epoch 33/100\n",
            "195/195 [==============================] - 0s 268us/step - loss: 0.2248 - acc: 0.9231\n",
            "Epoch 34/100\n",
            "195/195 [==============================] - 0s 260us/step - loss: 0.2231 - acc: 0.9231\n",
            "Epoch 35/100\n",
            "195/195 [==============================] - 0s 250us/step - loss: 0.2195 - acc: 0.9282\n",
            "Epoch 36/100\n",
            "195/195 [==============================] - 0s 258us/step - loss: 0.2176 - acc: 0.9282\n",
            "Epoch 37/100\n",
            "195/195 [==============================] - 0s 255us/step - loss: 0.2151 - acc: 0.9282\n",
            "Epoch 38/100\n",
            "195/195 [==============================] - 0s 267us/step - loss: 0.2128 - acc: 0.9282\n",
            "Epoch 39/100\n",
            "195/195 [==============================] - 0s 249us/step - loss: 0.2116 - acc: 0.9282\n",
            "Epoch 40/100\n",
            "195/195 [==============================] - 0s 253us/step - loss: 0.2093 - acc: 0.9282\n",
            "Epoch 41/100\n",
            "195/195 [==============================] - 0s 257us/step - loss: 0.2065 - acc: 0.9282\n",
            "Epoch 42/100\n",
            "195/195 [==============================] - 0s 249us/step - loss: 0.2047 - acc: 0.9282\n",
            "Epoch 43/100\n",
            "195/195 [==============================] - 0s 263us/step - loss: 0.2025 - acc: 0.9282\n",
            "Epoch 44/100\n",
            "195/195 [==============================] - 0s 251us/step - loss: 0.2006 - acc: 0.9282\n",
            "Epoch 45/100\n",
            "195/195 [==============================] - 0s 359us/step - loss: 0.1986 - acc: 0.9282\n",
            "Epoch 46/100\n",
            "195/195 [==============================] - 0s 255us/step - loss: 0.1962 - acc: 0.9333\n",
            "Epoch 47/100\n",
            "195/195 [==============================] - 0s 272us/step - loss: 0.1945 - acc: 0.9333\n",
            "Epoch 48/100\n",
            "195/195 [==============================] - 0s 252us/step - loss: 0.1930 - acc: 0.9333\n",
            "Epoch 49/100\n",
            "195/195 [==============================] - 0s 269us/step - loss: 0.1914 - acc: 0.9333\n",
            "Epoch 50/100\n",
            "195/195 [==============================] - 0s 252us/step - loss: 0.1891 - acc: 0.9333\n",
            "Epoch 51/100\n",
            "195/195 [==============================] - 0s 256us/step - loss: 0.1874 - acc: 0.9333\n",
            "Epoch 52/100\n",
            "195/195 [==============================] - 0s 256us/step - loss: 0.1869 - acc: 0.9333\n",
            "Epoch 53/100\n",
            "195/195 [==============================] - 0s 257us/step - loss: 0.1845 - acc: 0.9333\n",
            "Epoch 54/100\n",
            "195/195 [==============================] - 0s 261us/step - loss: 0.1824 - acc: 0.9333\n",
            "Epoch 55/100\n",
            "195/195 [==============================] - 0s 251us/step - loss: 0.1808 - acc: 0.9282\n",
            "Epoch 56/100\n",
            "195/195 [==============================] - 0s 260us/step - loss: 0.1793 - acc: 0.9385\n",
            "Epoch 57/100\n",
            "195/195 [==============================] - 0s 248us/step - loss: 0.1774 - acc: 0.9487\n",
            "Epoch 58/100\n",
            "195/195 [==============================] - 0s 262us/step - loss: 0.1758 - acc: 0.9538\n",
            "Epoch 59/100\n",
            "195/195 [==============================] - 0s 248us/step - loss: 0.1754 - acc: 0.9385\n",
            "Epoch 60/100\n",
            "195/195 [==============================] - 0s 265us/step - loss: 0.1735 - acc: 0.9487\n",
            "Epoch 61/100\n",
            "195/195 [==============================] - 0s 240us/step - loss: 0.1721 - acc: 0.9538\n",
            "Epoch 62/100\n",
            "195/195 [==============================] - 0s 283us/step - loss: 0.1703 - acc: 0.9538\n",
            "Epoch 63/100\n",
            "195/195 [==============================] - 0s 264us/step - loss: 0.1688 - acc: 0.9538\n",
            "Epoch 64/100\n",
            "195/195 [==============================] - 0s 258us/step - loss: 0.1675 - acc: 0.9538\n",
            "Epoch 65/100\n",
            "195/195 [==============================] - 0s 252us/step - loss: 0.1664 - acc: 0.9538\n",
            "Epoch 66/100\n",
            "195/195 [==============================] - 0s 265us/step - loss: 0.1652 - acc: 0.9487\n",
            "Epoch 67/100\n",
            "195/195 [==============================] - 0s 246us/step - loss: 0.1643 - acc: 0.9538\n",
            "Epoch 68/100\n",
            "195/195 [==============================] - 0s 248us/step - loss: 0.1633 - acc: 0.9487\n",
            "Epoch 69/100\n",
            "195/195 [==============================] - 0s 237us/step - loss: 0.1609 - acc: 0.9487\n",
            "Epoch 70/100\n",
            "195/195 [==============================] - 0s 289us/step - loss: 0.1600 - acc: 0.9487\n",
            "Epoch 71/100\n",
            "195/195 [==============================] - 0s 245us/step - loss: 0.1587 - acc: 0.9487\n",
            "Epoch 72/100\n",
            "195/195 [==============================] - 0s 257us/step - loss: 0.1574 - acc: 0.9487\n",
            "Epoch 73/100\n",
            "195/195 [==============================] - 0s 240us/step - loss: 0.1568 - acc: 0.9487\n",
            "Epoch 74/100\n",
            "195/195 [==============================] - 0s 293us/step - loss: 0.1553 - acc: 0.9487\n",
            "Epoch 75/100\n",
            "195/195 [==============================] - 0s 254us/step - loss: 0.1539 - acc: 0.9487\n",
            "Epoch 76/100\n",
            "195/195 [==============================] - 0s 253us/step - loss: 0.1522 - acc: 0.9487\n",
            "Epoch 77/100\n",
            "195/195 [==============================] - 0s 254us/step - loss: 0.1511 - acc: 0.9487\n",
            "Epoch 78/100\n",
            "195/195 [==============================] - 0s 255us/step - loss: 0.1504 - acc: 0.9538\n",
            "Epoch 79/100\n",
            "195/195 [==============================] - 0s 256us/step - loss: 0.1491 - acc: 0.9487\n",
            "Epoch 80/100\n",
            "195/195 [==============================] - 0s 249us/step - loss: 0.1481 - acc: 0.9487\n",
            "Epoch 81/100\n",
            "195/195 [==============================] - 0s 262us/step - loss: 0.1468 - acc: 0.9538\n",
            "Epoch 82/100\n",
            "195/195 [==============================] - 0s 258us/step - loss: 0.1458 - acc: 0.9538\n",
            "Epoch 83/100\n",
            "195/195 [==============================] - 0s 268us/step - loss: 0.1445 - acc: 0.9538\n",
            "Epoch 84/100\n",
            "195/195 [==============================] - 0s 251us/step - loss: 0.1447 - acc: 0.9538\n",
            "Epoch 85/100\n",
            "195/195 [==============================] - 0s 244us/step - loss: 0.1423 - acc: 0.9538\n",
            "Epoch 86/100\n",
            "195/195 [==============================] - 0s 266us/step - loss: 0.1410 - acc: 0.9538\n",
            "Epoch 87/100\n",
            "195/195 [==============================] - 0s 300us/step - loss: 0.1398 - acc: 0.9538\n",
            "Epoch 88/100\n",
            "195/195 [==============================] - 0s 259us/step - loss: 0.1387 - acc: 0.9538\n",
            "Epoch 89/100\n",
            "195/195 [==============================] - 0s 271us/step - loss: 0.1374 - acc: 0.9590\n",
            "Epoch 90/100\n",
            "195/195 [==============================] - 0s 257us/step - loss: 0.1367 - acc: 0.9538\n",
            "Epoch 91/100\n",
            "195/195 [==============================] - 0s 260us/step - loss: 0.1352 - acc: 0.9590\n",
            "Epoch 92/100\n",
            "195/195 [==============================] - 0s 268us/step - loss: 0.1338 - acc: 0.9590\n",
            "Epoch 93/100\n",
            "195/195 [==============================] - 0s 257us/step - loss: 0.1329 - acc: 0.9590\n",
            "Epoch 94/100\n",
            "195/195 [==============================] - 0s 249us/step - loss: 0.1326 - acc: 0.9590\n",
            "Epoch 95/100\n",
            "195/195 [==============================] - 0s 243us/step - loss: 0.1316 - acc: 0.9590\n",
            "Epoch 96/100\n",
            "195/195 [==============================] - 0s 249us/step - loss: 0.1297 - acc: 0.9692\n",
            "Epoch 97/100\n",
            "195/195 [==============================] - 0s 238us/step - loss: 0.1281 - acc: 0.9692\n",
            "Epoch 98/100\n",
            "195/195 [==============================] - 0s 251us/step - loss: 0.1273 - acc: 0.9692\n",
            "Epoch 99/100\n",
            "195/195 [==============================] - 0s 298us/step - loss: 0.1258 - acc: 0.9692\n",
            "Epoch 100/100\n",
            "195/195 [==============================] - 0s 270us/step - loss: 0.1249 - acc: 0.9641\n",
            "47/47 [==============================] - 0s 5ms/step\n",
            "Results: 82.61% (4.60%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U63KYTEtpcK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Dense, Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import SGD, Adam, Nadam\n",
        "from keras.wrappers.scikit_learn import KerasClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uyj4PQN1iyeJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = KerasClassifier(build_fn=baseline, \n",
        "                               epochs=5,\n",
        "                               batch_size=10,\n",
        "                               verbose=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyfQB6mToFBK",
        "colab_type": "code",
        "outputId": "c29fb15f-1579-4e81-f16c-7748da5f6089",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1231
        }
      },
      "source": [
        "\n",
        "param_grid = {}\n",
        "# checking that this works\n",
        "grid = GridSearchCV(estimator=model, \n",
        "                    param_grid=param_grid, \n",
        "                    cv=5,\n",
        "                    n_jobs=1)\n",
        "grid_result = grid.fit(X_train, y_train)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "193/193 [==============================] - 5s 28ms/step - loss: 0.6921 - acc: 0.5544\n",
            "Epoch 2/5\n",
            "193/193 [==============================] - 0s 250us/step - loss: 0.6432 - acc: 0.6166\n",
            "Epoch 3/5\n",
            "193/193 [==============================] - 0s 262us/step - loss: 0.6018 - acc: 0.6684\n",
            "Epoch 4/5\n",
            "193/193 [==============================] - 0s 250us/step - loss: 0.5680 - acc: 0.7306\n",
            "Epoch 5/5\n",
            "193/193 [==============================] - 0s 262us/step - loss: 0.5382 - acc: 0.7824\n",
            "49/49 [==============================] - 2s 44ms/step\n",
            "193/193 [==============================] - 0s 189us/step\n",
            "Epoch 1/5\n",
            "193/193 [==============================] - 5s 27ms/step - loss: 0.6225 - acc: 0.6321\n",
            "Epoch 2/5\n",
            "193/193 [==============================] - 0s 255us/step - loss: 0.5782 - acc: 0.7254\n",
            "Epoch 3/5\n",
            "193/193 [==============================] - 0s 261us/step - loss: 0.5451 - acc: 0.7513\n",
            "Epoch 4/5\n",
            "193/193 [==============================] - 0s 252us/step - loss: 0.5140 - acc: 0.7668\n",
            "Epoch 5/5\n",
            "193/193 [==============================] - 0s 265us/step - loss: 0.4870 - acc: 0.7772\n",
            "49/49 [==============================] - 2s 45ms/step\n",
            "193/193 [==============================] - 0s 205us/step\n",
            "Epoch 1/5\n",
            "194/194 [==============================] - 5s 27ms/step - loss: 0.7560 - acc: 0.4794\n",
            "Epoch 2/5\n",
            "194/194 [==============================] - 0s 249us/step - loss: 0.6902 - acc: 0.5567\n",
            "Epoch 3/5\n",
            "194/194 [==============================] - 0s 254us/step - loss: 0.6356 - acc: 0.6186\n",
            "Epoch 4/5\n",
            "194/194 [==============================] - 0s 257us/step - loss: 0.5879 - acc: 0.7165\n",
            "Epoch 5/5\n",
            "194/194 [==============================] - 0s 256us/step - loss: 0.5493 - acc: 0.7371\n",
            "48/48 [==============================] - 2s 46ms/step\n",
            "194/194 [==============================] - 0s 180us/step\n",
            "Epoch 1/5\n",
            "194/194 [==============================] - 5s 28ms/step - loss: 0.6993 - acc: 0.5670\n",
            "Epoch 2/5\n",
            "194/194 [==============================] - 0s 248us/step - loss: 0.6423 - acc: 0.6237\n",
            "Epoch 3/5\n",
            "194/194 [==============================] - 0s 243us/step - loss: 0.5974 - acc: 0.7113\n",
            "Epoch 4/5\n",
            "194/194 [==============================] - 0s 251us/step - loss: 0.5609 - acc: 0.7423\n",
            "Epoch 5/5\n",
            "194/194 [==============================] - 0s 274us/step - loss: 0.5304 - acc: 0.7938\n",
            "48/48 [==============================] - 2s 48ms/step\n",
            "194/194 [==============================] - 0s 192us/step\n",
            "Epoch 1/5\n",
            "194/194 [==============================] - 5s 28ms/step - loss: 0.8614 - acc: 0.4381\n",
            "Epoch 2/5\n",
            "194/194 [==============================] - 0s 247us/step - loss: 0.7797 - acc: 0.4794\n",
            "Epoch 3/5\n",
            "194/194 [==============================] - 0s 242us/step - loss: 0.7142 - acc: 0.5567\n",
            "Epoch 4/5\n",
            "194/194 [==============================] - 0s 267us/step - loss: 0.6618 - acc: 0.6134\n",
            "Epoch 5/5\n",
            "194/194 [==============================] - 0s 264us/step - loss: 0.6164 - acc: 0.6959\n",
            "48/48 [==============================] - 2s 48ms/step\n",
            "194/194 [==============================] - 0s 205us/step\n",
            "Epoch 1/5\n",
            "242/242 [==============================] - 6s 23ms/step - loss: 0.9078 - acc: 0.5165\n",
            "Epoch 2/5\n",
            "242/242 [==============================] - 0s 256us/step - loss: 0.7941 - acc: 0.5620\n",
            "Epoch 3/5\n",
            "242/242 [==============================] - 0s 261us/step - loss: 0.7065 - acc: 0.5992\n",
            "Epoch 4/5\n",
            "242/242 [==============================] - 0s 251us/step - loss: 0.6442 - acc: 0.6446\n",
            "Epoch 5/5\n",
            "242/242 [==============================] - 0s 255us/step - loss: 0.5914 - acc: 0.6653\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2scXwxjoSa0",
        "colab_type": "code",
        "outputId": "48ec74ea-c67a-4dec-a7e9-622b701caf93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "grid_result"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
              "       estimator=<keras.wrappers.scikit_learn.KerasClassifier object at 0x7fcd2f1ff3c8>,\n",
              "       fit_params=None, iid='warn', n_jobs=1, param_grid={},\n",
              "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
              "       scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8H1mqf6spAKk",
        "colab_type": "code",
        "outputId": "8a596313-b5c1-4df1-882f-74b55c7a974d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "grid_result.best_estimator_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.wrappers.scikit_learn.KerasClassifier at 0x7fcd2ed2def0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBT0hy9IpCXf",
        "colab_type": "code",
        "outputId": "f9101f80-c346-41b7-c42f-6b552af348cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "grid_result.best_params_\n",
        "# i just checked how the grid was working so this is expected"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyZkBWePpEJH",
        "colab_type": "code",
        "outputId": "351c3dfd-9099-425c-a758-04a55e0fa0d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "grid_result.best_score_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7644628069617532"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhsmpM_bpFp_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# need to reset my model building stage. \n",
        "\n",
        "def create_model(dense_layers=2,\n",
        "                 dense_nodes=5,\n",
        "                 dropout=False,\n",
        "                 dropout_pct=0.0,\n",
        "                 activation='sigmoid',\n",
        "                 weight_initializer='glorot_uniform',\n",
        "                 optimizer=SGD,\n",
        "                 lr=0.0001,\n",
        "                 input_shape=(X_train.shape[1],)):\n",
        "    \n",
        "    model = Sequential()\n",
        "   \n",
        "    model.add(Dense(dense_nodes, \n",
        "                    input_shape=input_shape,\n",
        "                    kernel_initializer=weight_initializer,\n",
        "                    activation=activation))\n",
        "    for _ in range(dense_layers):\n",
        "        model.add(Dense(dense_nodes,\n",
        "                        kernel_initializer=weight_initializer,\n",
        "                        activation=activation))\n",
        "        if dropout:\n",
        "            model.add(Dropout(rate=dropout_pct))\n",
        "\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    \n",
        "    optimizer=optimizer(lr=lr)\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['acc'])\n",
        "              \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_1uulPypQwY",
        "colab_type": "code",
        "outputId": "a8a75a0d-3fc6-4b22-dfca-1fe3f1452805",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 13052
        }
      },
      "source": [
        "\n",
        "param_grid = {'batch_size': [10, 80],\n",
        "              'epochs': [20,90]}\n",
        "\n",
        "model = KerasClassifier(build_fn=create_model, \n",
        "                               epochs=20,\n",
        "                               batch_size=10,\n",
        "                               verbose=10)\n",
        "\n",
        "grid = GridSearchCV(estimator=model, \n",
        "                    param_grid=param_grid, \n",
        "                    cv=3,\n",
        "                    n_jobs=1)\n",
        "grid_result = grid.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "Epoch 2/20\n",
            "Epoch 3/20\n",
            "Epoch 4/20\n",
            "Epoch 5/20\n",
            "Epoch 6/20\n",
            "Epoch 7/20\n",
            "Epoch 8/20\n",
            "Epoch 9/20\n",
            "Epoch 10/20\n",
            "Epoch 11/20\n",
            "Epoch 12/20\n",
            "Epoch 13/20\n",
            "Epoch 14/20\n",
            "Epoch 15/20\n",
            "Epoch 16/20\n",
            "Epoch 17/20\n",
            "Epoch 18/20\n",
            "Epoch 19/20\n",
            "Epoch 20/20\n",
            "Epoch 1/20\n",
            "Epoch 2/20\n",
            "Epoch 3/20\n",
            "Epoch 4/20\n",
            "Epoch 5/20\n",
            "Epoch 6/20\n",
            "Epoch 7/20\n",
            "Epoch 8/20\n",
            "Epoch 9/20\n",
            "Epoch 10/20\n",
            "Epoch 11/20\n",
            "Epoch 12/20\n",
            "Epoch 13/20\n",
            "Epoch 14/20\n",
            "Epoch 15/20\n",
            "Epoch 16/20\n",
            "Epoch 17/20\n",
            "Epoch 18/20\n",
            "Epoch 19/20\n",
            "Epoch 20/20\n",
            "Epoch 1/20\n",
            "Epoch 2/20\n",
            "Epoch 3/20\n",
            "Epoch 4/20\n",
            "Epoch 5/20\n",
            "Epoch 6/20\n",
            "Epoch 7/20\n",
            "Epoch 8/20\n",
            "Epoch 9/20\n",
            "Epoch 10/20\n",
            "Epoch 11/20\n",
            "Epoch 12/20\n",
            "Epoch 13/20\n",
            "Epoch 14/20\n",
            "Epoch 15/20\n",
            "Epoch 16/20\n",
            "Epoch 17/20\n",
            "Epoch 18/20\n",
            "Epoch 19/20\n",
            "Epoch 20/20\n",
            "Epoch 1/90\n",
            "Epoch 2/90\n",
            "Epoch 3/90\n",
            "Epoch 4/90\n",
            "Epoch 5/90\n",
            "Epoch 6/90\n",
            "Epoch 7/90\n",
            "Epoch 8/90\n",
            "Epoch 9/90\n",
            "Epoch 10/90\n",
            "Epoch 11/90\n",
            "Epoch 12/90\n",
            "Epoch 13/90\n",
            "Epoch 14/90\n",
            "Epoch 15/90\n",
            "Epoch 16/90\n",
            "Epoch 17/90\n",
            "Epoch 18/90\n",
            "Epoch 19/90\n",
            "Epoch 20/90\n",
            "Epoch 21/90\n",
            "Epoch 22/90\n",
            "Epoch 23/90\n",
            "Epoch 24/90\n",
            "Epoch 25/90\n",
            "Epoch 26/90\n",
            "Epoch 27/90\n",
            "Epoch 28/90\n",
            "Epoch 29/90\n",
            "Epoch 30/90\n",
            "Epoch 31/90\n",
            "Epoch 32/90\n",
            "Epoch 33/90\n",
            "Epoch 34/90\n",
            "Epoch 35/90\n",
            "Epoch 36/90\n",
            "Epoch 37/90\n",
            "Epoch 38/90\n",
            "Epoch 39/90\n",
            "Epoch 40/90\n",
            "Epoch 41/90\n",
            "Epoch 42/90\n",
            "Epoch 43/90\n",
            "Epoch 44/90\n",
            "Epoch 45/90\n",
            "Epoch 46/90\n",
            "Epoch 47/90\n",
            "Epoch 48/90\n",
            "Epoch 49/90\n",
            "Epoch 50/90\n",
            "Epoch 51/90\n",
            "Epoch 52/90\n",
            "Epoch 53/90\n",
            "Epoch 54/90\n",
            "Epoch 55/90\n",
            "Epoch 56/90\n",
            "Epoch 57/90\n",
            "Epoch 58/90\n",
            "Epoch 59/90\n",
            "Epoch 60/90\n",
            "Epoch 61/90\n",
            "Epoch 62/90\n",
            "Epoch 63/90\n",
            "Epoch 64/90\n",
            "Epoch 65/90\n",
            "Epoch 66/90\n",
            "Epoch 67/90\n",
            "Epoch 68/90\n",
            "Epoch 69/90\n",
            "Epoch 70/90\n",
            "Epoch 71/90\n",
            "Epoch 72/90\n",
            "Epoch 73/90\n",
            "Epoch 74/90\n",
            "Epoch 75/90\n",
            "Epoch 76/90\n",
            "Epoch 77/90\n",
            "Epoch 78/90\n",
            "Epoch 79/90\n",
            "Epoch 80/90\n",
            "Epoch 81/90\n",
            "Epoch 82/90\n",
            "Epoch 83/90\n",
            "Epoch 84/90\n",
            "Epoch 85/90\n",
            "Epoch 86/90\n",
            "Epoch 87/90\n",
            "Epoch 88/90\n",
            "Epoch 89/90\n",
            "Epoch 90/90\n",
            "Epoch 1/90\n",
            "Epoch 2/90\n",
            "Epoch 3/90\n",
            "Epoch 4/90\n",
            "Epoch 5/90\n",
            "Epoch 6/90\n",
            "Epoch 7/90\n",
            "Epoch 8/90\n",
            "Epoch 9/90\n",
            "Epoch 10/90\n",
            "Epoch 11/90\n",
            "Epoch 12/90\n",
            "Epoch 13/90\n",
            "Epoch 14/90\n",
            "Epoch 15/90\n",
            "Epoch 16/90\n",
            "Epoch 17/90\n",
            "Epoch 18/90\n",
            "Epoch 19/90\n",
            "Epoch 20/90\n",
            "Epoch 21/90\n",
            "Epoch 22/90\n",
            "Epoch 23/90\n",
            "Epoch 24/90\n",
            "Epoch 25/90\n",
            "Epoch 26/90\n",
            "Epoch 27/90\n",
            "Epoch 28/90\n",
            "Epoch 29/90\n",
            "Epoch 30/90\n",
            "Epoch 31/90\n",
            "Epoch 32/90\n",
            "Epoch 33/90\n",
            "Epoch 34/90\n",
            "Epoch 35/90\n",
            "Epoch 36/90\n",
            "Epoch 37/90\n",
            "Epoch 38/90\n",
            "Epoch 39/90\n",
            "Epoch 40/90\n",
            "Epoch 41/90\n",
            "Epoch 42/90\n",
            "Epoch 43/90\n",
            "Epoch 44/90\n",
            "Epoch 45/90\n",
            "Epoch 46/90\n",
            "Epoch 47/90\n",
            "Epoch 48/90\n",
            "Epoch 49/90\n",
            "Epoch 50/90\n",
            "Epoch 51/90\n",
            "Epoch 52/90\n",
            "Epoch 53/90\n",
            "Epoch 54/90\n",
            "Epoch 55/90\n",
            "Epoch 56/90\n",
            "Epoch 57/90\n",
            "Epoch 58/90\n",
            "Epoch 59/90\n",
            "Epoch 60/90\n",
            "Epoch 61/90\n",
            "Epoch 62/90\n",
            "Epoch 63/90\n",
            "Epoch 64/90\n",
            "Epoch 65/90\n",
            "Epoch 66/90\n",
            "Epoch 67/90\n",
            "Epoch 68/90\n",
            "Epoch 69/90\n",
            "Epoch 70/90\n",
            "Epoch 71/90\n",
            "Epoch 72/90\n",
            "Epoch 73/90\n",
            "Epoch 74/90\n",
            "Epoch 75/90\n",
            "Epoch 76/90\n",
            "Epoch 77/90\n",
            "Epoch 78/90\n",
            "Epoch 79/90\n",
            "Epoch 80/90\n",
            "Epoch 81/90\n",
            "Epoch 82/90\n",
            "Epoch 83/90\n",
            "Epoch 84/90\n",
            "Epoch 85/90\n",
            "Epoch 86/90\n",
            "Epoch 87/90\n",
            "Epoch 88/90\n",
            "Epoch 89/90\n",
            "Epoch 90/90\n",
            "Epoch 1/90\n",
            "Epoch 2/90\n",
            "Epoch 3/90\n",
            "Epoch 4/90\n",
            "Epoch 5/90\n",
            "Epoch 6/90\n",
            "Epoch 7/90\n",
            "Epoch 8/90\n",
            "Epoch 9/90\n",
            "Epoch 10/90\n",
            "Epoch 11/90\n",
            "Epoch 12/90\n",
            "Epoch 13/90\n",
            "Epoch 14/90\n",
            "Epoch 15/90\n",
            "Epoch 16/90\n",
            "Epoch 17/90\n",
            "Epoch 18/90\n",
            "Epoch 19/90\n",
            "Epoch 20/90\n",
            "Epoch 21/90\n",
            "Epoch 22/90\n",
            "Epoch 23/90\n",
            "Epoch 24/90\n",
            "Epoch 25/90\n",
            "Epoch 26/90\n",
            "Epoch 27/90\n",
            "Epoch 28/90\n",
            "Epoch 29/90\n",
            "Epoch 30/90\n",
            "Epoch 31/90\n",
            "Epoch 32/90\n",
            "Epoch 33/90\n",
            "Epoch 34/90\n",
            "Epoch 35/90\n",
            "Epoch 36/90\n",
            "Epoch 37/90\n",
            "Epoch 38/90\n",
            "Epoch 39/90\n",
            "Epoch 40/90\n",
            "Epoch 41/90\n",
            "Epoch 42/90\n",
            "Epoch 43/90\n",
            "Epoch 44/90\n",
            "Epoch 45/90\n",
            "Epoch 46/90\n",
            "Epoch 47/90\n",
            "Epoch 48/90\n",
            "Epoch 49/90\n",
            "Epoch 50/90\n",
            "Epoch 51/90\n",
            "Epoch 52/90\n",
            "Epoch 53/90\n",
            "Epoch 54/90\n",
            "Epoch 55/90\n",
            "Epoch 56/90\n",
            "Epoch 57/90\n",
            "Epoch 58/90\n",
            "Epoch 59/90\n",
            "Epoch 60/90\n",
            "Epoch 61/90\n",
            "Epoch 62/90\n",
            "Epoch 63/90\n",
            "Epoch 64/90\n",
            "Epoch 65/90\n",
            "Epoch 66/90\n",
            "Epoch 67/90\n",
            "Epoch 68/90\n",
            "Epoch 69/90\n",
            "Epoch 70/90\n",
            "Epoch 71/90\n",
            "Epoch 72/90\n",
            "Epoch 73/90\n",
            "Epoch 74/90\n",
            "Epoch 75/90\n",
            "Epoch 76/90\n",
            "Epoch 77/90\n",
            "Epoch 78/90\n",
            "Epoch 79/90\n",
            "Epoch 80/90\n",
            "Epoch 81/90\n",
            "Epoch 82/90\n",
            "Epoch 83/90\n",
            "Epoch 84/90\n",
            "Epoch 85/90\n",
            "Epoch 86/90\n",
            "Epoch 87/90\n",
            "Epoch 88/90\n",
            "Epoch 89/90\n",
            "Epoch 90/90\n",
            "Epoch 1/20\n",
            "Epoch 2/20\n",
            "Epoch 3/20\n",
            "Epoch 4/20\n",
            "Epoch 5/20\n",
            "Epoch 6/20\n",
            "Epoch 7/20\n",
            "Epoch 8/20\n",
            "Epoch 9/20\n",
            "Epoch 10/20\n",
            "Epoch 11/20\n",
            "Epoch 12/20\n",
            "Epoch 13/20\n",
            "Epoch 14/20\n",
            "Epoch 15/20\n",
            "Epoch 16/20\n",
            "Epoch 17/20\n",
            "Epoch 18/20\n",
            "Epoch 19/20\n",
            "Epoch 20/20\n",
            "Epoch 1/20\n",
            "Epoch 2/20\n",
            "Epoch 3/20\n",
            "Epoch 4/20\n",
            "Epoch 5/20\n",
            "Epoch 6/20\n",
            "Epoch 7/20\n",
            "Epoch 8/20\n",
            "Epoch 9/20\n",
            "Epoch 10/20\n",
            "Epoch 11/20\n",
            "Epoch 12/20\n",
            "Epoch 13/20\n",
            "Epoch 14/20\n",
            "Epoch 15/20\n",
            "Epoch 16/20\n",
            "Epoch 17/20\n",
            "Epoch 18/20\n",
            "Epoch 19/20\n",
            "Epoch 20/20\n",
            "Epoch 1/20\n",
            "Epoch 2/20\n",
            "Epoch 3/20\n",
            "Epoch 4/20\n",
            "Epoch 5/20\n",
            "Epoch 6/20\n",
            "Epoch 7/20\n",
            "Epoch 8/20\n",
            "Epoch 9/20\n",
            "Epoch 10/20\n",
            "Epoch 11/20\n",
            "Epoch 12/20\n",
            "Epoch 13/20\n",
            "Epoch 14/20\n",
            "Epoch 15/20\n",
            "Epoch 16/20\n",
            "Epoch 17/20\n",
            "Epoch 18/20\n",
            "Epoch 19/20\n",
            "Epoch 20/20\n",
            "Epoch 1/90\n",
            "Epoch 2/90\n",
            "Epoch 3/90\n",
            "Epoch 4/90\n",
            "Epoch 5/90\n",
            "Epoch 6/90\n",
            "Epoch 7/90\n",
            "Epoch 8/90\n",
            "Epoch 9/90\n",
            "Epoch 10/90\n",
            "Epoch 11/90\n",
            "Epoch 12/90\n",
            "Epoch 13/90\n",
            "Epoch 14/90\n",
            "Epoch 15/90\n",
            "Epoch 16/90\n",
            "Epoch 17/90\n",
            "Epoch 18/90\n",
            "Epoch 19/90\n",
            "Epoch 20/90\n",
            "Epoch 21/90\n",
            "Epoch 22/90\n",
            "Epoch 23/90\n",
            "Epoch 24/90\n",
            "Epoch 25/90\n",
            "Epoch 26/90\n",
            "Epoch 27/90\n",
            "Epoch 28/90\n",
            "Epoch 29/90\n",
            "Epoch 30/90\n",
            "Epoch 31/90\n",
            "Epoch 32/90\n",
            "Epoch 33/90\n",
            "Epoch 34/90\n",
            "Epoch 35/90\n",
            "Epoch 36/90\n",
            "Epoch 37/90\n",
            "Epoch 38/90\n",
            "Epoch 39/90\n",
            "Epoch 40/90\n",
            "Epoch 41/90\n",
            "Epoch 42/90\n",
            "Epoch 43/90\n",
            "Epoch 44/90\n",
            "Epoch 45/90\n",
            "Epoch 46/90\n",
            "Epoch 47/90\n",
            "Epoch 48/90\n",
            "Epoch 49/90\n",
            "Epoch 50/90\n",
            "Epoch 51/90\n",
            "Epoch 52/90\n",
            "Epoch 53/90\n",
            "Epoch 54/90\n",
            "Epoch 55/90\n",
            "Epoch 56/90\n",
            "Epoch 57/90\n",
            "Epoch 58/90\n",
            "Epoch 59/90\n",
            "Epoch 60/90\n",
            "Epoch 61/90\n",
            "Epoch 62/90\n",
            "Epoch 63/90\n",
            "Epoch 64/90\n",
            "Epoch 65/90\n",
            "Epoch 66/90\n",
            "Epoch 67/90\n",
            "Epoch 68/90\n",
            "Epoch 69/90\n",
            "Epoch 70/90\n",
            "Epoch 71/90\n",
            "Epoch 72/90\n",
            "Epoch 73/90\n",
            "Epoch 74/90\n",
            "Epoch 75/90\n",
            "Epoch 76/90\n",
            "Epoch 77/90\n",
            "Epoch 78/90\n",
            "Epoch 79/90\n",
            "Epoch 80/90\n",
            "Epoch 81/90\n",
            "Epoch 82/90\n",
            "Epoch 83/90\n",
            "Epoch 84/90\n",
            "Epoch 85/90\n",
            "Epoch 86/90\n",
            "Epoch 87/90\n",
            "Epoch 88/90\n",
            "Epoch 89/90\n",
            "Epoch 90/90\n",
            "Epoch 1/90\n",
            "Epoch 2/90\n",
            "Epoch 3/90\n",
            "Epoch 4/90\n",
            "Epoch 5/90\n",
            "Epoch 6/90\n",
            "Epoch 7/90\n",
            "Epoch 8/90\n",
            "Epoch 9/90\n",
            "Epoch 10/90\n",
            "Epoch 11/90\n",
            "Epoch 12/90\n",
            "Epoch 13/90\n",
            "Epoch 14/90\n",
            "Epoch 15/90\n",
            "Epoch 16/90\n",
            "Epoch 17/90\n",
            "Epoch 18/90\n",
            "Epoch 19/90\n",
            "Epoch 20/90\n",
            "Epoch 21/90\n",
            "Epoch 22/90\n",
            "Epoch 23/90\n",
            "Epoch 24/90\n",
            "Epoch 25/90\n",
            "Epoch 26/90\n",
            "Epoch 27/90\n",
            "Epoch 28/90\n",
            "Epoch 29/90\n",
            "Epoch 30/90\n",
            "Epoch 31/90\n",
            "Epoch 32/90\n",
            "Epoch 33/90\n",
            "Epoch 34/90\n",
            "Epoch 35/90\n",
            "Epoch 36/90\n",
            "Epoch 37/90\n",
            "Epoch 38/90\n",
            "Epoch 39/90\n",
            "Epoch 40/90\n",
            "Epoch 41/90\n",
            "Epoch 42/90\n",
            "Epoch 43/90\n",
            "Epoch 44/90\n",
            "Epoch 45/90\n",
            "Epoch 46/90\n",
            "Epoch 47/90\n",
            "Epoch 48/90\n",
            "Epoch 49/90\n",
            "Epoch 50/90\n",
            "Epoch 51/90\n",
            "Epoch 52/90\n",
            "Epoch 53/90\n",
            "Epoch 54/90\n",
            "Epoch 55/90\n",
            "Epoch 56/90\n",
            "Epoch 57/90\n",
            "Epoch 58/90\n",
            "Epoch 59/90\n",
            "Epoch 60/90\n",
            "Epoch 61/90\n",
            "Epoch 62/90\n",
            "Epoch 63/90\n",
            "Epoch 64/90\n",
            "Epoch 65/90\n",
            "Epoch 66/90\n",
            "Epoch 67/90\n",
            "Epoch 68/90\n",
            "Epoch 69/90\n",
            "Epoch 70/90\n",
            "Epoch 71/90\n",
            "Epoch 72/90\n",
            "Epoch 73/90\n",
            "Epoch 74/90\n",
            "Epoch 75/90\n",
            "Epoch 76/90\n",
            "Epoch 77/90\n",
            "Epoch 78/90\n",
            "Epoch 79/90\n",
            "Epoch 80/90\n",
            "Epoch 81/90\n",
            "Epoch 82/90\n",
            "Epoch 83/90\n",
            "Epoch 84/90\n",
            "Epoch 85/90\n",
            "Epoch 86/90\n",
            "Epoch 87/90\n",
            "Epoch 88/90\n",
            "Epoch 89/90\n",
            "Epoch 90/90\n",
            "Epoch 1/90\n",
            "Epoch 2/90\n",
            "Epoch 3/90\n",
            "Epoch 4/90\n",
            "Epoch 5/90\n",
            "Epoch 6/90\n",
            "Epoch 7/90\n",
            "Epoch 8/90\n",
            "Epoch 9/90\n",
            "Epoch 10/90\n",
            "Epoch 11/90\n",
            "Epoch 12/90\n",
            "Epoch 13/90\n",
            "Epoch 14/90\n",
            "Epoch 15/90\n",
            "Epoch 16/90\n",
            "Epoch 17/90\n",
            "Epoch 18/90\n",
            "Epoch 19/90\n",
            "Epoch 20/90\n",
            "Epoch 21/90\n",
            "Epoch 22/90\n",
            "Epoch 23/90\n",
            "Epoch 24/90\n",
            "Epoch 25/90\n",
            "Epoch 26/90\n",
            "Epoch 27/90\n",
            "Epoch 28/90\n",
            "Epoch 29/90\n",
            "Epoch 30/90\n",
            "Epoch 31/90\n",
            "Epoch 32/90\n",
            "Epoch 33/90\n",
            "Epoch 34/90\n",
            "Epoch 35/90\n",
            "Epoch 36/90\n",
            "Epoch 37/90\n",
            "Epoch 38/90\n",
            "Epoch 39/90\n",
            "Epoch 40/90\n",
            "Epoch 41/90\n",
            "Epoch 42/90\n",
            "Epoch 43/90\n",
            "Epoch 44/90\n",
            "Epoch 45/90\n",
            "Epoch 46/90\n",
            "Epoch 47/90\n",
            "Epoch 48/90\n",
            "Epoch 49/90\n",
            "Epoch 50/90\n",
            "Epoch 51/90\n",
            "Epoch 52/90\n",
            "Epoch 53/90\n",
            "Epoch 54/90\n",
            "Epoch 55/90\n",
            "Epoch 56/90\n",
            "Epoch 57/90\n",
            "Epoch 58/90\n",
            "Epoch 59/90\n",
            "Epoch 60/90\n",
            "Epoch 61/90\n",
            "Epoch 62/90\n",
            "Epoch 63/90\n",
            "Epoch 64/90\n",
            "Epoch 65/90\n",
            "Epoch 66/90\n",
            "Epoch 67/90\n",
            "Epoch 68/90\n",
            "Epoch 69/90\n",
            "Epoch 70/90\n",
            "Epoch 71/90\n",
            "Epoch 72/90\n",
            "Epoch 73/90\n",
            "Epoch 74/90\n",
            "Epoch 75/90\n",
            "Epoch 76/90\n",
            "Epoch 77/90\n",
            "Epoch 78/90\n",
            "Epoch 79/90\n",
            "Epoch 80/90\n",
            "Epoch 81/90\n",
            "Epoch 82/90\n",
            "Epoch 83/90\n",
            "Epoch 84/90\n",
            "Epoch 85/90\n",
            "Epoch 86/90\n",
            "Epoch 87/90\n",
            "Epoch 88/90\n",
            "Epoch 89/90\n",
            "Epoch 90/90\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/90\n",
            "Epoch 2/90\n",
            "Epoch 3/90\n",
            "Epoch 4/90\n",
            "Epoch 5/90\n",
            "Epoch 6/90\n",
            "Epoch 7/90\n",
            "Epoch 8/90\n",
            "Epoch 9/90\n",
            "Epoch 10/90\n",
            "Epoch 11/90\n",
            "Epoch 12/90\n",
            "Epoch 13/90\n",
            "Epoch 14/90\n",
            "Epoch 15/90\n",
            "Epoch 16/90\n",
            "Epoch 17/90\n",
            "Epoch 18/90\n",
            "Epoch 19/90\n",
            "Epoch 20/90\n",
            "Epoch 21/90\n",
            "Epoch 22/90\n",
            "Epoch 23/90\n",
            "Epoch 24/90\n",
            "Epoch 25/90\n",
            "Epoch 26/90\n",
            "Epoch 27/90\n",
            "Epoch 28/90\n",
            "Epoch 29/90\n",
            "Epoch 30/90\n",
            "Epoch 31/90\n",
            "Epoch 32/90\n",
            "Epoch 33/90\n",
            "Epoch 34/90\n",
            "Epoch 35/90\n",
            "Epoch 36/90\n",
            "Epoch 37/90\n",
            "Epoch 38/90\n",
            "Epoch 39/90\n",
            "Epoch 40/90\n",
            "Epoch 41/90\n",
            "Epoch 42/90\n",
            "Epoch 43/90\n",
            "Epoch 44/90\n",
            "Epoch 45/90\n",
            "Epoch 46/90\n",
            "Epoch 47/90\n",
            "Epoch 48/90\n",
            "Epoch 49/90\n",
            "Epoch 50/90\n",
            "Epoch 51/90\n",
            "Epoch 52/90\n",
            "Epoch 53/90\n",
            "Epoch 54/90\n",
            "Epoch 55/90\n",
            "Epoch 56/90\n",
            "Epoch 57/90\n",
            "Epoch 58/90\n",
            "Epoch 59/90\n",
            "Epoch 60/90\n",
            "Epoch 61/90\n",
            "Epoch 62/90\n",
            "Epoch 63/90\n",
            "Epoch 64/90\n",
            "Epoch 65/90\n",
            "Epoch 66/90\n",
            "Epoch 67/90\n",
            "Epoch 68/90\n",
            "Epoch 69/90\n",
            "Epoch 70/90\n",
            "Epoch 71/90\n",
            "Epoch 72/90\n",
            "Epoch 73/90\n",
            "Epoch 74/90\n",
            "Epoch 75/90\n",
            "Epoch 76/90\n",
            "Epoch 77/90\n",
            "Epoch 78/90\n",
            "Epoch 79/90\n",
            "Epoch 80/90\n",
            "Epoch 81/90\n",
            "Epoch 82/90\n",
            "Epoch 83/90\n",
            "Epoch 84/90\n",
            "Epoch 85/90\n",
            "Epoch 86/90\n",
            "Epoch 87/90\n",
            "Epoch 88/90\n",
            "Epoch 89/90\n",
            "Epoch 90/90\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PitR-XbfrMsD",
        "colab_type": "code",
        "outputId": "90f98ac7-8398-45d9-f8ec-93f75275ddba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "grid_result"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
              "       estimator=<keras.wrappers.scikit_learn.KerasClassifier object at 0x7fcd26116d68>,\n",
              "       fit_params=None, iid='warn', n_jobs=1,\n",
              "       param_grid={'batch_size': [10, 80], 'epochs': [20, 90]},\n",
              "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
              "       scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGzpeZWpsOlt",
        "colab_type": "code",
        "outputId": "fb1a7d10-50cb-4de8-d3b1-4f271fc4bc28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "grid_result.best_estimator_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.wrappers.scikit_learn.KerasClassifier at 0x7fcd212587b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8zOIAAcsTeW",
        "colab_type": "code",
        "outputId": "25476f24-957f-401e-a68c-c3c5c90093e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "grid_result.best_params_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': 10, 'epochs': 90}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2k6q5hkfsTwG",
        "colab_type": "code",
        "outputId": "bf1a979c-03f6-477d-f81b-db799be61015",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "grid_result.best_score_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5413223211922922"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1Yz2MzWsYbK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# terrible grid search. just using my initial defaults was better."
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}