{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_433_Keras_Lecture.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "HJzTIkYAsLxw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lets Use Libraries!\n",
        "\n",
        "The objective of the last two days has been to familiarize you with the fundamentals of neural networks: terminology, structure of networks, forward propagation, error/cost functions, backpropagation, epochs, and gradient descent. We have tried to reinforce these topics by requiring to you code some of the simplest neural networks by hand including Perceptrons (single node neural networks) and Multi-Layer Perceptrons also known as Feed-Forward Neural Networks. Continuing to do things by hand would not be the best use of our limited time. You're ready to graduate from doing things by hand and start using some powerful libraries to build cutting-edge predictive models. "
      ]
    },
    {
      "metadata": {
        "id": "MFYCXc5KdkPE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Keras\n",
        "\n",
        "> \"Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research. Use Keras if you need a deep learning library that:\n",
        "\n",
        "> Allows for easy and fast prototyping (through user friendliness, modularity, and extensibility).\n",
        "Supports both convolutional networks and recurrent networks, as well as combinations of the two.\n",
        "Runs seamlessly on CPU and GPU.\" \n",
        "\n",
        "## Installation\n",
        "\n",
        "The Keras API is particularly straightforward and it already comes pre-installed on Google Colab! \n",
        "\n",
        "<img src=\"http://www.ryanleeallred.com/wp-content/uploads/2019/04/pip-freeze-keras.png\" width=\"300\">\n",
        "\n",
        "If you're not on Google Colab you'll need to install one of the \"backend\" engines that Keras runs on top of. I recommend Tensorflow:\n",
        "\n",
        "> `pip install tensorflow`\n",
        "\n",
        "Google Colab does not have the latest Tensorflow 2.0 installation, so you'll need to upgrade to that if you want to experiment with it. However Tensorflow 2.0 was just released last month and is still in \"alpha\" so if you **really** want to use the latest and greatest be prepared for odd bugs that you don't have control over every once in a while. <https://www.tensorflow.org/install/>"
      ]
    },
    {
      "metadata": {
        "id": "QEncs0SOsFMT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Use pip freeze to see what packages/libraries your notebook has access to\n",
        "# !pip freeze"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VxgUUpIKn54a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Our First Keras Model - Perceptron, Batch epochs\n",
        "\n",
        "1) Load Data\n",
        "\n",
        "2) Define Model\n",
        "\n",
        "3) Compile Model\n",
        "\n",
        "4) Fit Model\n",
        "\n",
        "5) Evaluate Model"
      ]
    },
    {
      "metadata": {
        "id": "Md5D67XwqVAf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load Data\n",
        "\n",
        "Our life is going to be easier if our data is already cleaned up and numeric, so lets use this dataset from Jason Brownlee that is already numeric and has no column headers so we'll need to slice off the last column of data to act as our y values."
      ]
    },
    {
      "metadata": {
        "id": "bn09phMBpY1J",
        "colab_type": "code",
        "outputId": "5f04e343-bd0e-4ba5-a9db-7200f3cda629",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "cell_type": "code",
      "source": [
        "# https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-73d3845a-a10a-45d7-ab61-db440e33a73a\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-73d3845a-a10a-45d7-ab61-db440e33a73a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving pima-indians-diabetes.csv to pima-indians-diabetes.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'pima-indians-diabetes.csv': b'6,148,72,35,0,33.6,0.627,50,1\\n1,85,66,29,0,26.6,0.351,31,0\\n8,183,64,0,0,23.3,0.672,32,1\\n1,89,66,23,94,28.1,0.167,21,0\\n0,137,40,35,168,43.1,2.288,33,1\\n5,116,74,0,0,25.6,0.201,30,0\\n3,78,50,32,88,31.0,0.248,26,1\\n10,115,0,0,0,35.3,0.134,29,0\\n2,197,70,45,543,30.5,0.158,53,1\\n8,125,96,0,0,0.0,0.232,54,1\\n4,110,92,0,0,37.6,0.191,30,0\\n10,168,74,0,0,38.0,0.537,34,1\\n10,139,80,0,0,27.1,1.441,57,0\\n1,189,60,23,846,30.1,0.398,59,1\\n5,166,72,19,175,25.8,0.587,51,1\\n7,100,0,0,0,30.0,0.484,32,1\\n0,118,84,47,230,45.8,0.551,31,1\\n7,107,74,0,0,29.6,0.254,31,1\\n1,103,30,38,83,43.3,0.183,33,0\\n1,115,70,30,96,34.6,0.529,32,1\\n3,126,88,41,235,39.3,0.704,27,0\\n8,99,84,0,0,35.4,0.388,50,0\\n7,196,90,0,0,39.8,0.451,41,1\\n9,119,80,35,0,29.0,0.263,29,1\\n11,143,94,33,146,36.6,0.254,51,1\\n10,125,70,26,115,31.1,0.205,41,1\\n7,147,76,0,0,39.4,0.257,43,1\\n1,97,66,15,140,23.2,0.487,22,0\\n13,145,82,19,110,22.2,0.245,57,0\\n5,117,92,0,0,34.1,0.337,38,0\\n5,109,75,26,0,36.0,0.546,60,0\\n3,158,76,36,245,31.6,0.851,28,1\\n3,88,58,11,54,24.8,0.267,22,0\\n6,92,92,0,0,19.9,0.188,28,0\\n10,122,78,31,0,27.6,0.512,45,0\\n4,103,60,33,192,24.0,0.966,33,0\\n11,138,76,0,0,33.2,0.420,35,0\\n9,102,76,37,0,32.9,0.665,46,1\\n2,90,68,42,0,38.2,0.503,27,1\\n4,111,72,47,207,37.1,1.390,56,1\\n3,180,64,25,70,34.0,0.271,26,0\\n7,133,84,0,0,40.2,0.696,37,0\\n7,106,92,18,0,22.7,0.235,48,0\\n9,171,110,24,240,45.4,0.721,54,1\\n7,159,64,0,0,27.4,0.294,40,0\\n0,180,66,39,0,42.0,1.893,25,1\\n1,146,56,0,0,29.7,0.564,29,0\\n2,71,70,27,0,28.0,0.586,22,0\\n7,103,66,32,0,39.1,0.344,31,1\\n7,105,0,0,0,0.0,0.305,24,0\\n1,103,80,11,82,19.4,0.491,22,0\\n1,101,50,15,36,24.2,0.526,26,0\\n5,88,66,21,23,24.4,0.342,30,0\\n8,176,90,34,300,33.7,0.467,58,1\\n7,150,66,42,342,34.7,0.718,42,0\\n1,73,50,10,0,23.0,0.248,21,0\\n7,187,68,39,304,37.7,0.254,41,1\\n0,100,88,60,110,46.8,0.962,31,0\\n0,146,82,0,0,40.5,1.781,44,0\\n0,105,64,41,142,41.5,0.173,22,0\\n2,84,0,0,0,0.0,0.304,21,0\\n8,133,72,0,0,32.9,0.270,39,1\\n5,44,62,0,0,25.0,0.587,36,0\\n2,141,58,34,128,25.4,0.699,24,0\\n7,114,66,0,0,32.8,0.258,42,1\\n5,99,74,27,0,29.0,0.203,32,0\\n0,109,88,30,0,32.5,0.855,38,1\\n2,109,92,0,0,42.7,0.845,54,0\\n1,95,66,13,38,19.6,0.334,25,0\\n4,146,85,27,100,28.9,0.189,27,0\\n2,100,66,20,90,32.9,0.867,28,1\\n5,139,64,35,140,28.6,0.411,26,0\\n13,126,90,0,0,43.4,0.583,42,1\\n4,129,86,20,270,35.1,0.231,23,0\\n1,79,75,30,0,32.0,0.396,22,0\\n1,0,48,20,0,24.7,0.140,22,0\\n7,62,78,0,0,32.6,0.391,41,0\\n5,95,72,33,0,37.7,0.370,27,0\\n0,131,0,0,0,43.2,0.270,26,1\\n2,112,66,22,0,25.0,0.307,24,0\\n3,113,44,13,0,22.4,0.140,22,0\\n2,74,0,0,0,0.0,0.102,22,0\\n7,83,78,26,71,29.3,0.767,36,0\\n0,101,65,28,0,24.6,0.237,22,0\\n5,137,108,0,0,48.8,0.227,37,1\\n2,110,74,29,125,32.4,0.698,27,0\\n13,106,72,54,0,36.6,0.178,45,0\\n2,100,68,25,71,38.5,0.324,26,0\\n15,136,70,32,110,37.1,0.153,43,1\\n1,107,68,19,0,26.5,0.165,24,0\\n1,80,55,0,0,19.1,0.258,21,0\\n4,123,80,15,176,32.0,0.443,34,0\\n7,81,78,40,48,46.7,0.261,42,0\\n4,134,72,0,0,23.8,0.277,60,1\\n2,142,82,18,64,24.7,0.761,21,0\\n6,144,72,27,228,33.9,0.255,40,0\\n2,92,62,28,0,31.6,0.130,24,0\\n1,71,48,18,76,20.4,0.323,22,0\\n6,93,50,30,64,28.7,0.356,23,0\\n1,122,90,51,220,49.7,0.325,31,1\\n1,163,72,0,0,39.0,1.222,33,1\\n1,151,60,0,0,26.1,0.179,22,0\\n0,125,96,0,0,22.5,0.262,21,0\\n1,81,72,18,40,26.6,0.283,24,0\\n2,85,65,0,0,39.6,0.930,27,0\\n1,126,56,29,152,28.7,0.801,21,0\\n1,96,122,0,0,22.4,0.207,27,0\\n4,144,58,28,140,29.5,0.287,37,0\\n3,83,58,31,18,34.3,0.336,25,0\\n0,95,85,25,36,37.4,0.247,24,1\\n3,171,72,33,135,33.3,0.199,24,1\\n8,155,62,26,495,34.0,0.543,46,1\\n1,89,76,34,37,31.2,0.192,23,0\\n4,76,62,0,0,34.0,0.391,25,0\\n7,160,54,32,175,30.5,0.588,39,1\\n4,146,92,0,0,31.2,0.539,61,1\\n5,124,74,0,0,34.0,0.220,38,1\\n5,78,48,0,0,33.7,0.654,25,0\\n4,97,60,23,0,28.2,0.443,22,0\\n4,99,76,15,51,23.2,0.223,21,0\\n0,162,76,56,100,53.2,0.759,25,1\\n6,111,64,39,0,34.2,0.260,24,0\\n2,107,74,30,100,33.6,0.404,23,0\\n5,132,80,0,0,26.8,0.186,69,0\\n0,113,76,0,0,33.3,0.278,23,1\\n1,88,30,42,99,55.0,0.496,26,1\\n3,120,70,30,135,42.9,0.452,30,0\\n1,118,58,36,94,33.3,0.261,23,0\\n1,117,88,24,145,34.5,0.403,40,1\\n0,105,84,0,0,27.9,0.741,62,1\\n4,173,70,14,168,29.7,0.361,33,1\\n9,122,56,0,0,33.3,1.114,33,1\\n3,170,64,37,225,34.5,0.356,30,1\\n8,84,74,31,0,38.3,0.457,39,0\\n2,96,68,13,49,21.1,0.647,26,0\\n2,125,60,20,140,33.8,0.088,31,0\\n0,100,70,26,50,30.8,0.597,21,0\\n0,93,60,25,92,28.7,0.532,22,0\\n0,129,80,0,0,31.2,0.703,29,0\\n5,105,72,29,325,36.9,0.159,28,0\\n3,128,78,0,0,21.1,0.268,55,0\\n5,106,82,30,0,39.5,0.286,38,0\\n2,108,52,26,63,32.5,0.318,22,0\\n10,108,66,0,0,32.4,0.272,42,1\\n4,154,62,31,284,32.8,0.237,23,0\\n0,102,75,23,0,0.0,0.572,21,0\\n9,57,80,37,0,32.8,0.096,41,0\\n2,106,64,35,119,30.5,1.400,34,0\\n5,147,78,0,0,33.7,0.218,65,0\\n2,90,70,17,0,27.3,0.085,22,0\\n1,136,74,50,204,37.4,0.399,24,0\\n4,114,65,0,0,21.9,0.432,37,0\\n9,156,86,28,155,34.3,1.189,42,1\\n1,153,82,42,485,40.6,0.687,23,0\\n8,188,78,0,0,47.9,0.137,43,1\\n7,152,88,44,0,50.0,0.337,36,1\\n2,99,52,15,94,24.6,0.637,21,0\\n1,109,56,21,135,25.2,0.833,23,0\\n2,88,74,19,53,29.0,0.229,22,0\\n17,163,72,41,114,40.9,0.817,47,1\\n4,151,90,38,0,29.7,0.294,36,0\\n7,102,74,40,105,37.2,0.204,45,0\\n0,114,80,34,285,44.2,0.167,27,0\\n2,100,64,23,0,29.7,0.368,21,0\\n0,131,88,0,0,31.6,0.743,32,1\\n6,104,74,18,156,29.9,0.722,41,1\\n3,148,66,25,0,32.5,0.256,22,0\\n4,120,68,0,0,29.6,0.709,34,0\\n4,110,66,0,0,31.9,0.471,29,0\\n3,111,90,12,78,28.4,0.495,29,0\\n6,102,82,0,0,30.8,0.180,36,1\\n6,134,70,23,130,35.4,0.542,29,1\\n2,87,0,23,0,28.9,0.773,25,0\\n1,79,60,42,48,43.5,0.678,23,0\\n2,75,64,24,55,29.7,0.370,33,0\\n8,179,72,42,130,32.7,0.719,36,1\\n6,85,78,0,0,31.2,0.382,42,0\\n0,129,110,46,130,67.1,0.319,26,1\\n5,143,78,0,0,45.0,0.190,47,0\\n5,130,82,0,0,39.1,0.956,37,1\\n6,87,80,0,0,23.2,0.084,32,0\\n0,119,64,18,92,34.9,0.725,23,0\\n1,0,74,20,23,27.7,0.299,21,0\\n5,73,60,0,0,26.8,0.268,27,0\\n4,141,74,0,0,27.6,0.244,40,0\\n7,194,68,28,0,35.9,0.745,41,1\\n8,181,68,36,495,30.1,0.615,60,1\\n1,128,98,41,58,32.0,1.321,33,1\\n8,109,76,39,114,27.9,0.640,31,1\\n5,139,80,35,160,31.6,0.361,25,1\\n3,111,62,0,0,22.6,0.142,21,0\\n9,123,70,44,94,33.1,0.374,40,0\\n7,159,66,0,0,30.4,0.383,36,1\\n11,135,0,0,0,52.3,0.578,40,1\\n8,85,55,20,0,24.4,0.136,42,0\\n5,158,84,41,210,39.4,0.395,29,1\\n1,105,58,0,0,24.3,0.187,21,0\\n3,107,62,13,48,22.9,0.678,23,1\\n4,109,64,44,99,34.8,0.905,26,1\\n4,148,60,27,318,30.9,0.150,29,1\\n0,113,80,16,0,31.0,0.874,21,0\\n1,138,82,0,0,40.1,0.236,28,0\\n0,108,68,20,0,27.3,0.787,32,0\\n2,99,70,16,44,20.4,0.235,27,0\\n6,103,72,32,190,37.7,0.324,55,0\\n5,111,72,28,0,23.9,0.407,27,0\\n8,196,76,29,280,37.5,0.605,57,1\\n5,162,104,0,0,37.7,0.151,52,1\\n1,96,64,27,87,33.2,0.289,21,0\\n7,184,84,33,0,35.5,0.355,41,1\\n2,81,60,22,0,27.7,0.290,25,0\\n0,147,85,54,0,42.8,0.375,24,0\\n7,179,95,31,0,34.2,0.164,60,0\\n0,140,65,26,130,42.6,0.431,24,1\\n9,112,82,32,175,34.2,0.260,36,1\\n12,151,70,40,271,41.8,0.742,38,1\\n5,109,62,41,129,35.8,0.514,25,1\\n6,125,68,30,120,30.0,0.464,32,0\\n5,85,74,22,0,29.0,1.224,32,1\\n5,112,66,0,0,37.8,0.261,41,1\\n0,177,60,29,478,34.6,1.072,21,1\\n2,158,90,0,0,31.6,0.805,66,1\\n7,119,0,0,0,25.2,0.209,37,0\\n7,142,60,33,190,28.8,0.687,61,0\\n1,100,66,15,56,23.6,0.666,26,0\\n1,87,78,27,32,34.6,0.101,22,0\\n0,101,76,0,0,35.7,0.198,26,0\\n3,162,52,38,0,37.2,0.652,24,1\\n4,197,70,39,744,36.7,2.329,31,0\\n0,117,80,31,53,45.2,0.089,24,0\\n4,142,86,0,0,44.0,0.645,22,1\\n6,134,80,37,370,46.2,0.238,46,1\\n1,79,80,25,37,25.4,0.583,22,0\\n4,122,68,0,0,35.0,0.394,29,0\\n3,74,68,28,45,29.7,0.293,23,0\\n4,171,72,0,0,43.6,0.479,26,1\\n7,181,84,21,192,35.9,0.586,51,1\\n0,179,90,27,0,44.1,0.686,23,1\\n9,164,84,21,0,30.8,0.831,32,1\\n0,104,76,0,0,18.4,0.582,27,0\\n1,91,64,24,0,29.2,0.192,21,0\\n4,91,70,32,88,33.1,0.446,22,0\\n3,139,54,0,0,25.6,0.402,22,1\\n6,119,50,22,176,27.1,1.318,33,1\\n2,146,76,35,194,38.2,0.329,29,0\\n9,184,85,15,0,30.0,1.213,49,1\\n10,122,68,0,0,31.2,0.258,41,0\\n0,165,90,33,680,52.3,0.427,23,0\\n9,124,70,33,402,35.4,0.282,34,0\\n1,111,86,19,0,30.1,0.143,23,0\\n9,106,52,0,0,31.2,0.380,42,0\\n2,129,84,0,0,28.0,0.284,27,0\\n2,90,80,14,55,24.4,0.249,24,0\\n0,86,68,32,0,35.8,0.238,25,0\\n12,92,62,7,258,27.6,0.926,44,1\\n1,113,64,35,0,33.6,0.543,21,1\\n3,111,56,39,0,30.1,0.557,30,0\\n2,114,68,22,0,28.7,0.092,25,0\\n1,193,50,16,375,25.9,0.655,24,0\\n11,155,76,28,150,33.3,1.353,51,1\\n3,191,68,15,130,30.9,0.299,34,0\\n3,141,0,0,0,30.0,0.761,27,1\\n4,95,70,32,0,32.1,0.612,24,0\\n3,142,80,15,0,32.4,0.200,63,0\\n4,123,62,0,0,32.0,0.226,35,1\\n5,96,74,18,67,33.6,0.997,43,0\\n0,138,0,0,0,36.3,0.933,25,1\\n2,128,64,42,0,40.0,1.101,24,0\\n0,102,52,0,0,25.1,0.078,21,0\\n2,146,0,0,0,27.5,0.240,28,1\\n10,101,86,37,0,45.6,1.136,38,1\\n2,108,62,32,56,25.2,0.128,21,0\\n3,122,78,0,0,23.0,0.254,40,0\\n1,71,78,50,45,33.2,0.422,21,0\\n13,106,70,0,0,34.2,0.251,52,0\\n2,100,70,52,57,40.5,0.677,25,0\\n7,106,60,24,0,26.5,0.296,29,1\\n0,104,64,23,116,27.8,0.454,23,0\\n5,114,74,0,0,24.9,0.744,57,0\\n2,108,62,10,278,25.3,0.881,22,0\\n0,146,70,0,0,37.9,0.334,28,1\\n10,129,76,28,122,35.9,0.280,39,0\\n7,133,88,15,155,32.4,0.262,37,0\\n7,161,86,0,0,30.4,0.165,47,1\\n2,108,80,0,0,27.0,0.259,52,1\\n7,136,74,26,135,26.0,0.647,51,0\\n5,155,84,44,545,38.7,0.619,34,0\\n1,119,86,39,220,45.6,0.808,29,1\\n4,96,56,17,49,20.8,0.340,26,0\\n5,108,72,43,75,36.1,0.263,33,0\\n0,78,88,29,40,36.9,0.434,21,0\\n0,107,62,30,74,36.6,0.757,25,1\\n2,128,78,37,182,43.3,1.224,31,1\\n1,128,48,45,194,40.5,0.613,24,1\\n0,161,50,0,0,21.9,0.254,65,0\\n6,151,62,31,120,35.5,0.692,28,0\\n2,146,70,38,360,28.0,0.337,29,1\\n0,126,84,29,215,30.7,0.520,24,0\\n14,100,78,25,184,36.6,0.412,46,1\\n8,112,72,0,0,23.6,0.840,58,0\\n0,167,0,0,0,32.3,0.839,30,1\\n2,144,58,33,135,31.6,0.422,25,1\\n5,77,82,41,42,35.8,0.156,35,0\\n5,115,98,0,0,52.9,0.209,28,1\\n3,150,76,0,0,21.0,0.207,37,0\\n2,120,76,37,105,39.7,0.215,29,0\\n10,161,68,23,132,25.5,0.326,47,1\\n0,137,68,14,148,24.8,0.143,21,0\\n0,128,68,19,180,30.5,1.391,25,1\\n2,124,68,28,205,32.9,0.875,30,1\\n6,80,66,30,0,26.2,0.313,41,0\\n0,106,70,37,148,39.4,0.605,22,0\\n2,155,74,17,96,26.6,0.433,27,1\\n3,113,50,10,85,29.5,0.626,25,0\\n7,109,80,31,0,35.9,1.127,43,1\\n2,112,68,22,94,34.1,0.315,26,0\\n3,99,80,11,64,19.3,0.284,30,0\\n3,182,74,0,0,30.5,0.345,29,1\\n3,115,66,39,140,38.1,0.150,28,0\\n6,194,78,0,0,23.5,0.129,59,1\\n4,129,60,12,231,27.5,0.527,31,0\\n3,112,74,30,0,31.6,0.197,25,1\\n0,124,70,20,0,27.4,0.254,36,1\\n13,152,90,33,29,26.8,0.731,43,1\\n2,112,75,32,0,35.7,0.148,21,0\\n1,157,72,21,168,25.6,0.123,24,0\\n1,122,64,32,156,35.1,0.692,30,1\\n10,179,70,0,0,35.1,0.200,37,0\\n2,102,86,36,120,45.5,0.127,23,1\\n6,105,70,32,68,30.8,0.122,37,0\\n8,118,72,19,0,23.1,1.476,46,0\\n2,87,58,16,52,32.7,0.166,25,0\\n1,180,0,0,0,43.3,0.282,41,1\\n12,106,80,0,0,23.6,0.137,44,0\\n1,95,60,18,58,23.9,0.260,22,0\\n0,165,76,43,255,47.9,0.259,26,0\\n0,117,0,0,0,33.8,0.932,44,0\\n5,115,76,0,0,31.2,0.343,44,1\\n9,152,78,34,171,34.2,0.893,33,1\\n7,178,84,0,0,39.9,0.331,41,1\\n1,130,70,13,105,25.9,0.472,22,0\\n1,95,74,21,73,25.9,0.673,36,0\\n1,0,68,35,0,32.0,0.389,22,0\\n5,122,86,0,0,34.7,0.290,33,0\\n8,95,72,0,0,36.8,0.485,57,0\\n8,126,88,36,108,38.5,0.349,49,0\\n1,139,46,19,83,28.7,0.654,22,0\\n3,116,0,0,0,23.5,0.187,23,0\\n3,99,62,19,74,21.8,0.279,26,0\\n5,0,80,32,0,41.0,0.346,37,1\\n4,92,80,0,0,42.2,0.237,29,0\\n4,137,84,0,0,31.2,0.252,30,0\\n3,61,82,28,0,34.4,0.243,46,0\\n1,90,62,12,43,27.2,0.580,24,0\\n3,90,78,0,0,42.7,0.559,21,0\\n9,165,88,0,0,30.4,0.302,49,1\\n1,125,50,40,167,33.3,0.962,28,1\\n13,129,0,30,0,39.9,0.569,44,1\\n12,88,74,40,54,35.3,0.378,48,0\\n1,196,76,36,249,36.5,0.875,29,1\\n5,189,64,33,325,31.2,0.583,29,1\\n5,158,70,0,0,29.8,0.207,63,0\\n5,103,108,37,0,39.2,0.305,65,0\\n4,146,78,0,0,38.5,0.520,67,1\\n4,147,74,25,293,34.9,0.385,30,0\\n5,99,54,28,83,34.0,0.499,30,0\\n6,124,72,0,0,27.6,0.368,29,1\\n0,101,64,17,0,21.0,0.252,21,0\\n3,81,86,16,66,27.5,0.306,22,0\\n1,133,102,28,140,32.8,0.234,45,1\\n3,173,82,48,465,38.4,2.137,25,1\\n0,118,64,23,89,0.0,1.731,21,0\\n0,84,64,22,66,35.8,0.545,21,0\\n2,105,58,40,94,34.9,0.225,25,0\\n2,122,52,43,158,36.2,0.816,28,0\\n12,140,82,43,325,39.2,0.528,58,1\\n0,98,82,15,84,25.2,0.299,22,0\\n1,87,60,37,75,37.2,0.509,22,0\\n4,156,75,0,0,48.3,0.238,32,1\\n0,93,100,39,72,43.4,1.021,35,0\\n1,107,72,30,82,30.8,0.821,24,0\\n0,105,68,22,0,20.0,0.236,22,0\\n1,109,60,8,182,25.4,0.947,21,0\\n1,90,62,18,59,25.1,1.268,25,0\\n1,125,70,24,110,24.3,0.221,25,0\\n1,119,54,13,50,22.3,0.205,24,0\\n5,116,74,29,0,32.3,0.660,35,1\\n8,105,100,36,0,43.3,0.239,45,1\\n5,144,82,26,285,32.0,0.452,58,1\\n3,100,68,23,81,31.6,0.949,28,0\\n1,100,66,29,196,32.0,0.444,42,0\\n5,166,76,0,0,45.7,0.340,27,1\\n1,131,64,14,415,23.7,0.389,21,0\\n4,116,72,12,87,22.1,0.463,37,0\\n4,158,78,0,0,32.9,0.803,31,1\\n2,127,58,24,275,27.7,1.600,25,0\\n3,96,56,34,115,24.7,0.944,39,0\\n0,131,66,40,0,34.3,0.196,22,1\\n3,82,70,0,0,21.1,0.389,25,0\\n3,193,70,31,0,34.9,0.241,25,1\\n4,95,64,0,0,32.0,0.161,31,1\\n6,137,61,0,0,24.2,0.151,55,0\\n5,136,84,41,88,35.0,0.286,35,1\\n9,72,78,25,0,31.6,0.280,38,0\\n5,168,64,0,0,32.9,0.135,41,1\\n2,123,48,32,165,42.1,0.520,26,0\\n4,115,72,0,0,28.9,0.376,46,1\\n0,101,62,0,0,21.9,0.336,25,0\\n8,197,74,0,0,25.9,1.191,39,1\\n1,172,68,49,579,42.4,0.702,28,1\\n6,102,90,39,0,35.7,0.674,28,0\\n1,112,72,30,176,34.4,0.528,25,0\\n1,143,84,23,310,42.4,1.076,22,0\\n1,143,74,22,61,26.2,0.256,21,0\\n0,138,60,35,167,34.6,0.534,21,1\\n3,173,84,33,474,35.7,0.258,22,1\\n1,97,68,21,0,27.2,1.095,22,0\\n4,144,82,32,0,38.5,0.554,37,1\\n1,83,68,0,0,18.2,0.624,27,0\\n3,129,64,29,115,26.4,0.219,28,1\\n1,119,88,41,170,45.3,0.507,26,0\\n2,94,68,18,76,26.0,0.561,21,0\\n0,102,64,46,78,40.6,0.496,21,0\\n2,115,64,22,0,30.8,0.421,21,0\\n8,151,78,32,210,42.9,0.516,36,1\\n4,184,78,39,277,37.0,0.264,31,1\\n0,94,0,0,0,0.0,0.256,25,0\\n1,181,64,30,180,34.1,0.328,38,1\\n0,135,94,46,145,40.6,0.284,26,0\\n1,95,82,25,180,35.0,0.233,43,1\\n2,99,0,0,0,22.2,0.108,23,0\\n3,89,74,16,85,30.4,0.551,38,0\\n1,80,74,11,60,30.0,0.527,22,0\\n2,139,75,0,0,25.6,0.167,29,0\\n1,90,68,8,0,24.5,1.138,36,0\\n0,141,0,0,0,42.4,0.205,29,1\\n12,140,85,33,0,37.4,0.244,41,0\\n5,147,75,0,0,29.9,0.434,28,0\\n1,97,70,15,0,18.2,0.147,21,0\\n6,107,88,0,0,36.8,0.727,31,0\\n0,189,104,25,0,34.3,0.435,41,1\\n2,83,66,23,50,32.2,0.497,22,0\\n4,117,64,27,120,33.2,0.230,24,0\\n8,108,70,0,0,30.5,0.955,33,1\\n4,117,62,12,0,29.7,0.380,30,1\\n0,180,78,63,14,59.4,2.420,25,1\\n1,100,72,12,70,25.3,0.658,28,0\\n0,95,80,45,92,36.5,0.330,26,0\\n0,104,64,37,64,33.6,0.510,22,1\\n0,120,74,18,63,30.5,0.285,26,0\\n1,82,64,13,95,21.2,0.415,23,0\\n2,134,70,0,0,28.9,0.542,23,1\\n0,91,68,32,210,39.9,0.381,25,0\\n2,119,0,0,0,19.6,0.832,72,0\\n2,100,54,28,105,37.8,0.498,24,0\\n14,175,62,30,0,33.6,0.212,38,1\\n1,135,54,0,0,26.7,0.687,62,0\\n5,86,68,28,71,30.2,0.364,24,0\\n10,148,84,48,237,37.6,1.001,51,1\\n9,134,74,33,60,25.9,0.460,81,0\\n9,120,72,22,56,20.8,0.733,48,0\\n1,71,62,0,0,21.8,0.416,26,0\\n8,74,70,40,49,35.3,0.705,39,0\\n5,88,78,30,0,27.6,0.258,37,0\\n10,115,98,0,0,24.0,1.022,34,0\\n0,124,56,13,105,21.8,0.452,21,0\\n0,74,52,10,36,27.8,0.269,22,0\\n0,97,64,36,100,36.8,0.600,25,0\\n8,120,0,0,0,30.0,0.183,38,1\\n6,154,78,41,140,46.1,0.571,27,0\\n1,144,82,40,0,41.3,0.607,28,0\\n0,137,70,38,0,33.2,0.170,22,0\\n0,119,66,27,0,38.8,0.259,22,0\\n7,136,90,0,0,29.9,0.210,50,0\\n4,114,64,0,0,28.9,0.126,24,0\\n0,137,84,27,0,27.3,0.231,59,0\\n2,105,80,45,191,33.7,0.711,29,1\\n7,114,76,17,110,23.8,0.466,31,0\\n8,126,74,38,75,25.9,0.162,39,0\\n4,132,86,31,0,28.0,0.419,63,0\\n3,158,70,30,328,35.5,0.344,35,1\\n0,123,88,37,0,35.2,0.197,29,0\\n4,85,58,22,49,27.8,0.306,28,0\\n0,84,82,31,125,38.2,0.233,23,0\\n0,145,0,0,0,44.2,0.630,31,1\\n0,135,68,42,250,42.3,0.365,24,1\\n1,139,62,41,480,40.7,0.536,21,0\\n0,173,78,32,265,46.5,1.159,58,0\\n4,99,72,17,0,25.6,0.294,28,0\\n8,194,80,0,0,26.1,0.551,67,0\\n2,83,65,28,66,36.8,0.629,24,0\\n2,89,90,30,0,33.5,0.292,42,0\\n4,99,68,38,0,32.8,0.145,33,0\\n4,125,70,18,122,28.9,1.144,45,1\\n3,80,0,0,0,0.0,0.174,22,0\\n6,166,74,0,0,26.6,0.304,66,0\\n5,110,68,0,0,26.0,0.292,30,0\\n2,81,72,15,76,30.1,0.547,25,0\\n7,195,70,33,145,25.1,0.163,55,1\\n6,154,74,32,193,29.3,0.839,39,0\\n2,117,90,19,71,25.2,0.313,21,0\\n3,84,72,32,0,37.2,0.267,28,0\\n6,0,68,41,0,39.0,0.727,41,1\\n7,94,64,25,79,33.3,0.738,41,0\\n3,96,78,39,0,37.3,0.238,40,0\\n10,75,82,0,0,33.3,0.263,38,0\\n0,180,90,26,90,36.5,0.314,35,1\\n1,130,60,23,170,28.6,0.692,21,0\\n2,84,50,23,76,30.4,0.968,21,0\\n8,120,78,0,0,25.0,0.409,64,0\\n12,84,72,31,0,29.7,0.297,46,1\\n0,139,62,17,210,22.1,0.207,21,0\\n9,91,68,0,0,24.2,0.200,58,0\\n2,91,62,0,0,27.3,0.525,22,0\\n3,99,54,19,86,25.6,0.154,24,0\\n3,163,70,18,105,31.6,0.268,28,1\\n9,145,88,34,165,30.3,0.771,53,1\\n7,125,86,0,0,37.6,0.304,51,0\\n13,76,60,0,0,32.8,0.180,41,0\\n6,129,90,7,326,19.6,0.582,60,0\\n2,68,70,32,66,25.0,0.187,25,0\\n3,124,80,33,130,33.2,0.305,26,0\\n6,114,0,0,0,0.0,0.189,26,0\\n9,130,70,0,0,34.2,0.652,45,1\\n3,125,58,0,0,31.6,0.151,24,0\\n3,87,60,18,0,21.8,0.444,21,0\\n1,97,64,19,82,18.2,0.299,21,0\\n3,116,74,15,105,26.3,0.107,24,0\\n0,117,66,31,188,30.8,0.493,22,0\\n0,111,65,0,0,24.6,0.660,31,0\\n2,122,60,18,106,29.8,0.717,22,0\\n0,107,76,0,0,45.3,0.686,24,0\\n1,86,66,52,65,41.3,0.917,29,0\\n6,91,0,0,0,29.8,0.501,31,0\\n1,77,56,30,56,33.3,1.251,24,0\\n4,132,0,0,0,32.9,0.302,23,1\\n0,105,90,0,0,29.6,0.197,46,0\\n0,57,60,0,0,21.7,0.735,67,0\\n0,127,80,37,210,36.3,0.804,23,0\\n3,129,92,49,155,36.4,0.968,32,1\\n8,100,74,40,215,39.4,0.661,43,1\\n3,128,72,25,190,32.4,0.549,27,1\\n10,90,85,32,0,34.9,0.825,56,1\\n4,84,90,23,56,39.5,0.159,25,0\\n1,88,78,29,76,32.0,0.365,29,0\\n8,186,90,35,225,34.5,0.423,37,1\\n5,187,76,27,207,43.6,1.034,53,1\\n4,131,68,21,166,33.1,0.160,28,0\\n1,164,82,43,67,32.8,0.341,50,0\\n4,189,110,31,0,28.5,0.680,37,0\\n1,116,70,28,0,27.4,0.204,21,0\\n3,84,68,30,106,31.9,0.591,25,0\\n6,114,88,0,0,27.8,0.247,66,0\\n1,88,62,24,44,29.9,0.422,23,0\\n1,84,64,23,115,36.9,0.471,28,0\\n7,124,70,33,215,25.5,0.161,37,0\\n1,97,70,40,0,38.1,0.218,30,0\\n8,110,76,0,0,27.8,0.237,58,0\\n11,103,68,40,0,46.2,0.126,42,0\\n11,85,74,0,0,30.1,0.300,35,0\\n6,125,76,0,0,33.8,0.121,54,1\\n0,198,66,32,274,41.3,0.502,28,1\\n1,87,68,34,77,37.6,0.401,24,0\\n6,99,60,19,54,26.9,0.497,32,0\\n0,91,80,0,0,32.4,0.601,27,0\\n2,95,54,14,88,26.1,0.748,22,0\\n1,99,72,30,18,38.6,0.412,21,0\\n6,92,62,32,126,32.0,0.085,46,0\\n4,154,72,29,126,31.3,0.338,37,0\\n0,121,66,30,165,34.3,0.203,33,1\\n3,78,70,0,0,32.5,0.270,39,0\\n2,130,96,0,0,22.6,0.268,21,0\\n3,111,58,31,44,29.5,0.430,22,0\\n2,98,60,17,120,34.7,0.198,22,0\\n1,143,86,30,330,30.1,0.892,23,0\\n1,119,44,47,63,35.5,0.280,25,0\\n6,108,44,20,130,24.0,0.813,35,0\\n2,118,80,0,0,42.9,0.693,21,1\\n10,133,68,0,0,27.0,0.245,36,0\\n2,197,70,99,0,34.7,0.575,62,1\\n0,151,90,46,0,42.1,0.371,21,1\\n6,109,60,27,0,25.0,0.206,27,0\\n12,121,78,17,0,26.5,0.259,62,0\\n8,100,76,0,0,38.7,0.190,42,0\\n8,124,76,24,600,28.7,0.687,52,1\\n1,93,56,11,0,22.5,0.417,22,0\\n8,143,66,0,0,34.9,0.129,41,1\\n6,103,66,0,0,24.3,0.249,29,0\\n3,176,86,27,156,33.3,1.154,52,1\\n0,73,0,0,0,21.1,0.342,25,0\\n11,111,84,40,0,46.8,0.925,45,1\\n2,112,78,50,140,39.4,0.175,24,0\\n3,132,80,0,0,34.4,0.402,44,1\\n2,82,52,22,115,28.5,1.699,25,0\\n6,123,72,45,230,33.6,0.733,34,0\\n0,188,82,14,185,32.0,0.682,22,1\\n0,67,76,0,0,45.3,0.194,46,0\\n1,89,24,19,25,27.8,0.559,21,0\\n1,173,74,0,0,36.8,0.088,38,1\\n1,109,38,18,120,23.1,0.407,26,0\\n1,108,88,19,0,27.1,0.400,24,0\\n6,96,0,0,0,23.7,0.190,28,0\\n1,124,74,36,0,27.8,0.100,30,0\\n7,150,78,29,126,35.2,0.692,54,1\\n4,183,0,0,0,28.4,0.212,36,1\\n1,124,60,32,0,35.8,0.514,21,0\\n1,181,78,42,293,40.0,1.258,22,1\\n1,92,62,25,41,19.5,0.482,25,0\\n0,152,82,39,272,41.5,0.270,27,0\\n1,111,62,13,182,24.0,0.138,23,0\\n3,106,54,21,158,30.9,0.292,24,0\\n3,174,58,22,194,32.9,0.593,36,1\\n7,168,88,42,321,38.2,0.787,40,1\\n6,105,80,28,0,32.5,0.878,26,0\\n11,138,74,26,144,36.1,0.557,50,1\\n3,106,72,0,0,25.8,0.207,27,0\\n6,117,96,0,0,28.7,0.157,30,0\\n2,68,62,13,15,20.1,0.257,23,0\\n9,112,82,24,0,28.2,1.282,50,1\\n0,119,0,0,0,32.4,0.141,24,1\\n2,112,86,42,160,38.4,0.246,28,0\\n2,92,76,20,0,24.2,1.698,28,0\\n6,183,94,0,0,40.8,1.461,45,0\\n0,94,70,27,115,43.5,0.347,21,0\\n2,108,64,0,0,30.8,0.158,21,0\\n4,90,88,47,54,37.7,0.362,29,0\\n0,125,68,0,0,24.7,0.206,21,0\\n0,132,78,0,0,32.4,0.393,21,0\\n5,128,80,0,0,34.6,0.144,45,0\\n4,94,65,22,0,24.7,0.148,21,0\\n7,114,64,0,0,27.4,0.732,34,1\\n0,102,78,40,90,34.5,0.238,24,0\\n2,111,60,0,0,26.2,0.343,23,0\\n1,128,82,17,183,27.5,0.115,22,0\\n10,92,62,0,0,25.9,0.167,31,0\\n13,104,72,0,0,31.2,0.465,38,1\\n5,104,74,0,0,28.8,0.153,48,0\\n2,94,76,18,66,31.6,0.649,23,0\\n7,97,76,32,91,40.9,0.871,32,1\\n1,100,74,12,46,19.5,0.149,28,0\\n0,102,86,17,105,29.3,0.695,27,0\\n4,128,70,0,0,34.3,0.303,24,0\\n6,147,80,0,0,29.5,0.178,50,1\\n4,90,0,0,0,28.0,0.610,31,0\\n3,103,72,30,152,27.6,0.730,27,0\\n2,157,74,35,440,39.4,0.134,30,0\\n1,167,74,17,144,23.4,0.447,33,1\\n0,179,50,36,159,37.8,0.455,22,1\\n11,136,84,35,130,28.3,0.260,42,1\\n0,107,60,25,0,26.4,0.133,23,0\\n1,91,54,25,100,25.2,0.234,23,0\\n1,117,60,23,106,33.8,0.466,27,0\\n5,123,74,40,77,34.1,0.269,28,0\\n2,120,54,0,0,26.8,0.455,27,0\\n1,106,70,28,135,34.2,0.142,22,0\\n2,155,52,27,540,38.7,0.240,25,1\\n2,101,58,35,90,21.8,0.155,22,0\\n1,120,80,48,200,38.9,1.162,41,0\\n11,127,106,0,0,39.0,0.190,51,0\\n3,80,82,31,70,34.2,1.292,27,1\\n10,162,84,0,0,27.7,0.182,54,0\\n1,199,76,43,0,42.9,1.394,22,1\\n8,167,106,46,231,37.6,0.165,43,1\\n9,145,80,46,130,37.9,0.637,40,1\\n6,115,60,39,0,33.7,0.245,40,1\\n1,112,80,45,132,34.8,0.217,24,0\\n4,145,82,18,0,32.5,0.235,70,1\\n10,111,70,27,0,27.5,0.141,40,1\\n6,98,58,33,190,34.0,0.430,43,0\\n9,154,78,30,100,30.9,0.164,45,0\\n6,165,68,26,168,33.6,0.631,49,0\\n1,99,58,10,0,25.4,0.551,21,0\\n10,68,106,23,49,35.5,0.285,47,0\\n3,123,100,35,240,57.3,0.880,22,0\\n8,91,82,0,0,35.6,0.587,68,0\\n6,195,70,0,0,30.9,0.328,31,1\\n9,156,86,0,0,24.8,0.230,53,1\\n0,93,60,0,0,35.3,0.263,25,0\\n3,121,52,0,0,36.0,0.127,25,1\\n2,101,58,17,265,24.2,0.614,23,0\\n2,56,56,28,45,24.2,0.332,22,0\\n0,162,76,36,0,49.6,0.364,26,1\\n0,95,64,39,105,44.6,0.366,22,0\\n4,125,80,0,0,32.3,0.536,27,1\\n5,136,82,0,0,0.0,0.640,69,0\\n2,129,74,26,205,33.2,0.591,25,0\\n3,130,64,0,0,23.1,0.314,22,0\\n1,107,50,19,0,28.3,0.181,29,0\\n1,140,74,26,180,24.1,0.828,23,0\\n1,144,82,46,180,46.1,0.335,46,1\\n8,107,80,0,0,24.6,0.856,34,0\\n13,158,114,0,0,42.3,0.257,44,1\\n2,121,70,32,95,39.1,0.886,23,0\\n7,129,68,49,125,38.5,0.439,43,1\\n2,90,60,0,0,23.5,0.191,25,0\\n7,142,90,24,480,30.4,0.128,43,1\\n3,169,74,19,125,29.9,0.268,31,1\\n0,99,0,0,0,25.0,0.253,22,0\\n4,127,88,11,155,34.5,0.598,28,0\\n4,118,70,0,0,44.5,0.904,26,0\\n2,122,76,27,200,35.9,0.483,26,0\\n6,125,78,31,0,27.6,0.565,49,1\\n1,168,88,29,0,35.0,0.905,52,1\\n2,129,0,0,0,38.5,0.304,41,0\\n4,110,76,20,100,28.4,0.118,27,0\\n6,80,80,36,0,39.8,0.177,28,0\\n10,115,0,0,0,0.0,0.261,30,1\\n2,127,46,21,335,34.4,0.176,22,0\\n9,164,78,0,0,32.8,0.148,45,1\\n2,93,64,32,160,38.0,0.674,23,1\\n3,158,64,13,387,31.2,0.295,24,0\\n5,126,78,27,22,29.6,0.439,40,0\\n10,129,62,36,0,41.2,0.441,38,1\\n0,134,58,20,291,26.4,0.352,21,0\\n3,102,74,0,0,29.5,0.121,32,0\\n7,187,50,33,392,33.9,0.826,34,1\\n3,173,78,39,185,33.8,0.970,31,1\\n10,94,72,18,0,23.1,0.595,56,0\\n1,108,60,46,178,35.5,0.415,24,0\\n5,97,76,27,0,35.6,0.378,52,1\\n4,83,86,19,0,29.3,0.317,34,0\\n1,114,66,36,200,38.1,0.289,21,0\\n1,149,68,29,127,29.3,0.349,42,1\\n5,117,86,30,105,39.1,0.251,42,0\\n1,111,94,0,0,32.8,0.265,45,0\\n4,112,78,40,0,39.4,0.236,38,0\\n1,116,78,29,180,36.1,0.496,25,0\\n0,141,84,26,0,32.4,0.433,22,0\\n2,175,88,0,0,22.9,0.326,22,0\\n2,92,52,0,0,30.1,0.141,22,0\\n3,130,78,23,79,28.4,0.323,34,1\\n8,120,86,0,0,28.4,0.259,22,1\\n2,174,88,37,120,44.5,0.646,24,1\\n2,106,56,27,165,29.0,0.426,22,0\\n2,105,75,0,0,23.3,0.560,53,0\\n4,95,60,32,0,35.4,0.284,28,0\\n0,126,86,27,120,27.4,0.515,21,0\\n8,65,72,23,0,32.0,0.600,42,0\\n2,99,60,17,160,36.6,0.453,21,0\\n1,102,74,0,0,39.5,0.293,42,1\\n11,120,80,37,150,42.3,0.785,48,1\\n3,102,44,20,94,30.8,0.400,26,0\\n1,109,58,18,116,28.5,0.219,22,0\\n9,140,94,0,0,32.7,0.734,45,1\\n13,153,88,37,140,40.6,1.174,39,0\\n12,100,84,33,105,30.0,0.488,46,0\\n1,147,94,41,0,49.3,0.358,27,1\\n1,81,74,41,57,46.3,1.096,32,0\\n3,187,70,22,200,36.4,0.408,36,1\\n6,162,62,0,0,24.3,0.178,50,1\\n4,136,70,0,0,31.2,1.182,22,1\\n1,121,78,39,74,39.0,0.261,28,0\\n3,108,62,24,0,26.0,0.223,25,0\\n0,181,88,44,510,43.3,0.222,26,1\\n8,154,78,32,0,32.4,0.443,45,1\\n1,128,88,39,110,36.5,1.057,37,1\\n7,137,90,41,0,32.0,0.391,39,0\\n0,123,72,0,0,36.3,0.258,52,1\\n1,106,76,0,0,37.5,0.197,26,0\\n6,190,92,0,0,35.5,0.278,66,1\\n2,88,58,26,16,28.4,0.766,22,0\\n9,170,74,31,0,44.0,0.403,43,1\\n9,89,62,0,0,22.5,0.142,33,0\\n10,101,76,48,180,32.9,0.171,63,0\\n2,122,70,27,0,36.8,0.340,27,0\\n5,121,72,23,112,26.2,0.245,30,0\\n1,126,60,0,0,30.1,0.349,47,1\\n1,93,70,31,0,30.4,0.315,23,0'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "FKuofD3Pogil",
        "colab_type": "code",
        "outputId": "c1384c74-9979-4b5a-d0a9-6be841d0e999",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 725
        }
      },
      "cell_type": "code",
      "source": [
        "# Load Pima Indians Dataset\n",
        "dataset = numpy.loadtxt('pima-indians-diabetes.csv', delimiter=',')\n",
        "# Split into X and Y variables\n",
        "X = dataset[:,0:8]\n",
        "print(X.shape)\n",
        "print(X)\n",
        "Y = dataset[:,-1]\n",
        "print(Y.shape)\n",
        "print(Y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(768, 8)\n",
            "[[  6.    148.     72.    ...  33.6     0.627  50.   ]\n",
            " [  1.     85.     66.    ...  26.6     0.351  31.   ]\n",
            " [  8.    183.     64.    ...  23.3     0.672  32.   ]\n",
            " ...\n",
            " [  5.    121.     72.    ...  26.2     0.245  30.   ]\n",
            " [  1.    126.     60.    ...  30.1     0.349  47.   ]\n",
            " [  1.     93.     70.    ...  30.4     0.315  23.   ]]\n",
            "(768,)\n",
            "[1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0.\n",
            " 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0.\n",
            " 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.\n",
            " 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0.\n",
            " 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0.\n",
            " 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1.\n",
            " 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0.\n",
            " 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0.\n",
            " 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.\n",
            " 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0.\n",
            " 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
            " 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1.\n",
            " 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.\n",
            " 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0.\n",
            " 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0.\n",
            " 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.\n",
            " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.\n",
            " 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
            " 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0.\n",
            " 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1.\n",
            " 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.\n",
            " 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1.\n",
            " 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.\n",
            " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1.\n",
            " 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "o0xMqOyTs5xt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define Model"
      ]
    },
    {
      "metadata": {
        "id": "Bp9USczrfu6M",
        "colab_type": "code",
        "outputId": "5a9003ce-bd6e-49fb-b9ed-0b84a35730d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import numpy\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "wAzHLg27thoN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I'll instantiate my model as a \"sequential\" model. This just means that I'm going to tell Keras what my model's architecture should be one layer at a time."
      ]
    },
    {
      "metadata": {
        "id": "DSNsL49Xp6KI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# https://keras.io/getting-started/sequential-model-guide/\n",
        "model = Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZCYX6QzJtvpG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Adding a \"Dense\" layer to our model is how we add \"vanilla\" perceptron-based layers to our neural network. These are also called \"fully-connected\" or \"densely-connected\" layers. They're used as a layer type in lots of other Neural Net Architectures but they're not referred to as perceptrons or multi-layer perceptrons very often in those situations even though that's what they are.\n",
        "\n",
        " > [\"Just your regular densely-connected NN layer.\"](https://keras.io/layers/core/)\n",
        " \n",
        " The first argument is how many neurons we want to have in that layer. To create a perceptron model we will just set it to 1. We will tell it that there will be 8 inputs coming into this layer from our dataset and set it to use the sigmoid activation function."
      ]
    },
    {
      "metadata": {
        "id": "GNzOLidxtvFa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "78e7a8b7-bf12-4323-ebd7-82c20702059b"
      },
      "cell_type": "code",
      "source": [
        "model.add(Dense(1, input_dim=8, activation=\"sigmoid\"))\n",
        "\n",
        "# The above is the same as adding these separately:\n",
        "# model = Sequential()\n",
        "# model.add(Dense(1, input_dim=8))\n",
        "# model.add(Activation('sigmoid'))\n",
        "\n",
        "# If you then want to add more layers to your model:\n",
        "# with 8 layers and activation function 'relu'\n",
        "model.add(Dense(8, input_dim=8, activation=\"relu\"))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-aeb6598e5125>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# The above is the same as doing:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "EnI3jwKMtBL2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Compile Model\n",
        "Using binary_crossentropy as the loss function here is just telling keras that I'm doing binary classification so that it can use the appropriate loss function accordingly. If we were predicting non-binary categories we might assign something like `categorical_crossentropy`. We're also telling keras that we want it to report model accuracy as our main error metric for each epoch. We will also be able to see the overall accuracy once the model has finished training."
      ]
    },
    {
      "metadata": {
        "id": "qp6xwYaqurRO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 'binary_crossentropy': tells Keras that we want to predict 'binary' classes as losses are calculated (reporting loss sensible for binary classification)\n",
        "# while -> 'categorical_crossentropy': for categorical classification\n",
        "# for list of optimizers: http://keras.io/optimizers/\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SiyGzebJFmJ6",
        "colab_type": "code",
        "outputId": "2a93bf29-97e3-49b7-994b-9dddccb19a4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "cell_type": "code",
      "source": [
        "# Param #: number of parameters\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_24 (Dense)             (None, 1)                 9         \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 1)                 2         \n",
            "=================================================================\n",
            "Total params: 11\n",
            "Trainable params: 11\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5dW8SZ2Ls9SX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Fit Model\n",
        "\n",
        "Lets train it up! `model.fit()` has a `batch_size` parameter that we can use if we want to do mini-batch epochs, but since this tabular dataset is pretty small we're just going to delete that parameter. Keras' default `batch_size` is `None` so omiting it will tell Keras to do batch epochs. "
      ]
    },
    {
      "metadata": {
        "id": "nJxdmX_-u5MJ",
        "colab_type": "code",
        "outputId": "0e376bad-8951-4fe1-adc4-62808ee529b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5199
        }
      },
      "cell_type": "code",
      "source": [
        "# Each of the 'epochs' represents 1 feed-forward and 1 backpropagation train cycle\n",
        "# where each of the weights are updated 1 time in each epoch\n",
        "model.fit(X, Y, epochs=150)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "768/768 [==============================] - 1s 1ms/step - loss: 0.7018 - acc: 0.4245\n",
            "Epoch 2/150\n",
            "768/768 [==============================] - 0s 42us/step - loss: 0.6880 - acc: 0.6758\n",
            "Epoch 3/150\n",
            "768/768 [==============================] - 0s 43us/step - loss: 0.6848 - acc: 0.6836\n",
            "Epoch 4/150\n",
            "768/768 [==============================] - 0s 41us/step - loss: 0.6811 - acc: 0.6836\n",
            "Epoch 5/150\n",
            "768/768 [==============================] - 0s 43us/step - loss: 0.6775 - acc: 0.6888\n",
            "Epoch 6/150\n",
            "768/768 [==============================] - 0s 41us/step - loss: 0.6731 - acc: 0.6966\n",
            "Epoch 7/150\n",
            "768/768 [==============================] - 0s 42us/step - loss: 0.6690 - acc: 0.7057\n",
            "Epoch 8/150\n",
            "768/768 [==============================] - 0s 42us/step - loss: 0.6641 - acc: 0.6966\n",
            "Epoch 9/150\n",
            "768/768 [==============================] - 0s 43us/step - loss: 0.6615 - acc: 0.7044\n",
            "Epoch 10/150\n",
            "768/768 [==============================] - 0s 42us/step - loss: 0.6567 - acc: 0.7214\n",
            "Epoch 11/150\n",
            "768/768 [==============================] - 0s 43us/step - loss: 0.6524 - acc: 0.7227\n",
            "Epoch 12/150\n",
            "768/768 [==============================] - 0s 41us/step - loss: 0.6484 - acc: 0.7305\n",
            "Epoch 13/150\n",
            "768/768 [==============================] - 0s 41us/step - loss: 0.6460 - acc: 0.7253\n",
            "Epoch 14/150\n",
            "768/768 [==============================] - 0s 43us/step - loss: 0.6413 - acc: 0.7331\n",
            "Epoch 15/150\n",
            "768/768 [==============================] - 0s 41us/step - loss: 0.6409 - acc: 0.7227\n",
            "Epoch 16/150\n",
            "768/768 [==============================] - 0s 44us/step - loss: 0.6362 - acc: 0.7344\n",
            "Epoch 17/150\n",
            "768/768 [==============================] - 0s 42us/step - loss: 0.6351 - acc: 0.7370\n",
            "Epoch 18/150\n",
            "768/768 [==============================] - 0s 41us/step - loss: 0.6320 - acc: 0.7279\n",
            "Epoch 19/150\n",
            "768/768 [==============================] - 0s 43us/step - loss: 0.6276 - acc: 0.7370\n",
            "Epoch 20/150\n",
            "768/768 [==============================] - 0s 44us/step - loss: 0.6252 - acc: 0.7422\n",
            "Epoch 21/150\n",
            "768/768 [==============================] - 0s 44us/step - loss: 0.6237 - acc: 0.7370\n",
            "Epoch 22/150\n",
            "768/768 [==============================] - 0s 42us/step - loss: 0.6216 - acc: 0.7383\n",
            "Epoch 23/150\n",
            "768/768 [==============================] - 0s 40us/step - loss: 0.6196 - acc: 0.7305\n",
            "Epoch 24/150\n",
            "768/768 [==============================] - 0s 40us/step - loss: 0.6171 - acc: 0.7331\n",
            "Epoch 25/150\n",
            "768/768 [==============================] - 0s 40us/step - loss: 0.6148 - acc: 0.7409\n",
            "Epoch 26/150\n",
            "768/768 [==============================] - 0s 41us/step - loss: 0.6148 - acc: 0.7396\n",
            "Epoch 27/150\n",
            "768/768 [==============================] - 0s 45us/step - loss: 0.6110 - acc: 0.7422\n",
            "Epoch 28/150\n",
            "768/768 [==============================] - 0s 39us/step - loss: 0.6131 - acc: 0.7292\n",
            "Epoch 29/150\n",
            "768/768 [==============================] - 0s 48us/step - loss: 0.6081 - acc: 0.7409\n",
            "Epoch 30/150\n",
            "768/768 [==============================] - 0s 41us/step - loss: 0.6094 - acc: 0.7201\n",
            "Epoch 31/150\n",
            "768/768 [==============================] - 0s 40us/step - loss: 0.6048 - acc: 0.7357\n",
            "Epoch 32/150\n",
            "768/768 [==============================] - 0s 41us/step - loss: 0.6075 - acc: 0.7318\n",
            "Epoch 33/150\n",
            "768/768 [==============================] - 0s 44us/step - loss: 0.6033 - acc: 0.7240\n",
            "Epoch 34/150\n",
            "768/768 [==============================] - 0s 43us/step - loss: 0.6004 - acc: 0.7409\n",
            "Epoch 35/150\n",
            "768/768 [==============================] - 0s 40us/step - loss: 0.5967 - acc: 0.7435\n",
            "Epoch 36/150\n",
            "768/768 [==============================] - 0s 42us/step - loss: 0.5965 - acc: 0.7474\n",
            "Epoch 37/150\n",
            "768/768 [==============================] - 0s 45us/step - loss: 0.5973 - acc: 0.7253\n",
            "Epoch 38/150\n",
            "768/768 [==============================] - 0s 41us/step - loss: 0.5929 - acc: 0.7461\n",
            "Epoch 39/150\n",
            "768/768 [==============================] - 0s 42us/step - loss: 0.5918 - acc: 0.7474\n",
            "Epoch 40/150\n",
            "768/768 [==============================] - 0s 44us/step - loss: 0.5915 - acc: 0.7422\n",
            "Epoch 41/150\n",
            "768/768 [==============================] - 0s 40us/step - loss: 0.5897 - acc: 0.7383\n",
            "Epoch 42/150\n",
            "768/768 [==============================] - 0s 43us/step - loss: 0.5894 - acc: 0.7487\n",
            "Epoch 43/150\n",
            "768/768 [==============================] - 0s 42us/step - loss: 0.5864 - acc: 0.7422\n",
            "Epoch 44/150\n",
            "768/768 [==============================] - 0s 51us/step - loss: 0.5859 - acc: 0.7357\n",
            "Epoch 45/150\n",
            "768/768 [==============================] - 0s 48us/step - loss: 0.5841 - acc: 0.7513\n",
            "Epoch 46/150\n",
            "768/768 [==============================] - 0s 41us/step - loss: 0.5844 - acc: 0.7448\n",
            "Epoch 47/150\n",
            "768/768 [==============================] - 0s 41us/step - loss: 0.5838 - acc: 0.7331\n",
            "Epoch 48/150\n",
            "768/768 [==============================] - 0s 41us/step - loss: 0.5805 - acc: 0.7461\n",
            "Epoch 49/150\n",
            "768/768 [==============================] - 0s 54us/step - loss: 0.5810 - acc: 0.7448\n",
            "Epoch 50/150\n",
            "768/768 [==============================] - 0s 42us/step - loss: 0.5790 - acc: 0.7448\n",
            "Epoch 51/150\n",
            "768/768 [==============================] - 0s 40us/step - loss: 0.5787 - acc: 0.7500\n",
            "Epoch 52/150\n",
            "768/768 [==============================] - 0s 40us/step - loss: 0.5804 - acc: 0.7331\n",
            "Epoch 53/150\n",
            "768/768 [==============================] - 0s 42us/step - loss: 0.5756 - acc: 0.7500\n",
            "Epoch 54/150\n",
            "768/768 [==============================] - 0s 40us/step - loss: 0.5748 - acc: 0.7513\n",
            "Epoch 55/150\n",
            "768/768 [==============================] - 0s 43us/step - loss: 0.5739 - acc: 0.7539\n",
            "Epoch 56/150\n",
            "768/768 [==============================] - 0s 50us/step - loss: 0.5760 - acc: 0.7448\n",
            "Epoch 57/150\n",
            "768/768 [==============================] - 0s 43us/step - loss: 0.5758 - acc: 0.7383\n",
            "Epoch 58/150\n",
            "768/768 [==============================] - 0s 42us/step - loss: 0.5709 - acc: 0.7474\n",
            "Epoch 59/150\n",
            "768/768 [==============================] - 0s 48us/step - loss: 0.5718 - acc: 0.7500\n",
            "Epoch 60/150\n",
            "768/768 [==============================] - 0s 44us/step - loss: 0.5697 - acc: 0.7500\n",
            "Epoch 61/150\n",
            "768/768 [==============================] - 0s 43us/step - loss: 0.5699 - acc: 0.7539\n",
            "Epoch 62/150\n",
            "768/768 [==============================] - 0s 42us/step - loss: 0.5691 - acc: 0.7539\n",
            "Epoch 63/150\n",
            "768/768 [==============================] - 0s 47us/step - loss: 0.5677 - acc: 0.7526\n",
            "Epoch 64/150\n",
            "768/768 [==============================] - 0s 41us/step - loss: 0.5679 - acc: 0.7461\n",
            "Epoch 65/150\n",
            "768/768 [==============================] - 0s 45us/step - loss: 0.5667 - acc: 0.7487\n",
            "Epoch 66/150\n",
            "768/768 [==============================] - 0s 43us/step - loss: 0.5665 - acc: 0.7526\n",
            "Epoch 67/150\n",
            "768/768 [==============================] - 0s 43us/step - loss: 0.5655 - acc: 0.7500\n",
            "Epoch 68/150\n",
            "768/768 [==============================] - 0s 43us/step - loss: 0.5642 - acc: 0.7461\n",
            "Epoch 69/150\n",
            "768/768 [==============================] - 0s 42us/step - loss: 0.5653 - acc: 0.7552\n",
            "Epoch 70/150\n",
            "768/768 [==============================] - 0s 40us/step - loss: 0.5685 - acc: 0.7435\n",
            "Epoch 71/150\n",
            "768/768 [==============================] - 0s 44us/step - loss: 0.5629 - acc: 0.7409\n",
            "Epoch 72/150\n",
            "768/768 [==============================] - 0s 43us/step - loss: 0.5615 - acc: 0.7578\n",
            "Epoch 73/150\n",
            "768/768 [==============================] - 0s 44us/step - loss: 0.5606 - acc: 0.7513\n",
            "Epoch 74/150\n",
            "768/768 [==============================] - 0s 48us/step - loss: 0.5702 - acc: 0.7396\n",
            "Epoch 75/150\n",
            "768/768 [==============================] - 0s 41us/step - loss: 0.5631 - acc: 0.7552\n",
            "Epoch 76/150\n",
            "768/768 [==============================] - 0s 41us/step - loss: 0.5595 - acc: 0.7500\n",
            "Epoch 77/150\n",
            "768/768 [==============================] - 0s 43us/step - loss: 0.5593 - acc: 0.7552\n",
            "Epoch 78/150\n",
            "768/768 [==============================] - 0s 43us/step - loss: 0.5582 - acc: 0.7565\n",
            "Epoch 79/150\n",
            "768/768 [==============================] - 0s 40us/step - loss: 0.5569 - acc: 0.7539\n",
            "Epoch 80/150\n",
            "768/768 [==============================] - 0s 42us/step - loss: 0.5569 - acc: 0.7669\n",
            "Epoch 81/150\n",
            "768/768 [==============================] - 0s 44us/step - loss: 0.5574 - acc: 0.7565\n",
            "Epoch 82/150\n",
            "768/768 [==============================] - 0s 41us/step - loss: 0.5566 - acc: 0.7565\n",
            "Epoch 83/150\n",
            "768/768 [==============================] - 0s 43us/step - loss: 0.5576 - acc: 0.7500\n",
            "Epoch 84/150\n",
            "768/768 [==============================] - 0s 45us/step - loss: 0.5560 - acc: 0.7591\n",
            "Epoch 85/150\n",
            "768/768 [==============================] - 0s 42us/step - loss: 0.5542 - acc: 0.7539\n",
            "Epoch 86/150\n",
            "768/768 [==============================] - 0s 41us/step - loss: 0.5578 - acc: 0.7526\n",
            "Epoch 87/150\n",
            "768/768 [==============================] - 0s 42us/step - loss: 0.5523 - acc: 0.7526\n",
            "Epoch 88/150\n",
            "768/768 [==============================] - 0s 42us/step - loss: 0.5531 - acc: 0.7461\n",
            "Epoch 89/150\n",
            "768/768 [==============================] - 0s 43us/step - loss: 0.5513 - acc: 0.7448\n",
            "Epoch 90/150\n",
            "768/768 [==============================] - 0s 43us/step - loss: 0.5527 - acc: 0.7552\n",
            "Epoch 91/150\n",
            "768/768 [==============================] - 0s 42us/step - loss: 0.5546 - acc: 0.7487\n",
            "Epoch 92/150\n",
            "768/768 [==============================] - 0s 43us/step - loss: 0.5540 - acc: 0.7474\n",
            "Epoch 93/150\n",
            "768/768 [==============================] - 0s 41us/step - loss: 0.5502 - acc: 0.7552\n",
            "Epoch 94/150\n",
            "768/768 [==============================] - 0s 43us/step - loss: 0.5483 - acc: 0.7630\n",
            "Epoch 95/150\n",
            "768/768 [==============================] - 0s 41us/step - loss: 0.5502 - acc: 0.7578\n",
            "Epoch 96/150\n",
            "768/768 [==============================] - 0s 43us/step - loss: 0.5480 - acc: 0.7617\n",
            "Epoch 97/150\n",
            "768/768 [==============================] - 0s 44us/step - loss: 0.5495 - acc: 0.7487\n",
            "Epoch 98/150\n",
            "768/768 [==============================] - 0s 41us/step - loss: 0.5481 - acc: 0.7487\n",
            "Epoch 99/150\n",
            "768/768 [==============================] - 0s 41us/step - loss: 0.5484 - acc: 0.7630\n",
            "Epoch 100/150\n",
            "768/768 [==============================] - 0s 45us/step - loss: 0.5493 - acc: 0.7539\n",
            "Epoch 101/150\n",
            "768/768 [==============================] - 0s 43us/step - loss: 0.5472 - acc: 0.7474\n",
            "Epoch 102/150\n",
            "768/768 [==============================] - 0s 57us/step - loss: 0.5511 - acc: 0.7435\n",
            "Epoch 103/150\n",
            "768/768 [==============================] - 0s 44us/step - loss: 0.5460 - acc: 0.7500\n",
            "Epoch 104/150\n",
            "768/768 [==============================] - 0s 42us/step - loss: 0.5445 - acc: 0.7500\n",
            "Epoch 105/150\n",
            "768/768 [==============================] - 0s 45us/step - loss: 0.5522 - acc: 0.7331\n",
            "Epoch 106/150\n",
            "768/768 [==============================] - 0s 45us/step - loss: 0.5481 - acc: 0.7461\n",
            "Epoch 107/150\n",
            "768/768 [==============================] - 0s 48us/step - loss: 0.5443 - acc: 0.7565\n",
            "Epoch 108/150\n",
            "768/768 [==============================] - 0s 41us/step - loss: 0.5443 - acc: 0.7578\n",
            "Epoch 109/150\n",
            "768/768 [==============================] - 0s 40us/step - loss: 0.5428 - acc: 0.7617\n",
            "Epoch 110/150\n",
            "768/768 [==============================] - 0s 41us/step - loss: 0.5437 - acc: 0.7539\n",
            "Epoch 111/150\n",
            "768/768 [==============================] - 0s 41us/step - loss: 0.5464 - acc: 0.7474\n",
            "Epoch 112/150\n",
            "768/768 [==============================] - 0s 41us/step - loss: 0.5476 - acc: 0.7487\n",
            "Epoch 113/150\n",
            "768/768 [==============================] - 0s 44us/step - loss: 0.5434 - acc: 0.7474\n",
            "Epoch 114/150\n",
            "768/768 [==============================] - 0s 39us/step - loss: 0.5400 - acc: 0.7630\n",
            "Epoch 115/150\n",
            "768/768 [==============================] - 0s 42us/step - loss: 0.5400 - acc: 0.7591\n",
            "Epoch 116/150\n",
            "768/768 [==============================] - 0s 41us/step - loss: 0.5419 - acc: 0.7513\n",
            "Epoch 117/150\n",
            "768/768 [==============================] - 0s 43us/step - loss: 0.5411 - acc: 0.7513\n",
            "Epoch 118/150\n",
            "768/768 [==============================] - 0s 41us/step - loss: 0.5399 - acc: 0.7552\n",
            "Epoch 119/150\n",
            "768/768 [==============================] - 0s 42us/step - loss: 0.5407 - acc: 0.7500\n",
            "Epoch 120/150\n",
            "768/768 [==============================] - 0s 42us/step - loss: 0.5499 - acc: 0.7396\n",
            "Epoch 121/150\n",
            "768/768 [==============================] - 0s 41us/step - loss: 0.5406 - acc: 0.7513\n",
            "Epoch 122/150\n",
            "768/768 [==============================] - 0s 46us/step - loss: 0.5373 - acc: 0.7552\n",
            "Epoch 123/150\n",
            "768/768 [==============================] - 0s 46us/step - loss: 0.5407 - acc: 0.7578\n",
            "Epoch 124/150\n",
            "768/768 [==============================] - 0s 45us/step - loss: 0.5376 - acc: 0.7565\n",
            "Epoch 125/150\n",
            "768/768 [==============================] - 0s 43us/step - loss: 0.5357 - acc: 0.7565\n",
            "Epoch 126/150\n",
            "768/768 [==============================] - 0s 41us/step - loss: 0.5355 - acc: 0.7578\n",
            "Epoch 127/150\n",
            "768/768 [==============================] - 0s 42us/step - loss: 0.5400 - acc: 0.7526\n",
            "Epoch 128/150\n",
            "768/768 [==============================] - 0s 42us/step - loss: 0.5371 - acc: 0.7513\n",
            "Epoch 129/150\n",
            "768/768 [==============================] - 0s 45us/step - loss: 0.5344 - acc: 0.7552\n",
            "Epoch 130/150\n",
            "768/768 [==============================] - 0s 45us/step - loss: 0.5374 - acc: 0.7461\n",
            "Epoch 131/150\n",
            "768/768 [==============================] - 0s 43us/step - loss: 0.5367 - acc: 0.7539\n",
            "Epoch 132/150\n",
            "768/768 [==============================] - 0s 44us/step - loss: 0.5318 - acc: 0.7513\n",
            "Epoch 133/150\n",
            "768/768 [==============================] - 0s 46us/step - loss: 0.5374 - acc: 0.7526\n",
            "Epoch 134/150\n",
            "768/768 [==============================] - 0s 44us/step - loss: 0.5362 - acc: 0.7643\n",
            "Epoch 135/150\n",
            "768/768 [==============================] - 0s 44us/step - loss: 0.5357 - acc: 0.7487\n",
            "Epoch 136/150\n",
            "768/768 [==============================] - 0s 41us/step - loss: 0.5339 - acc: 0.7513\n",
            "Epoch 137/150\n",
            "768/768 [==============================] - 0s 43us/step - loss: 0.5356 - acc: 0.7578\n",
            "Epoch 138/150\n",
            "768/768 [==============================] - 0s 40us/step - loss: 0.5336 - acc: 0.7552\n",
            "Epoch 139/150\n",
            "768/768 [==============================] - 0s 42us/step - loss: 0.5278 - acc: 0.7539\n",
            "Epoch 140/150\n",
            "768/768 [==============================] - 0s 48us/step - loss: 0.5348 - acc: 0.7552\n",
            "Epoch 141/150\n",
            "768/768 [==============================] - 0s 42us/step - loss: 0.5299 - acc: 0.7695\n",
            "Epoch 142/150\n",
            "768/768 [==============================] - 0s 45us/step - loss: 0.5342 - acc: 0.7487\n",
            "Epoch 143/150\n",
            "768/768 [==============================] - 0s 44us/step - loss: 0.5331 - acc: 0.7539\n",
            "Epoch 144/150\n",
            "768/768 [==============================] - 0s 43us/step - loss: 0.5374 - acc: 0.7565\n",
            "Epoch 145/150\n",
            "768/768 [==============================] - 0s 44us/step - loss: 0.5297 - acc: 0.7604\n",
            "Epoch 146/150\n",
            "768/768 [==============================] - 0s 40us/step - loss: 0.5296 - acc: 0.7500\n",
            "Epoch 147/150\n",
            "768/768 [==============================] - 0s 45us/step - loss: 0.5295 - acc: 0.7669\n",
            "Epoch 148/150\n",
            "768/768 [==============================] - 0s 43us/step - loss: 0.5341 - acc: 0.7487\n",
            "Epoch 149/150\n",
            "768/768 [==============================] - 0s 40us/step - loss: 0.5260 - acc: 0.7617\n",
            "Epoch 150/150\n",
            "768/768 [==============================] - 0s 40us/step - loss: 0.5263 - acc: 0.7643\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BSybwWEJtGFm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Evaluate Model"
      ]
    },
    {
      "metadata": {
        "id": "34wh8z9MvFMp",
        "colab_type": "code",
        "outputId": "50c72690-412a-4ed5-ccb6-c8d2bc382eaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "# After you fit your model you can print a final score using 'model.evaluate()'\n",
        "scores = model.evaluate(X, Y)\n",
        "print(f\"{model.metrics_names[1]}: {scores[1]*100}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "768/768 [==============================] - 0s 458us/step\n",
            "acc: 75.78125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Istg3HxnDGVy",
        "colab_type": "code",
        "outputId": "c1d397cc-a16b-41df-feda-eda927de7ec9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "cell_type": "code",
      "source": [
        "# to get history to create graphs of how loss or accuracy changed overtime (keras.io/callbacks/)\n",
        "import matplotlib.pyplot as plt\n",
        "training_loss = history.history['loss']\n",
        "epoch_count = range(1, len(training_loss) + 1)\n",
        "\n",
        "plt.plot(epoch_count, training_loss, 'r--')\n",
        "plt.show();"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAFKCAYAAAAwrQetAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmjiN+bZ08ccVAIuAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "tuOA7d4IGbG_",
        "colab_type": "code",
        "outputId": "3200f834-944c-464a-e5bd-82876c0ffba4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "cell_type": "code",
      "source": [
        "training_accuracy = history.history['acc']\n",
        "epoch_count = range(1, len(training_accuracy) + 1)\n",
        "\n",
        "plt.plot(epoch_count, training_accuracy, 'r--')\n",
        "plt.show();"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFKCAYAAAAnj5dkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "C724P0eFFf2f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# dir(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AIJoRBxHy27n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Keras Perceptron Model in 4 lines of code:"
      ]
    },
    {
      "metadata": {
        "id": "TQxyONqKvFxB",
        "colab_type": "code",
        "outputId": "12966e66-2297-4f82-85b3-c275a9c38563",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5216
        }
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(1, input_dim=8, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X, Y, epochs=150)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "768/768 [==============================] - 0s 415us/step - loss: 7.9936 - acc: 0.3984\n",
            "Epoch 2/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 7.5918 - acc: 0.4336\n",
            "Epoch 3/150\n",
            "768/768 [==============================] - 0s 40us/step - loss: 7.0022 - acc: 0.4661\n",
            "Epoch 4/150\n",
            "768/768 [==============================] - 0s 42us/step - loss: 6.2710 - acc: 0.4922\n",
            "Epoch 5/150\n",
            "768/768 [==============================] - 0s 42us/step - loss: 5.6661 - acc: 0.5469\n",
            "Epoch 6/150\n",
            "768/768 [==============================] - 0s 40us/step - loss: 5.2267 - acc: 0.5768\n",
            "Epoch 7/150\n",
            "768/768 [==============================] - 0s 46us/step - loss: 4.8732 - acc: 0.6055\n",
            "Epoch 8/150\n",
            "768/768 [==============================] - 0s 47us/step - loss: 4.6400 - acc: 0.6250\n",
            "Epoch 9/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 4.5347 - acc: 0.6172\n",
            "Epoch 10/150\n",
            "768/768 [==============================] - 0s 41us/step - loss: 4.4936 - acc: 0.6159\n",
            "Epoch 11/150\n",
            "768/768 [==============================] - 0s 43us/step - loss: 4.4815 - acc: 0.6198\n",
            "Epoch 12/150\n",
            "768/768 [==============================] - 0s 35us/step - loss: 4.4625 - acc: 0.6185\n",
            "Epoch 13/150\n",
            "768/768 [==============================] - 0s 44us/step - loss: 4.4444 - acc: 0.6237\n",
            "Epoch 14/150\n",
            "768/768 [==============================] - 0s 39us/step - loss: 4.4206 - acc: 0.6224\n",
            "Epoch 15/150\n",
            "768/768 [==============================] - 0s 48us/step - loss: 4.4001 - acc: 0.6211\n",
            "Epoch 16/150\n",
            "768/768 [==============================] - 0s 41us/step - loss: 4.3810 - acc: 0.6211\n",
            "Epoch 17/150\n",
            "768/768 [==============================] - 0s 35us/step - loss: 4.3679 - acc: 0.6198\n",
            "Epoch 18/150\n",
            "768/768 [==============================] - 0s 40us/step - loss: 4.3424 - acc: 0.6211\n",
            "Epoch 19/150\n",
            "768/768 [==============================] - 0s 38us/step - loss: 4.3205 - acc: 0.6198\n",
            "Epoch 20/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 4.3036 - acc: 0.6250\n",
            "Epoch 21/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 4.2856 - acc: 0.6185\n",
            "Epoch 22/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 4.2640 - acc: 0.6237\n",
            "Epoch 23/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 4.2337 - acc: 0.6198\n",
            "Epoch 24/150\n",
            "768/768 [==============================] - 0s 38us/step - loss: 4.2074 - acc: 0.6172\n",
            "Epoch 25/150\n",
            "768/768 [==============================] - 0s 39us/step - loss: 4.1836 - acc: 0.6172\n",
            "Epoch 26/150\n",
            "768/768 [==============================] - 0s 40us/step - loss: 4.1549 - acc: 0.6198\n",
            "Epoch 27/150\n",
            "768/768 [==============================] - 0s 39us/step - loss: 4.1246 - acc: 0.6172\n",
            "Epoch 28/150\n",
            "768/768 [==============================] - 0s 38us/step - loss: 4.0928 - acc: 0.6172\n",
            "Epoch 29/150\n",
            "768/768 [==============================] - 0s 48us/step - loss: 4.0710 - acc: 0.6198\n",
            "Epoch 30/150\n",
            "768/768 [==============================] - 0s 38us/step - loss: 4.0325 - acc: 0.6159\n",
            "Epoch 31/150\n",
            "768/768 [==============================] - 0s 46us/step - loss: 3.9979 - acc: 0.6159\n",
            "Epoch 32/150\n",
            "768/768 [==============================] - 0s 41us/step - loss: 3.9632 - acc: 0.6172\n",
            "Epoch 33/150\n",
            "768/768 [==============================] - 0s 44us/step - loss: 3.9280 - acc: 0.6146\n",
            "Epoch 34/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 3.8915 - acc: 0.6133\n",
            "Epoch 35/150\n",
            "768/768 [==============================] - 0s 49us/step - loss: 3.8547 - acc: 0.6146\n",
            "Epoch 36/150\n",
            "768/768 [==============================] - 0s 35us/step - loss: 3.8311 - acc: 0.6120\n",
            "Epoch 37/150\n",
            "768/768 [==============================] - 0s 39us/step - loss: 3.7842 - acc: 0.6094\n",
            "Epoch 38/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 3.7484 - acc: 0.6068\n",
            "Epoch 39/150\n",
            "768/768 [==============================] - 0s 46us/step - loss: 3.7032 - acc: 0.6120\n",
            "Epoch 40/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 3.6651 - acc: 0.6016\n",
            "Epoch 41/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 3.6011 - acc: 0.6068\n",
            "Epoch 42/150\n",
            "768/768 [==============================] - 0s 41us/step - loss: 3.5087 - acc: 0.5990\n",
            "Epoch 43/150\n",
            "768/768 [==============================] - 0s 41us/step - loss: 3.1149 - acc: 0.5898\n",
            "Epoch 44/150\n",
            "768/768 [==============================] - 0s 41us/step - loss: 1.9003 - acc: 0.5000\n",
            "Epoch 45/150\n",
            "768/768 [==============================] - 0s 40us/step - loss: 1.6671 - acc: 0.5456\n",
            "Epoch 46/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 1.5055 - acc: 0.5365\n",
            "Epoch 47/150\n",
            "768/768 [==============================] - 0s 45us/step - loss: 1.4111 - acc: 0.5273\n",
            "Epoch 48/150\n",
            "768/768 [==============================] - 0s 41us/step - loss: 1.3060 - acc: 0.5273\n",
            "Epoch 49/150\n",
            "768/768 [==============================] - 0s 42us/step - loss: 1.2272 - acc: 0.5534\n",
            "Epoch 50/150\n",
            "768/768 [==============================] - 0s 46us/step - loss: 1.1318 - acc: 0.5508\n",
            "Epoch 51/150\n",
            "768/768 [==============================] - 0s 41us/step - loss: 1.0748 - acc: 0.5352\n",
            "Epoch 52/150\n",
            "768/768 [==============================] - 0s 45us/step - loss: 1.0015 - acc: 0.5716\n",
            "Epoch 53/150\n",
            "768/768 [==============================] - 0s 42us/step - loss: 0.9331 - acc: 0.5846\n",
            "Epoch 54/150\n",
            "768/768 [==============================] - 0s 39us/step - loss: 0.8887 - acc: 0.5742\n",
            "Epoch 55/150\n",
            "768/768 [==============================] - 0s 47us/step - loss: 0.8251 - acc: 0.5742\n",
            "Epoch 56/150\n",
            "768/768 [==============================] - 0s 39us/step - loss: 0.8036 - acc: 0.5742\n",
            "Epoch 57/150\n",
            "768/768 [==============================] - 0s 40us/step - loss: 0.7534 - acc: 0.6380\n",
            "Epoch 58/150\n",
            "768/768 [==============================] - 0s 52us/step - loss: 0.7141 - acc: 0.6250\n",
            "Epoch 59/150\n",
            "768/768 [==============================] - 0s 43us/step - loss: 0.6884 - acc: 0.6341\n",
            "Epoch 60/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 0.6751 - acc: 0.6589\n",
            "Epoch 61/150\n",
            "768/768 [==============================] - 0s 43us/step - loss: 0.6690 - acc: 0.6354\n",
            "Epoch 62/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 0.6484 - acc: 0.6510\n",
            "Epoch 63/150\n",
            "768/768 [==============================] - 0s 38us/step - loss: 0.6198 - acc: 0.6706\n",
            "Epoch 64/150\n",
            "768/768 [==============================] - 0s 38us/step - loss: 0.6216 - acc: 0.6745\n",
            "Epoch 65/150\n",
            "768/768 [==============================] - 0s 40us/step - loss: 0.6152 - acc: 0.6797\n",
            "Epoch 66/150\n",
            "768/768 [==============================] - 0s 35us/step - loss: 0.6067 - acc: 0.6966\n",
            "Epoch 67/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 0.6047 - acc: 0.6992\n",
            "Epoch 68/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 0.6014 - acc: 0.6914\n",
            "Epoch 69/150\n",
            "768/768 [==============================] - 0s 40us/step - loss: 0.5973 - acc: 0.7005\n",
            "Epoch 70/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 0.6043 - acc: 0.6836\n",
            "Epoch 71/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 0.6004 - acc: 0.6927\n",
            "Epoch 72/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 0.6068 - acc: 0.6940\n",
            "Epoch 73/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 0.6009 - acc: 0.6966\n",
            "Epoch 74/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 0.6024 - acc: 0.7018\n",
            "Epoch 75/150\n",
            "768/768 [==============================] - 0s 35us/step - loss: 0.5960 - acc: 0.6992\n",
            "Epoch 76/150\n",
            "768/768 [==============================] - 0s 39us/step - loss: 0.5901 - acc: 0.7122\n",
            "Epoch 77/150\n",
            "768/768 [==============================] - 0s 35us/step - loss: 0.5886 - acc: 0.7122\n",
            "Epoch 78/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 0.5982 - acc: 0.6901\n",
            "Epoch 79/150\n",
            "768/768 [==============================] - 0s 35us/step - loss: 0.5982 - acc: 0.6927\n",
            "Epoch 80/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 0.5958 - acc: 0.6953\n",
            "Epoch 81/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 0.5897 - acc: 0.6992\n",
            "Epoch 82/150\n",
            "768/768 [==============================] - 0s 39us/step - loss: 0.5950 - acc: 0.6914\n",
            "Epoch 83/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 0.5900 - acc: 0.6979\n",
            "Epoch 84/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 0.5920 - acc: 0.7070\n",
            "Epoch 85/150\n",
            "768/768 [==============================] - 0s 39us/step - loss: 0.5918 - acc: 0.7083\n",
            "Epoch 86/150\n",
            "768/768 [==============================] - 0s 38us/step - loss: 0.5975 - acc: 0.6862\n",
            "Epoch 87/150\n",
            "768/768 [==============================] - 0s 34us/step - loss: 0.5958 - acc: 0.6940\n",
            "Epoch 88/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 0.5956 - acc: 0.6992\n",
            "Epoch 89/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 0.5896 - acc: 0.7018\n",
            "Epoch 90/150\n",
            "768/768 [==============================] - 0s 42us/step - loss: 0.5966 - acc: 0.6953\n",
            "Epoch 91/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 0.5935 - acc: 0.7044\n",
            "Epoch 92/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 0.5916 - acc: 0.6914\n",
            "Epoch 93/150\n",
            "768/768 [==============================] - 0s 38us/step - loss: 0.5946 - acc: 0.6927\n",
            "Epoch 94/150\n",
            "768/768 [==============================] - 0s 34us/step - loss: 0.5908 - acc: 0.7031\n",
            "Epoch 95/150\n",
            "768/768 [==============================] - 0s 38us/step - loss: 0.5902 - acc: 0.7018\n",
            "Epoch 96/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 0.5945 - acc: 0.6888\n",
            "Epoch 97/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 0.5887 - acc: 0.6953\n",
            "Epoch 98/150\n",
            "768/768 [==============================] - 0s 38us/step - loss: 0.5925 - acc: 0.6966\n",
            "Epoch 99/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 0.5892 - acc: 0.7005\n",
            "Epoch 100/150\n",
            "768/768 [==============================] - 0s 42us/step - loss: 0.5852 - acc: 0.7174\n",
            "Epoch 101/150\n",
            "768/768 [==============================] - 0s 38us/step - loss: 0.5855 - acc: 0.7057\n",
            "Epoch 102/150\n",
            "768/768 [==============================] - 0s 39us/step - loss: 0.5819 - acc: 0.7148\n",
            "Epoch 103/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 0.5877 - acc: 0.7044\n",
            "Epoch 104/150\n",
            "768/768 [==============================] - 0s 38us/step - loss: 0.5876 - acc: 0.7057\n",
            "Epoch 105/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 0.5877 - acc: 0.7057\n",
            "Epoch 106/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 0.5863 - acc: 0.7044\n",
            "Epoch 107/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 0.5829 - acc: 0.7031\n",
            "Epoch 108/150\n",
            "768/768 [==============================] - 0s 35us/step - loss: 0.5846 - acc: 0.7057\n",
            "Epoch 109/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 0.5824 - acc: 0.7083\n",
            "Epoch 110/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 0.5824 - acc: 0.7070\n",
            "Epoch 111/150\n",
            "768/768 [==============================] - 0s 38us/step - loss: 0.5845 - acc: 0.7122\n",
            "Epoch 112/150\n",
            "768/768 [==============================] - 0s 35us/step - loss: 0.5855 - acc: 0.7214\n",
            "Epoch 113/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 0.5834 - acc: 0.7031\n",
            "Epoch 114/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 0.5819 - acc: 0.7188\n",
            "Epoch 115/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 0.5810 - acc: 0.7083\n",
            "Epoch 116/150\n",
            "768/768 [==============================] - 0s 38us/step - loss: 0.5840 - acc: 0.7135\n",
            "Epoch 117/150\n",
            "768/768 [==============================] - 0s 38us/step - loss: 0.5779 - acc: 0.7227\n",
            "Epoch 118/150\n",
            "768/768 [==============================] - 0s 39us/step - loss: 0.5810 - acc: 0.7083\n",
            "Epoch 119/150\n",
            "768/768 [==============================] - 0s 38us/step - loss: 0.5887 - acc: 0.6836\n",
            "Epoch 120/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 0.5787 - acc: 0.7096\n",
            "Epoch 121/150\n",
            "768/768 [==============================] - 0s 35us/step - loss: 0.5818 - acc: 0.7044\n",
            "Epoch 122/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 0.5823 - acc: 0.7044\n",
            "Epoch 123/150\n",
            "768/768 [==============================] - 0s 38us/step - loss: 0.5807 - acc: 0.7122\n",
            "Epoch 124/150\n",
            "768/768 [==============================] - 0s 42us/step - loss: 0.5778 - acc: 0.7148\n",
            "Epoch 125/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 0.5839 - acc: 0.7214\n",
            "Epoch 126/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 0.5800 - acc: 0.7096\n",
            "Epoch 127/150\n",
            "768/768 [==============================] - 0s 35us/step - loss: 0.5751 - acc: 0.7201\n",
            "Epoch 128/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 0.5785 - acc: 0.7109\n",
            "Epoch 129/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 0.5770 - acc: 0.7161\n",
            "Epoch 130/150\n",
            "768/768 [==============================] - 0s 35us/step - loss: 0.5844 - acc: 0.7109\n",
            "Epoch 131/150\n",
            "768/768 [==============================] - 0s 38us/step - loss: 0.5795 - acc: 0.7083\n",
            "Epoch 132/150\n",
            "768/768 [==============================] - 0s 39us/step - loss: 0.5812 - acc: 0.7070\n",
            "Epoch 133/150\n",
            "768/768 [==============================] - 0s 35us/step - loss: 0.5795 - acc: 0.7057\n",
            "Epoch 134/150\n",
            "768/768 [==============================] - 0s 40us/step - loss: 0.5757 - acc: 0.7148\n",
            "Epoch 135/150\n",
            "768/768 [==============================] - 0s 38us/step - loss: 0.5755 - acc: 0.7253\n",
            "Epoch 136/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 0.5762 - acc: 0.7135\n",
            "Epoch 137/150\n",
            "768/768 [==============================] - 0s 35us/step - loss: 0.5772 - acc: 0.7161\n",
            "Epoch 138/150\n",
            "768/768 [==============================] - 0s 35us/step - loss: 0.5764 - acc: 0.7096\n",
            "Epoch 139/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 0.5776 - acc: 0.7135\n",
            "Epoch 140/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 0.5713 - acc: 0.7253\n",
            "Epoch 141/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 0.5797 - acc: 0.7096\n",
            "Epoch 142/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 0.5749 - acc: 0.7174\n",
            "Epoch 143/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 0.5801 - acc: 0.7096\n",
            "Epoch 144/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 0.5764 - acc: 0.7109\n",
            "Epoch 145/150\n",
            "768/768 [==============================] - 0s 37us/step - loss: 0.5801 - acc: 0.7135\n",
            "Epoch 146/150\n",
            "768/768 [==============================] - 0s 35us/step - loss: 0.5798 - acc: 0.7070\n",
            "Epoch 147/150\n",
            "768/768 [==============================] - 0s 35us/step - loss: 0.5834 - acc: 0.7188\n",
            "Epoch 148/150\n",
            "768/768 [==============================] - 0s 39us/step - loss: 0.5844 - acc: 0.7070\n",
            "Epoch 149/150\n",
            "768/768 [==============================] - 0s 35us/step - loss: 0.5716 - acc: 0.7201\n",
            "Epoch 150/150\n",
            "768/768 [==============================] - 0s 36us/step - loss: 0.5747 - acc: 0.7161\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f17fe0860f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "metadata": {
        "id": "Z1wfKUxszPKa",
        "colab_type": "code",
        "outputId": "0cdacd1d-6e5a-4bbe-fabb-568cd94724be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "# evaluate the model\n",
        "scores = model.evaluate(X, Y)\n",
        "print(f\"{model.metrics_names[1]}: {scores[1]*100}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "768/768 [==============================] - 0s 134us/step\n",
            "\n",
            "acc: 73.83%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zHYB7k9q3O8T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Why are we getting such different results if we re-run the model?\n",
        "\n",
        "<https://machinelearningmastery.com/randomness-in-machine-learning/>"
      ]
    },
    {
      "metadata": {
        "id": "ueDVpctAzvy8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# What architecture should we try?"
      ]
    },
    {
      "metadata": {
        "id": "6W2Sc7-LzQo_",
        "colab_type": "code",
        "outputId": "0eb252ed-1d2a-4a3e-951d-3e8a8115e1e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        }
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "# 'input_dim': only needed in first layer!\n",
        "model.add(Dense(3, input_dim=8, activation='relu'))\n",
        "model.add(Dense(2, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_27 (Dense)             (None, 3)                 27        \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 2)                 8         \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 1)                 3         \n",
            "=================================================================\n",
            "Total params: 38\n",
            "Trainable params: 38\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OCE8ElpmJORy",
        "colab_type": "code",
        "outputId": "8bfb481d-028c-424c-a951-618e29a0840e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1762
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit(X, Y, epochs=50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "768/768 [==============================] - 0s 49us/step - loss: 0.6499 - acc: 0.6576\n",
            "Epoch 2/50\n",
            "768/768 [==============================] - 0s 41us/step - loss: 0.6486 - acc: 0.6589\n",
            "Epoch 3/50\n",
            "768/768 [==============================] - 0s 40us/step - loss: 0.6471 - acc: 0.6576\n",
            "Epoch 4/50\n",
            "768/768 [==============================] - 0s 41us/step - loss: 0.6458 - acc: 0.6589\n",
            "Epoch 5/50\n",
            "768/768 [==============================] - 0s 42us/step - loss: 0.6446 - acc: 0.6602\n",
            "Epoch 6/50\n",
            "768/768 [==============================] - 0s 40us/step - loss: 0.6434 - acc: 0.6641\n",
            "Epoch 7/50\n",
            "768/768 [==============================] - 0s 41us/step - loss: 0.6423 - acc: 0.6628\n",
            "Epoch 8/50\n",
            "768/768 [==============================] - 0s 48us/step - loss: 0.6411 - acc: 0.6602\n",
            "Epoch 9/50\n",
            "768/768 [==============================] - 0s 42us/step - loss: 0.6401 - acc: 0.6628\n",
            "Epoch 10/50\n",
            "768/768 [==============================] - 0s 43us/step - loss: 0.6390 - acc: 0.6628\n",
            "Epoch 11/50\n",
            "768/768 [==============================] - 0s 41us/step - loss: 0.6379 - acc: 0.6628\n",
            "Epoch 12/50\n",
            "768/768 [==============================] - 0s 42us/step - loss: 0.6370 - acc: 0.6706\n",
            "Epoch 13/50\n",
            "768/768 [==============================] - 0s 42us/step - loss: 0.6359 - acc: 0.6706\n",
            "Epoch 14/50\n",
            "768/768 [==============================] - 0s 41us/step - loss: 0.6350 - acc: 0.6680\n",
            "Epoch 15/50\n",
            "768/768 [==============================] - 0s 40us/step - loss: 0.6339 - acc: 0.6719\n",
            "Epoch 16/50\n",
            "768/768 [==============================] - 0s 40us/step - loss: 0.6331 - acc: 0.6745\n",
            "Epoch 17/50\n",
            "768/768 [==============================] - 0s 44us/step - loss: 0.6321 - acc: 0.6745\n",
            "Epoch 18/50\n",
            "768/768 [==============================] - 0s 42us/step - loss: 0.6312 - acc: 0.6719\n",
            "Epoch 19/50\n",
            "768/768 [==============================] - 0s 44us/step - loss: 0.6303 - acc: 0.6719\n",
            "Epoch 20/50\n",
            "768/768 [==============================] - 0s 41us/step - loss: 0.6294 - acc: 0.6732\n",
            "Epoch 21/50\n",
            "768/768 [==============================] - 0s 44us/step - loss: 0.6286 - acc: 0.6732\n",
            "Epoch 22/50\n",
            "768/768 [==============================] - 0s 45us/step - loss: 0.6280 - acc: 0.6719\n",
            "Epoch 23/50\n",
            "768/768 [==============================] - 0s 47us/step - loss: 0.6269 - acc: 0.6693\n",
            "Epoch 24/50\n",
            "768/768 [==============================] - 0s 45us/step - loss: 0.6262 - acc: 0.6693\n",
            "Epoch 25/50\n",
            "768/768 [==============================] - 0s 45us/step - loss: 0.6253 - acc: 0.6719\n",
            "Epoch 26/50\n",
            "768/768 [==============================] - 0s 44us/step - loss: 0.6245 - acc: 0.6719\n",
            "Epoch 27/50\n",
            "768/768 [==============================] - 0s 47us/step - loss: 0.6237 - acc: 0.6706\n",
            "Epoch 28/50\n",
            "768/768 [==============================] - 0s 45us/step - loss: 0.6229 - acc: 0.6719\n",
            "Epoch 29/50\n",
            "768/768 [==============================] - 0s 42us/step - loss: 0.6221 - acc: 0.6719\n",
            "Epoch 30/50\n",
            "768/768 [==============================] - 0s 48us/step - loss: 0.6213 - acc: 0.6719\n",
            "Epoch 31/50\n",
            "768/768 [==============================] - 0s 45us/step - loss: 0.6206 - acc: 0.6732\n",
            "Epoch 32/50\n",
            "768/768 [==============================] - 0s 44us/step - loss: 0.6198 - acc: 0.6745\n",
            "Epoch 33/50\n",
            "768/768 [==============================] - 0s 51us/step - loss: 0.6190 - acc: 0.6719\n",
            "Epoch 34/50\n",
            "768/768 [==============================] - 0s 45us/step - loss: 0.6185 - acc: 0.6667\n",
            "Epoch 35/50\n",
            "768/768 [==============================] - 0s 43us/step - loss: 0.6175 - acc: 0.6719\n",
            "Epoch 36/50\n",
            "768/768 [==============================] - 0s 46us/step - loss: 0.6167 - acc: 0.6758\n",
            "Epoch 37/50\n",
            "768/768 [==============================] - 0s 44us/step - loss: 0.6160 - acc: 0.6771\n",
            "Epoch 38/50\n",
            "768/768 [==============================] - 0s 45us/step - loss: 0.6152 - acc: 0.6797\n",
            "Epoch 39/50\n",
            "768/768 [==============================] - 0s 44us/step - loss: 0.6144 - acc: 0.6784\n",
            "Epoch 40/50\n",
            "768/768 [==============================] - 0s 42us/step - loss: 0.6139 - acc: 0.6797\n",
            "Epoch 41/50\n",
            "768/768 [==============================] - 0s 68us/step - loss: 0.6131 - acc: 0.6784\n",
            "Epoch 42/50\n",
            "768/768 [==============================] - 0s 45us/step - loss: 0.6124 - acc: 0.6849\n",
            "Epoch 43/50\n",
            "768/768 [==============================] - 0s 52us/step - loss: 0.6116 - acc: 0.6862\n",
            "Epoch 44/50\n",
            "768/768 [==============================] - 0s 46us/step - loss: 0.6110 - acc: 0.6836\n",
            "Epoch 45/50\n",
            "768/768 [==============================] - 0s 46us/step - loss: 0.6103 - acc: 0.6836\n",
            "Epoch 46/50\n",
            "768/768 [==============================] - 0s 45us/step - loss: 0.6096 - acc: 0.6849\n",
            "Epoch 47/50\n",
            "768/768 [==============================] - 0s 46us/step - loss: 0.6089 - acc: 0.6888\n",
            "Epoch 48/50\n",
            "768/768 [==============================] - 0s 44us/step - loss: 0.6080 - acc: 0.6849\n",
            "Epoch 49/50\n",
            "768/768 [==============================] - 0s 49us/step - loss: 0.6076 - acc: 0.6875\n",
            "Epoch 50/50\n",
            "768/768 [==============================] - 0s 51us/step - loss: 0.6072 - acc: 0.6823\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb5c37b6208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "hFlIa7aQKItX",
        "colab_type": "code",
        "outputId": "1e14423c-993c-43ef-c699-d0c996ad73b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "# evaluate the model\n",
        "scores = model.evaluate(X, Y)\n",
        "print(f\"{model.metrics_names[1]}: {scores[1]*100}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "768/768 [==============================] - 0s 37us/step\n",
            "acc: 68.61979166666666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tcjMuxtn6wIQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Activation Functions\n",
        "\n",
        "What is an activation function and how does it work?\n",
        "\n",
        "- Takes in a weighted sum of inputs + a bias from the previous layer and outputs an \"activation\" value.\n",
        "- Based its inputs the neuron decides how 'activated' it should be. This can be thought of as the neuron deciding how strongly to fire. You can also think of it as if the neuron is deciding how much of the signal that it has received to pass onto the next layer. \n",
        "- Our choice of activation function does not only affect signal that is passed forward but also affects the backpropagation algorithm. It affects how we update weights in reverse order since activated weight/input sums become the inputs of the next layer. \n"
      ]
    },
    {
      "metadata": {
        "id": "n_b0u8Ch60bA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step Function\n",
        "\n",
        "![Heaviside Step Function](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d9/Dirac_distribution_CDF.svg/325px-Dirac_distribution_CDF.svg.png)\n",
        "\n",
        "All or nothing, a little extreme, which is fine, but makes updating weights through backpropagation impossible. Why? remember that during backpropagation we use derivatives in order to determine how much to update or not update weights. What is the derivative of the step function?"
      ]
    },
    {
      "metadata": {
        "id": "vKR0YhIVEnXZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Linear Function\n",
        "\n",
        "![Linear Function](http://www.roconnell.net/Parent%20function/linear.gif)\n",
        "\n",
        "The linear function takes the opposite tact from the step function and passes the signal onto the next layer by a constant factor. There are problems with this but the biggest problems again lie in backpropagation. The derivative of any linear function is a horizontal line which would indicate that we should update all weights by a constant amount every time -which on balance wouldn't change the behavior of our network. Linear functions are typically only used for very simple tasks where interpretability is important, but if interpretability is your highest priority, you probably shouldn't be using neural networks in the first place."
      ]
    },
    {
      "metadata": {
        "id": "JFurIVL6EkQ8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Sigmoid Function\n",
        "\n",
        "![Sigmoid Function](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/480px-Logistic-curve.svg.png)\n",
        "\n",
        "The sigmoid function works great as an activation function! it's continuously differentiable, its derivative doesn't have a constant slope, and having the higher slope in the middle pushes y value predictions towards extremes which is particularly useful for binary classification problems. I mean, this is why we use it as the squishifier in logistic regression as well. It constrains output, but over repeated epochs pushes predictions towards a strong binary prediction. \n",
        "\n",
        "What's the biggest problem with the sigmoid function? The fact that its slope gets pretty flat so quickly after its departure from zero. This means that updating weights based on its gradient really diminishes the size of our weight updates as our model gets more confident about its classifications. This is why even after so many iterations with our test score example we couldn't reach the levels of fit that our gradient descent based model could reach in just a few epochs."
      ]
    },
    {
      "metadata": {
        "id": "hm6p1HWbEhYi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Tanh Function\n",
        "\n",
        "![Tanh Function](http://mathworld.wolfram.com/images/interactive/TanhReal.gif)\n",
        "\n",
        "What if the sigmoid function didn't get so flat quite as soon when moving away from zero and was a little bit steeper in the middle? That's basically the Tanh function. The Tanh function can actually be created by scaling the sigmoid function by 2 in the y dimension and subtracting 1 from all values. It has basically the same properties as the sigmoid, still struggles from diminishingly flat gradients as we move away from 0, but its derivative is higher around 0 causing weights to move to the extremes a little faster. "
      ]
    },
    {
      "metadata": {
        "id": "sFOn_L6gEcz1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## ReLU Function\n",
        "\n",
        "![ReLU Function](https://cdn-images-1.medium.com/max/937/1*oePAhrm74RNnNEolprmTaQ.png)\n",
        "\n",
        "ReLU stands for Rectified Linear Units it is by far the most commonly used activation function in modern neural networks. It doesn't activate neurons that are being passed a negative signal and passes on positive signals. Think about why this might be useful. Remember how a lot of our initial weights got set to negative numbers by chance? This would have dealt with those negative weights a lot faster than the sigmoid function updating. What does the derivative of this function look like? It looks like the step function! This means that not all neurons are activated. With sigmoid basically all of our neurons are passing some amount of signal even if it's small making it hard for the network to differentiate important and less important connections. ReLU turns off a portion of our less important neurons which decreases computational load, but also helps the network learn what the most important connections are faster. \n",
        "\n",
        "What's the problem with relu? Well the left half of its derivative function shows that for neurons that are initialized with weights that cause them to have no activation, our gradient will not update those neuron's weights, this can lead to dead neurons that never fire and whose weights never get updated. We would probably want to update the weights of neurons that didn't fire even if it's just by a little bit in case we got unlucky with our initial weights and want to give those neurons a chance of turning back on in the future."
      ]
    },
    {
      "metadata": {
        "id": "XWdvWOBIETwk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Leaky ReLU\n",
        "\n",
        "![Leaky ReLU](https://cdn-images-1.medium.com/max/1600/1*ypsvQH7kvtI2BhzR2eT_Sw.png)\n",
        "\n",
        "Leaky ReLU accomplishes exactly that! it avoids having a gradient of 0 on the left side of its derivative function. This means that even \"dead\" neurons have a chance of being revived over enough iterations. In some specifications the slope of the leaky left-hand side can also be experimented with as a hyperparameter of the model!"
      ]
    },
    {
      "metadata": {
        "id": "VxHk-ud2QCPh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# https://keras.io/layers/advanced-activations/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FcAxkNFREMFb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Softmax Function\n",
        "\n",
        "![Softmax Function](https://cdn-images-1.medium.com/max/800/1*670CdxchunD-yAuUWdI7Bw.png)\n",
        "\n",
        "Like the sigmoid function but more useful for multi-class classification problems. The softmax function can take any set of inputs and translate them into probabilities that sum up to 1. This means that we can throw any list of outputs at it and it will translate them into probabilities, this is extremely useful for multi-class classification problems. Like MNIST for example..."
      ]
    },
    {
      "metadata": {
        "id": "23-XRRXKHs34",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Major takeaways\n",
        "\n",
        "- ReLU is generally better at obtaining the optimal model fit.\n",
        "- Sigmoid and its derivatives are usually better at classification problems.\n",
        "- Softmax for multi-class classification problems (USE IT IN THE LAST LAYER!)\n",
        "\n",
        "You'll typically see ReLU used for all initial layers and then the final layer being sigmoid or softmax for classification problems. But you can experiment and tune these selections as hyperparameters as well!"
      ]
    },
    {
      "metadata": {
        "id": "TWuoXZCCKCI7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## MNIST with Keras \n",
        "\n",
        "### This will be a good chance to bring up dropout regularization. :)"
      ]
    },
    {
      "metadata": {
        "id": "jmJ_5azs04pU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xhVJxlnTR_Hg",
        "colab_type": "code",
        "outputId": "ee424e78-6fbc-454c-c915-a1a69827e9ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "# global hyperparameters\n",
        "\n",
        "batch_size = 64\n",
        "num_classes = 10\n",
        "epochs = 20\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Flatten the images\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KcX6FhreT5Ly",
        "colab_type": "code",
        "outputId": "29d2e3fc-7cf7-4b2b-f834-9f1f217fc2a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(16, activation='relu', input_shape=(784,)))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_45 (Dense)             (None, 16)                12560     \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_46 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_47 (Dense)             (None, 10)                170       \n",
            "=================================================================\n",
            "Total params: 13,002\n",
            "Trainable params: 13,002\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uQcfy8_YUrpw",
        "colab_type": "code",
        "outputId": "69f85117-cbf9-4cf2-891f-a5c6696d6782",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 925
        }
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train, epochs=epochs, validation_split=.1)\n",
        "scores = model.evaluate(x_test,y_test)\n",
        "print(f'{model.metric_names[1]}: {scores[1]*100}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 54000 samples, validate on 6000 samples\n",
            "Epoch 1/20\n",
            "54000/54000 [==============================] - 5s 85us/step - loss: 0.6979 - acc: 0.7718 - val_loss: 0.2447 - val_acc: 0.9305\n",
            "Epoch 2/20\n",
            "54000/54000 [==============================] - 3s 65us/step - loss: 0.4240 - acc: 0.8701 - val_loss: 0.2132 - val_acc: 0.9408\n",
            "Epoch 3/20\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.3740 - acc: 0.8851 - val_loss: 0.1876 - val_acc: 0.9482\n",
            "Epoch 4/20\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.3434 - acc: 0.8953 - val_loss: 0.1739 - val_acc: 0.9515\n",
            "Epoch 5/20\n",
            "54000/54000 [==============================] - 3s 62us/step - loss: 0.3287 - acc: 0.9014 - val_loss: 0.1656 - val_acc: 0.9520\n",
            "Epoch 6/20\n",
            "54000/54000 [==============================] - 3s 65us/step - loss: 0.3108 - acc: 0.9059 - val_loss: 0.1693 - val_acc: 0.9525\n",
            "Epoch 7/20\n",
            "54000/54000 [==============================] - 3s 64us/step - loss: 0.2964 - acc: 0.9098 - val_loss: 0.1563 - val_acc: 0.9553\n",
            "Epoch 8/20\n",
            "54000/54000 [==============================] - 3s 61us/step - loss: 0.2875 - acc: 0.9122 - val_loss: 0.1463 - val_acc: 0.9572\n",
            "Epoch 9/20\n",
            "54000/54000 [==============================] - 3s 63us/step - loss: 0.2818 - acc: 0.9139 - val_loss: 0.1485 - val_acc: 0.9567\n",
            "Epoch 10/20\n",
            "54000/54000 [==============================] - 3s 63us/step - loss: 0.2722 - acc: 0.9161 - val_loss: 0.1395 - val_acc: 0.9595\n",
            "Epoch 11/20\n",
            "54000/54000 [==============================] - 3s 63us/step - loss: 0.2728 - acc: 0.9171 - val_loss: 0.1393 - val_acc: 0.9593\n",
            "Epoch 12/20\n",
            "54000/54000 [==============================] - 3s 63us/step - loss: 0.2631 - acc: 0.9202 - val_loss: 0.1458 - val_acc: 0.9567\n",
            "Epoch 13/20\n",
            "54000/54000 [==============================] - 3s 63us/step - loss: 0.2601 - acc: 0.9199 - val_loss: 0.1382 - val_acc: 0.9587\n",
            "Epoch 14/20\n",
            "54000/54000 [==============================] - 3s 63us/step - loss: 0.2597 - acc: 0.9195 - val_loss: 0.1421 - val_acc: 0.9578\n",
            "Epoch 15/20\n",
            "54000/54000 [==============================] - 4s 65us/step - loss: 0.2581 - acc: 0.9195 - val_loss: 0.1375 - val_acc: 0.9598\n",
            "Epoch 16/20\n",
            "54000/54000 [==============================] - 3s 63us/step - loss: 0.2519 - acc: 0.9226 - val_loss: 0.1373 - val_acc: 0.9587\n",
            "Epoch 17/20\n",
            "54000/54000 [==============================] - 4s 66us/step - loss: 0.2516 - acc: 0.9221 - val_loss: 0.1381 - val_acc: 0.9598\n",
            "Epoch 18/20\n",
            "54000/54000 [==============================] - 3s 65us/step - loss: 0.2476 - acc: 0.9232 - val_loss: 0.1314 - val_acc: 0.9622\n",
            "Epoch 19/20\n",
            "54000/54000 [==============================] - 4s 65us/step - loss: 0.2456 - acc: 0.9234 - val_loss: 0.1421 - val_acc: 0.9577\n",
            "Epoch 20/20\n",
            "54000/54000 [==============================] - 4s 66us/step - loss: 0.2414 - acc: 0.9244 - val_loss: 0.1349 - val_acc: 0.9613\n",
            "10000/10000 [==============================] - 0s 32us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-4da6b316a0e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{model.metric_names[1]}: {scores[1]*100}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'metric_names'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "CKmx8153w9Ci",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## What if we use dropout techniques to prevent overfitting? How does that affect our model?\n",
        "\n",
        "![Regularization](https://upload.wikimedia.org/wikipedia/commons/thumb/0/02/Regularization.svg/354px-Regularization.svg.png)"
      ]
    },
    {
      "metadata": {
        "id": "CWDopERJ16yJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### Try it with dropout"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}