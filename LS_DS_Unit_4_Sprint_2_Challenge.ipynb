{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2*\n",
    "\n",
    "# Sprint Challenge - Neural Network Foundations\n",
    "\n",
    "Table of Problems\n",
    "\n",
    "1. [Defining Neural Networks](#Q1)\n",
    "2. [Perceptron on XOR Gates](#Q2)\n",
    "3. [Multilayer Perceptron](#Q3)\n",
    "4. [Keras MMP](#Q4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Q1\"></a>\n",
    "## 1. Define the following terms:\n",
    "\n",
    "- **Neuron:** A neuron takes inputs, processes these inputs, decides if its active and gives an output, this output is then \n",
    "       checked how far it is from the expected output and then it modifies the wieghts of it frmula in order to minimize the distanc ebetween observed and expected.\n",
    "\n",
    "- **Input Layer:** Input layer is the part of a neuron network that takes in the features and inputs\n",
    "- **Hidden Layer:** Layer that process the inputs and activate them\n",
    "- **Output Layer:** Layer that gives us an output based on the output funtion of the hidden layer\n",
    "- **Activation:** when inputs in a node in a hidden layer gets processed in an activation function the neuron gets activated or not based on the outcome of that function \n",
    "- **Backpropagation:** The act of learning and adjusting the weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Perceptron on XOR Gates <a id=\"Q2\"></a>\n",
    "\n",
    "The XOr, or “exclusive or”, problem is a classic problem in ANN research. It is the problem of using a neural network to predict the outputs of XOr logic gates given two binary inputs. An XOr function should return a true value if the two inputs are not equal and a false value if they are equal. Create a perceptron class that can model the behavior of an AND gate. You can use the following table as your training data:\n",
    "\n",
    "|x1\t|x2 | y |\n",
    "|---|---|---|\n",
    "| 0 | 0 | 0 |\n",
    "| 0 | 1 | 1 |\n",
    "| 1 | 1 | 0 |\n",
    "| 1 | 0 | 1 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[0, 0],\n",
    "                [0, 1],\n",
    "                [1, 1],\n",
    "                [1, 0]])\n",
    "y = np.array([[0], [1], [0], [1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(object):\n",
    "    def __init__(self, rate = 0.01, niter = 10):\n",
    "        self.rate = rate\n",
    "        self.niter = niter\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit training data\n",
    "        \"\"\"\n",
    "        \n",
    "        self.weight = np.zeros(1 + X.shape[1])\n",
    "        \n",
    "        self.errors = []\n",
    "        \n",
    "        for i in range(self.niter):\n",
    "            err = 0\n",
    "            for xi, target in zip(X, y):\n",
    "                delta_w = self.rate * (target - self.predict(xi))\n",
    "                self.weight[1:] += delta_w * xi\n",
    "                self.weight[0] += delta_w\n",
    "                err += int(delta_w != 0.0)\n",
    "            self.errors.append(err)\n",
    "        return self\n",
    "    \n",
    "    def net_input(self, X):\n",
    "        \"\"\"Calculate net input\"\"\"\n",
    "        return np.dot(X, self.weight[1:]) + self.weight[0]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.where(self.net_input(X) >= 1, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Perceptron at 0x1be6f2f8ba8>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pn = Perceptron(0.01, 50)\n",
    "pn.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pn.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multilayer Perceptron <a id=\"Q3\"></a>\n",
    "\n",
    "Implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights.\n",
    "Your network must have one hidden layer.\n",
    "You do not have to update weights via gradient descent. You can use something like the derivative of the sigmoid function to update weights.\n",
    "Train your model on the Heart Disease dataset from UCI:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, inputs, hiddenNodes, outputNodes):\n",
    "        self.inputs = inputs\n",
    "        self.hiddenNodes = hiddenNodes\n",
    "        self.outputNodes = outputNodes\n",
    "        \n",
    "        self.weights1 = np.random.randn(self.inputs, self.hiddenNodes)\n",
    "        self.weights2 = np.random.randn(self.hiddenNodes, self.outputNodes)\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1 + np.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        \n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "        \n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "        \n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "        \n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "        \n",
    "        return self.activated_output\n",
    "    \n",
    "    def backward(self, X, y, o):\n",
    "        self.o_error = y - o\n",
    "        \n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o)\n",
    "        \n",
    "        self.z2_error = self.o_delta.dot(self.weights2.T)\n",
    "        self.z2_delta = self.z2_error * self.sigmoidPrime(self.activated_hidden)\n",
    "        \n",
    "        self.weights2 += self.activated_hidden.T.dot(self.o_delta)\n",
    "        \n",
    "        self.weights1 += X.T.dot(self.z2_delta)\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        o = self.feed_forward(X)\n",
    "        self.backward(X, y, o)\n",
    "        \n",
    "    def accuracy(y,o):\n",
    "        listy=[]\n",
    "        for i in range(len(y)):\n",
    "            if y[i] == o[i]:\n",
    "                listy.append(1)\n",
    "            else:\n",
    "                listy.append(0)\n",
    "        return (sum(listy)/len(listy))\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import genfromtxt\n",
    "my_data = genfromtxt('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv', delimiter=',')[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[63.,  1.,  3., ...,  0.,  1.,  1.],\n",
       "       [37.,  1.,  2., ...,  0.,  2.,  1.],\n",
       "       [41.,  0.,  1., ...,  0.,  2.,  1.],\n",
       "       ...,\n",
       "       [68.,  1.,  0., ...,  2.,  3.,  0.],\n",
       "       [57.,  1.,  0., ...,  1.,  3.,  0.],\n",
       "       [57.,  0.,  1., ...,  1.,  2.,  0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = my_data[:,:-1]\n",
    "y = my_data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((303, 13), (303,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.reshape(303,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5445544554455446"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------EPOCH 1---------+\n",
      "Input: \n",
      " [[63.  1.  3. ...  0.  0.  1.]\n",
      " [37.  1.  2. ...  0.  0.  2.]\n",
      " [41.  0.  1. ...  2.  0.  2.]\n",
      " ...\n",
      " [68.  1.  0. ...  1.  2.  3.]\n",
      " [57.  1.  0. ...  1.  1.  3.]\n",
      " [57.  0.  1. ...  1.  1.  2.]]\n",
      "Actual Output: \n",
      " [1.]\n",
      "Predicted Output: \n",
      " [0.81330988]\n",
      "Loss: \n",
      " 0.3220954531334406\n",
      "+---------EPOCH 2---------+\n",
      "Input: \n",
      " [[63.  1.  3. ...  0.  0.  1.]\n",
      " [37.  1.  2. ...  0.  0.  2.]\n",
      " [41.  0.  1. ...  2.  0.  2.]\n",
      " ...\n",
      " [68.  1.  0. ...  1.  2.  3.]\n",
      " [57.  1.  0. ...  1.  1.  3.]\n",
      " [57.  0.  1. ...  1.  1.  2.]]\n",
      "Actual Output: \n",
      " [1.]\n",
      "Predicted Output: \n",
      " [6.90882972e-16]\n",
      "Loss: \n",
      " 0.5445544554455277\n",
      "+---------EPOCH 3---------+\n",
      "Input: \n",
      " [[63.  1.  3. ...  0.  0.  1.]\n",
      " [37.  1.  2. ...  0.  0.  2.]\n",
      " [41.  0.  1. ...  2.  0.  2.]\n",
      " ...\n",
      " [68.  1.  0. ...  1.  2.  3.]\n",
      " [57.  1.  0. ...  1.  1.  3.]\n",
      " [57.  0.  1. ...  1.  1.  2.]]\n",
      "Actual Output: \n",
      " [1.]\n",
      "Predicted Output: \n",
      " [6.90882972e-16]\n",
      "Loss: \n",
      " 0.5445544554455277\n",
      "+---------EPOCH 4---------+\n",
      "Input: \n",
      " [[63.  1.  3. ...  0.  0.  1.]\n",
      " [37.  1.  2. ...  0.  0.  2.]\n",
      " [41.  0.  1. ...  2.  0.  2.]\n",
      " ...\n",
      " [68.  1.  0. ...  1.  2.  3.]\n",
      " [57.  1.  0. ...  1.  1.  3.]\n",
      " [57.  0.  1. ...  1.  1.  2.]]\n",
      "Actual Output: \n",
      " [1.]\n",
      "Predicted Output: \n",
      " [6.90882972e-16]\n",
      "Loss: \n",
      " 0.5445544554455277\n",
      "+---------EPOCH 5---------+\n",
      "Input: \n",
      " [[63.  1.  3. ...  0.  0.  1.]\n",
      " [37.  1.  2. ...  0.  0.  2.]\n",
      " [41.  0.  1. ...  2.  0.  2.]\n",
      " ...\n",
      " [68.  1.  0. ...  1.  2.  3.]\n",
      " [57.  1.  0. ...  1.  1.  3.]\n",
      " [57.  0.  1. ...  1.  1.  2.]]\n",
      "Actual Output: \n",
      " [1.]\n",
      "Predicted Output: \n",
      " [6.90882972e-16]\n",
      "Loss: \n",
      " 0.5445544554455277\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(13, 10, 1)\n",
    "\n",
    "for i in range(5):\n",
    "    if (i+1 in [1,2,3,4,5]) or ((i+1) % 200 ==0):\n",
    "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
    "        print('Input: \\n', X)\n",
    "        print('Actual Output: \\n', y[10])\n",
    "        print('Predicted Output: \\n', str(nn.feed_forward(X)[10]))\n",
    "        print(\"Loss: \\n\", str(np.mean(np.square(y - nn.feed_forward(X)))))\n",
    "        \n",
    "    nn.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: \n",
      " 0.5445544554455446\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(13, 5000, 1)\n",
    "nn.train(X,y)\n",
    "print(\"Loss: \\n\", str(np.mean(np.square(y - nn.feed_forward(X)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Keras MMP <a id=\"Q4\"></a>\n",
    "\n",
    "Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy.\n",
    "Use the Heart Disease Dataset (binary classification)\n",
    "Use an appropriate loss function for a binary classification task\n",
    "Use an appropriate activation function on the final layer of your network.\n",
    "Train your model using verbose output for ease of grading.\n",
    "Use GridSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
    "When hyperparameter tuning, show you work by adding code cells for each new experiment.\n",
    "Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
    "You must hyperparameter tune at least 5 parameters in order to get a 3 on this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "##### Your Code Here #####\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age         0\n",
       "sex         0\n",
       "cp          0\n",
       "trestbps    0\n",
       "chol        0\n",
       "fbs         0\n",
       "restecg     0\n",
       "thalach     0\n",
       "exang       0\n",
       "oldpeak     0\n",
       "slope       0\n",
       "ca          0\n",
       "thal        0\n",
       "target      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.reshape(303,)\n",
    "#y = y.reshape(303,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1st trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0816 20:13:14.826681  2364 deprecation_wrapper.py:119] From C:\\Users\\mhdal\\Anaconda3\\envs\\lambda-neural\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0816 20:13:14.865544  2364 deprecation_wrapper.py:119] From C:\\Users\\mhdal\\Anaconda3\\envs\\lambda-neural\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0816 20:13:14.878509  2364 deprecation_wrapper.py:119] From C:\\Users\\mhdal\\Anaconda3\\envs\\lambda-neural\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0816 20:13:14.927939  2364 deprecation_wrapper.py:119] From C:\\Users\\mhdal\\Anaconda3\\envs\\lambda-neural\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0816 20:13:14.946919  2364 deprecation_wrapper.py:119] From C:\\Users\\mhdal\\Anaconda3\\envs\\lambda-neural\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0816 20:13:14.950911  2364 deprecation.py:323] From C:\\Users\\mhdal\\Anaconda3\\envs\\lambda-neural\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0816 20:13:15.122048  2364 deprecation_wrapper.py:119] From C:\\Users\\mhdal\\Anaconda3\\envs\\lambda-neural\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 272 samples, validate on 31 samples\n",
      "Epoch 1/10\n",
      "272/272 [==============================] - 1s 3ms/step - loss: 6.2715 - acc: 0.6066 - val_loss: 15.9424 - val_acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "272/272 [==============================] - 0s 73us/step - loss: 6.2715 - acc: 0.6066 - val_loss: 15.9424 - val_acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "272/272 [==============================] - 0s 77us/step - loss: 6.2715 - acc: 0.6066 - val_loss: 15.9424 - val_acc: 0.0000e+00\n",
      "Epoch 4/10\n",
      "272/272 [==============================] - 0s 103us/step - loss: 6.2715 - acc: 0.6066 - val_loss: 15.9424 - val_acc: 0.0000e+00\n",
      "Epoch 5/10\n",
      "272/272 [==============================] - 0s 95us/step - loss: 6.2715 - acc: 0.6066 - val_loss: 15.9424 - val_acc: 0.0000e+00\n",
      "Epoch 6/10\n",
      "272/272 [==============================] - 0s 70us/step - loss: 6.2715 - acc: 0.6066 - val_loss: 15.9424 - val_acc: 0.0000e+00\n",
      "Epoch 7/10\n",
      "272/272 [==============================] - 0s 66us/step - loss: 6.2715 - acc: 0.6066 - val_loss: 15.9424 - val_acc: 0.0000e+00\n",
      "Epoch 8/10\n",
      "272/272 [==============================] - 0s 70us/step - loss: 6.2715 - acc: 0.6066 - val_loss: 15.9424 - val_acc: 0.0000e+00\n",
      "Epoch 9/10\n",
      "272/272 [==============================] - 0s 73us/step - loss: 6.2715 - acc: 0.6066 - val_loss: 15.9424 - val_acc: 0.0000e+00\n",
      "Epoch 10/10\n",
      "272/272 [==============================] - 0s 77us/step - loss: 6.2715 - acc: 0.6066 - val_loss: 15.9424 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2263124a8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Important Hyperparameters\n",
    "inputs = X.shape[1]\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "\n",
    "# Create Model\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=inputs, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "# Fit Model\n",
    "model.fit(X, y, validation_split=0.1, epochs=10, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 272 samples, validate on 31 samples\n",
      "Epoch 1/30\n",
      "272/272 [==============================] - 11s 39ms/step - loss: 1.2192 - acc: 0.4449 - val_loss: 1.8965 - val_acc: 0.0000e+00\n",
      "Epoch 2/30\n",
      "272/272 [==============================] - 0s 198us/step - loss: 0.8306 - acc: 0.5772 - val_loss: 0.8317 - val_acc: 0.2903\n",
      "Epoch 3/30\n",
      "272/272 [==============================] - 0s 227us/step - loss: 0.7083 - acc: 0.5956 - val_loss: 1.1190 - val_acc: 0.1935\n",
      "Epoch 4/30\n",
      "272/272 [==============================] - 0s 198us/step - loss: 0.6725 - acc: 0.5772 - val_loss: 1.2797 - val_acc: 0.1290\n",
      "Epoch 5/30\n",
      "272/272 [==============================] - 0s 178us/step - loss: 0.6361 - acc: 0.6213 - val_loss: 0.9883 - val_acc: 0.2258\n",
      "Epoch 6/30\n",
      "272/272 [==============================] - 0s 202us/step - loss: 0.5736 - acc: 0.7132 - val_loss: 0.4423 - val_acc: 0.8387\n",
      "Epoch 7/30\n",
      "272/272 [==============================] - 0s 182us/step - loss: 0.6023 - acc: 0.6471 - val_loss: 1.2440 - val_acc: 0.1613\n",
      "Epoch 8/30\n",
      "272/272 [==============================] - 0s 183us/step - loss: 0.5959 - acc: 0.6875 - val_loss: 1.0880 - val_acc: 0.2903\n",
      "Epoch 9/30\n",
      "272/272 [==============================] - 0s 182us/step - loss: 0.6264 - acc: 0.6912 - val_loss: 0.7200 - val_acc: 0.6774\n",
      "Epoch 10/30\n",
      "272/272 [==============================] - 0s 176us/step - loss: 0.5717 - acc: 0.6912 - val_loss: 0.5126 - val_acc: 0.7419\n",
      "Epoch 11/30\n",
      "272/272 [==============================] - 0s 173us/step - loss: 0.5981 - acc: 0.7243 - val_loss: 0.6758 - val_acc: 0.6774\n",
      "Epoch 12/30\n",
      "272/272 [==============================] - 0s 180us/step - loss: 0.5534 - acc: 0.6985 - val_loss: 1.0462 - val_acc: 0.3548\n",
      "Epoch 13/30\n",
      "272/272 [==============================] - 0s 176us/step - loss: 0.5886 - acc: 0.6838 - val_loss: 0.9668 - val_acc: 0.3871\n",
      "Epoch 14/30\n",
      "272/272 [==============================] - 0s 198us/step - loss: 0.6032 - acc: 0.6544 - val_loss: 0.7065 - val_acc: 0.6774\n",
      "Epoch 15/30\n",
      "272/272 [==============================] - 0s 220us/step - loss: 0.5573 - acc: 0.7243 - val_loss: 0.4957 - val_acc: 0.7419\n",
      "Epoch 16/30\n",
      "272/272 [==============================] - 0s 180us/step - loss: 0.5998 - acc: 0.6838 - val_loss: 0.9607 - val_acc: 0.4516\n",
      "Epoch 17/30\n",
      "272/272 [==============================] - 0s 183us/step - loss: 0.5739 - acc: 0.6912 - val_loss: 1.1419 - val_acc: 0.2581\n",
      "Epoch 18/30\n",
      "272/272 [==============================] - 0s 205us/step - loss: 0.5732 - acc: 0.6949 - val_loss: 1.1705 - val_acc: 0.2258\n",
      "Epoch 19/30\n",
      "272/272 [==============================] - 0s 176us/step - loss: 0.5422 - acc: 0.7243 - val_loss: 0.9755 - val_acc: 0.4516\n",
      "Epoch 20/30\n",
      "272/272 [==============================] - 0s 183us/step - loss: 0.5371 - acc: 0.7426 - val_loss: 0.7192 - val_acc: 0.6452\n",
      "Epoch 21/30\n",
      "272/272 [==============================] - 0s 180us/step - loss: 0.5204 - acc: 0.7353 - val_loss: 0.8011 - val_acc: 0.5484\n",
      "Epoch 22/30\n",
      "272/272 [==============================] - 0s 195us/step - loss: 0.5193 - acc: 0.7463 - val_loss: 1.6540 - val_acc: 0.0645\n",
      "Epoch 23/30\n",
      "272/272 [==============================] - 0s 176us/step - loss: 0.6206 - acc: 0.7096 - val_loss: 0.6438 - val_acc: 0.6774\n",
      "Epoch 24/30\n",
      "272/272 [==============================] - 0s 172us/step - loss: 0.5132 - acc: 0.7426 - val_loss: 0.6881 - val_acc: 0.6452\n",
      "Epoch 25/30\n",
      "272/272 [==============================] - 0s 176us/step - loss: 0.5263 - acc: 0.7316 - val_loss: 0.9962 - val_acc: 0.4194\n",
      "Epoch 26/30\n",
      "272/272 [==============================] - 0s 176us/step - loss: 0.5090 - acc: 0.7500 - val_loss: 0.9138 - val_acc: 0.4516\n",
      "Epoch 27/30\n",
      "272/272 [==============================] - 0s 182us/step - loss: 0.5077 - acc: 0.7463 - val_loss: 1.4201 - val_acc: 0.1290\n",
      "Epoch 28/30\n",
      "272/272 [==============================] - 0s 176us/step - loss: 0.5223 - acc: 0.7500 - val_loss: 0.7970 - val_acc: 0.5484\n",
      "Epoch 29/30\n",
      "272/272 [==============================] - 0s 185us/step - loss: 0.4903 - acc: 0.7537 - val_loss: 0.9488 - val_acc: 0.4194\n",
      "Epoch 30/30\n",
      "272/272 [==============================] - 0s 220us/step - loss: 0.4870 - acc: 0.7537 - val_loss: 0.7719 - val_acc: 0.5484\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a3113e9a90>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Important Hyperparameters\n",
    "inputs = X.shape[1]\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "# Create Model\n",
    "model = Sequential()\n",
    "model.add(Dense(39, input_dim=inputs, activation='relu'))\n",
    "model.add(Dense(26, activation='relu'))\n",
    "model.add(Dense(13, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "# Fit Model\n",
    "model.fit(X, y, validation_split=0.1, epochs=30, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 45.90%\n",
      "acc: 68.85%\n",
      "acc: 72.13%\n",
      "acc: 45.00%\n",
      "acc: 68.33%\n",
      "60.04% +/- 11.99%\n"
     ]
    }
   ],
   "source": [
    "# MLP for Pima Indians Dataset with 10-fold cross validation\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 42\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# define 5-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "cvscores = []\n",
    "for train, test in kfold.split(X, y):\n",
    "  # create model\n",
    "  model = Sequential()\n",
    "  model.add(Dense(26, input_dim=13, activation='relu'))\n",
    "  model.add(Dense(13, activation='relu'))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "  # Compile model\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) # Fit the model\n",
    "  model.fit(X[train], y[train], epochs=10, batch_size=10, verbose=0)\n",
    "  # evaluate the model\n",
    "  scores = model.evaluate(X[test], y[test], verbose=0)\n",
    "  print(f'{model.metrics_names[1]}: {(scores[1]*100):.2f}%') \n",
    "  cvscores.append(scores[1]*100)\n",
    "print(f'{numpy.mean(cvscores):.2f}% +/- {numpy.std(cvscores):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mhdal\\Anaconda3\\envs\\lambda-neural\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.574257430316198 using {'batch_size': 40, 'epochs': 20}\n",
      "Means: 0.29042904501897665, Stdev: 0.24604618822183663 with: {'batch_size': 10, 'epochs': 20}\n",
      "Means: 0.3201320132504989, Stdev: 0.06534320093650178 with: {'batch_size': 16, 'epochs': 20}\n",
      "Means: 0.5313531343299563, Stdev: 0.07511423565426005 with: {'batch_size': 20, 'epochs': 20}\n",
      "Means: 0.574257430316198, Stdev: 0.14095152596546182 with: {'batch_size': 40, 'epochs': 20}\n",
      "Means: 0.39603960602590355, Stdev: 0.17110866261818813 with: {'batch_size': 60, 'epochs': 20}\n",
      "Means: 0.3861386224184886, Stdev: 0.21065311621672145 with: {'batch_size': 80, 'epochs': 20}\n",
      "Means: 0.36963696841752963, Stdev: 0.27879488478193154 with: {'batch_size': 100, 'epochs': 20}\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 42\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model():\n",
    "  # create model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(39, input_dim=inputs, activation='relu'))\n",
    "        model.add(Dense(26, activation='relu'))\n",
    "        model.add(Dense(13, activation='relu'))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "# Compile model\n",
    "  # Compile model\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "# batch_size = [10, 20, 40, 60, 80, 100]\n",
    "# param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "# define the grid search parameters\n",
    "param_grid = {'batch_size': [10, 16, 20, 40, 60, 80, 100],\n",
    "              'epochs': [20]}\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.9009900991082584 using {'batch_size': 16, 'epochs': 40}\n",
      "Means: 0.2541254129346841, Stdev: 0.24750274985316215 with: {'batch_size': 16, 'epochs': 10}\n",
      "Means: 0.6006600660557794, Stdev: 0.28379000428434986 with: {'batch_size': 16, 'epochs': 20}\n",
      "Means: 0.9009900991082584, Stdev: 0.14002114465031684 with: {'batch_size': 16, 'epochs': 40}\n",
      "Means: 0.25082508260660835, Stdev: 0.3547202336056174 with: {'batch_size': 16, 'epochs': 60}\n",
      "Means: 0.39273928125502644, Stdev: 0.27844304265960607 with: {'batch_size': 40, 'epochs': 10}\n",
      "Means: 0.1848184833481367, Stdev: 0.12841634686036554 with: {'batch_size': 40, 'epochs': 20}\n",
      "Means: 0.5445544585929846, Stdev: 0.41308215433215534 with: {'batch_size': 40, 'epochs': 40}\n",
      "Means: 0.2739273943129939, Stdev: 0.2948582157693152 with: {'batch_size': 40, 'epochs': 60}\n"
     ]
    }
   ],
   "source": [
    "# define the grid search parameters\n",
    "param_grid = {'batch_size': [16,40],\n",
    "              'epochs': [10, 20, 40, 60]}\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adadelta [8.777180698883246, 0.45544554455445546]\n",
      "acc: 45.90%\n",
      "Adadelta [7.260887781778994, 0.5445544554455446]\n",
      "acc: 54.10%\n",
      "Adadelta [8.777180698883246, 0.45544554455445546]\n",
      "acc: 45.90%\n",
      "Adadelta [7.260887781778994, 0.5445544554455446]\n",
      "acc: 55.00%\n",
      "Adadelta [0.7469227936598334, 0.5511551155115512]\n",
      "acc: 56.67%\n",
      "51.51% +/- 4.66%\n",
      "3.52% +/- 3.67%\n",
      "Adagrad [0.5045763531533798, 0.7590759083776191]\n",
      "acc: 78.69%\n",
      "Adagrad [8.777180698883246, 0.45544554455445546]\n",
      "acc: 45.90%\n",
      "Adagrad [0.43642596295564484, 0.8151815189386752]\n",
      "acc: 80.33%\n",
      "Adagrad [8.777180698883246, 0.45544554455445546]\n",
      "acc: 45.00%\n",
      "Adagrad [0.5197574151034402, 0.7293729374904444]\n",
      "acc: 70.00%\n",
      "63.98% +/- 15.54%\n",
      "2.22% +/- 3.29%\n"
     ]
    }
   ],
   "source": [
    "# define the grid search parameters\n",
    "param_grid = {'batch_size': [16],\n",
    "              'epochs': [40]}\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "for opt in ['Adadelta','Adagrad','Adam','Adamax','NAdam','RMSprop','SGD']:\n",
    "\n",
    "    score=[]\n",
    "    cvscores=[]\n",
    "    for train, test in kfold.split(X, y):\n",
    "      # create model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(39, input_dim=13, activation='relu'))\n",
    "        model.add(Dense(26, activation='relu'))\n",
    "        model.add(Dense(13, activation='relu'))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "        model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "        # Fit Model\n",
    "        model.fit(X, y, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "        # evaluate the model\n",
    "        scores = model.evaluate(X[test], y[test], verbose=0)\n",
    "        print(opt, model.evaluate(X, y, verbose=0))\n",
    "        print(f'{model.metrics_names[1]}: {(scores[1]*100):.2f}%')\n",
    "        score.append(scores)\n",
    "        cvscores.append(scores[1]*100)\n",
    "    print(f'{numpy.mean(cvscores):.2f}% +/- {numpy.std(cvscores):.2f}%')\n",
    "    print(f'{numpy.mean(score):.2f}% +/- {numpy.std(score):.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Scale input data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python [conda env:lambda-neural]",
   "language": "python",
   "name": "conda-env-lambda-neural-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "0.14.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
