{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_431_Intro_to_NN_Assignment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nickwinters1/DS-Unit-4-Sprint-2-Neural-Networks/blob/master/LS_DS_431_Intro_to_NN_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dVfaLrjLvxvQ"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "# Neural Networks\n",
        "\n",
        "## *Data Science Unit 4 Sprint 2 Assignment 1*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wxtoY12mwmih"
      },
      "source": [
        "## Define the Following:\n",
        "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
        "\n",
        "### Input Layer:\n",
        "* An input layer is the information that you feed in to a system.  Whatever the data is.  This is what the neural network will use to determine outputs.\n",
        "### Hidden Layer:\n",
        "* A hidden layer in a neural network is a layer that is doing some work on the inputs, but cannot be seen or desribed by looking at it.  They are the steps between the input and output of a network.  It uses weights and biases to perform work on the data. The outputs are also determined by a given activation function of the network.\n",
        "### Output Layer:\n",
        "* The output layer are the predictions or classification of the data after it has gone through the network. As stated previously, it is helped to be determined by the activation function used in the neural network.\n",
        "### Neuron:\n",
        "* A Neuron (or node) in a neural network is the what the basic make-up of the network is. Each input, bias, hidden node, and output is a Neuron in the network.  Things get passed on, again, determined by the activation function through each layer, to determine the final Neurons (outputs).\n",
        "### Weight:\n",
        "* A weight is something that is attached to an input of the network.  They are set at the beginnning and adjusted throughout the training period of the network.  Weights essentially determine how influential an input is in determining the ouput.\n",
        "### Activation Function:\n",
        "* Have been talknig a lot about activation functions.  Their main function .. tee hee .. is to deal with non-linear data.  It can shrink information down to lower dimensions to then get a number that the network can use to determine the output.  In a classification case, the network uses the number produced by the activation function to determine what class the input is.\n",
        "### Node Map:\n",
        "![Neural Network Zoo](http://www.asimovinstitute.org/wp-content/uploads/2016/09/neuralnetworks.png)\n",
        "### Perceptron:\n",
        "* A Perceptron is one of the more basic neural networks.  It is typically used to classify binary outputs. It consists of 1 working neuron, that takes information from data (inputs) and tosses it out so that the activation function can determine it's value.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NXuy9WcWzxa4"
      },
      "source": [
        "## Inputs -> Outputs\n",
        "\n",
        "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PlSwIJMC0A8F"
      },
      "source": [
        "#### Your Answer Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6sWR43PTwhSk"
      },
      "source": [
        "## Write your own perceptron code that can correctly classify a NAND gate. \n",
        "\n",
        "| x1 | x2 | y |\n",
        "|----|----|---|\n",
        "| 0  | 0  | 1 |\n",
        "| 1  | 0  | 1 |\n",
        "| 0  | 1  | 1 |\n",
        "| 1  | 1  | 0 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Sgh7VFGwnXGH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "2afb10b9-e095-4fac-d183-3b4601572792"
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed(555)\n",
        "inputs = np.array([[1,0,0],\n",
        "                  [1,1,0],\n",
        "                  [1,0,1],\n",
        "                  [0,1,1]])\n",
        "\n",
        "ground_truth = [[1],[1],[1],[0]]\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "def sigmoid_derivate(x):\n",
        "  sx = sigmoid(x)\n",
        "  return sx*(1-sx)\n",
        "\n",
        "weights = np.random.random((3,1))\n",
        "for iteration in range(10000):\n",
        "  #weighted sum of inputs/weights\n",
        "  weighted_sum = np.dot(inputs,weights)\n",
        "  #activate\n",
        "  activated_output = sigmoid(weighted_sum)\n",
        "  #calculate teh error\n",
        "  error = ground_truth - activated_output\n",
        "  #adjustments\n",
        "  adjustments = error * sigmoid_derivate(activated_output)\n",
        "  weights+=np.dot(inputs.T,adjustments)\n",
        " \n",
        "format_output = ['%.2f' % x for x in activated_output]\n",
        "print(\"weights and training\")\n",
        "print(weights)\n",
        "print(\"output after training\")\n",
        "print(format_output)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "weights and training\n",
            "[[11.69679857]\n",
            " [-3.81080473]\n",
            " [-3.80297293]]\n",
            "output after training\n",
            "['1.00', '1.00', '1.00', '0.00']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Xf7sdqVs0s4x"
      },
      "source": [
        "## Implement your own Perceptron Class and use it to classify a binary dataset like: \n",
        "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
        "- [Titanic](https://raw.githubusercontent.com/ryanleeallred/datasets/master/titanic.csv)\n",
        "- [A two-class version of the Iris dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/Iris.csv)\n",
        "\n",
        "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-W0tiX1F1hh2",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv')\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "mms = MinMaxScaler()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoZItPkQ2uj3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head()\n",
        "df['is_f']=np.where(df['sex']=='female',1,0)\n",
        "df['age']=np.where(df['age'].isna(),df['age'].mean(),df['age'])\n",
        "df['age']=mms.fit_transform(df[['age']])\n",
        "df['fare']=mms.fit_transform(df[['fare']])\n",
        "df['pclass']=mms.fit_transform(df[['pclass']])\n",
        "df['parch']=mms.fit_transform(df[['parch']])\n",
        "df[['alone']] *= 1\n",
        "features = ['survived','pclass','is_f','age','sibsp','parch','fare','alone']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CdfDi1o2x_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = 400\n",
        "train = df[:n]\n",
        "\n",
        "inputs = train[features].values\n",
        "ground_truth = df['survived'].values[:n].reshape(-1,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1jEKeai2ySk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0a9c7f5d-c735-4851-c2de-85af4c3737db"
      },
      "source": [
        "inputs.shape,ground_truth.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((400, 8), (400, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUi-PN962yfT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "728c3c39-6451-49a2-f7de-0c4ab984ae14"
      },
      "source": [
        "np.array(ground_truth)[:5]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3Ouy-552ymT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "def sigmoid_derivate(x):\n",
        "  sx = sigmoid(x)\n",
        "  return sx*(1-sx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDQIl_Uz2yta",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "dc1f869a-68e3-4862-8198-4b9c7c7f9bd9"
      },
      "source": [
        "weights = np.random.random((len(features),1))\n",
        "for iteration in range(100):\n",
        "\n",
        "  weighted_sum = np.dot(inputs,weights)\n",
        "\n",
        "  activated_output = sigmoid(weighted_sum)\n",
        " \n",
        "  error = ground_truth - activated_output\n",
        "  \n",
        "  \n",
        "  adjustments = error * sigmoid_derivate(activated_output)\n",
        " \n",
        "  weights=weights + np.dot(inputs.T,adjustments)\n",
        " \n",
        " \n",
        "format_output = [np.round(x,2) for x in activated_output]\n",
        "print(\"activated_output\")\n",
        "print(format_output)\n",
        "print(\"weights\")\n",
        "print(weights)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "activated_output\n",
            "[array([0.]), array([1.]), array([1.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([1.]), array([1.]), array([1.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([1.]), array([0.]), array([1.]), array([0.]), array([1.]), array([1.]), array([1.]), array([0.]), array([1.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([1.]), array([1.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([1.]), array([1.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([1.]), array([0.]), array([1.]), array([1.]), array([0.]), array([1.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([1.]), array([1.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([1.]), array([1.]), array([0.]), array([1.]), array([1.]), array([0.]), array([1.]), array([1.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.05]), array([0.]), array([0.]), array([0.]), array([1.]), array([1.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.02]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([1.]), array([0.]), array([1.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([1.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([1.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([1.]), array([0.]), array([1.]), array([1.]), array([0.]), array([0.]), array([1.]), array([0.]), array([1.]), array([1.]), array([1.]), array([1.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([1.]), array([1.]), array([1.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([1.]), array([1.]), array([0.]), array([1.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([1.]), array([1.]), array([1.]), array([1.]), array([0.]), array([0.98]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([1.]), array([1.]), array([0.]), array([1.]), array([1.]), array([0.01]), array([1.]), array([1.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([1.]), array([0.]), array([1.]), array([1.]), array([1.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.05]), array([1.]), array([1.]), array([1.]), array([1.]), array([0.]), array([1.]), array([0.]), array([1.]), array([1.]), array([1.]), array([0.]), array([1.]), array([1.]), array([1.]), array([0.]), array([0.]), array([0.]), array([1.]), array([1.]), array([0.]), array([1.]), array([1.]), array([0.]), array([0.]), array([1.]), array([1.]), array([0.]), array([1.]), array([0.]), array([1.]), array([1.]), array([1.]), array([1.]), array([0.]), array([0.01]), array([0.]), array([1.]), array([0.]), array([0.]), array([1.]), array([1.]), array([0.]), array([1.]), array([1.]), array([0.]), array([0.]), array([0.]), array([1.]), array([1.]), array([1.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([1.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([1.]), array([1.]), array([1.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([1.]), array([0.01]), array([0.]), array([0.]), array([1.]), array([1.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([1.]), array([1.]), array([1.]), array([0.]), array([1.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.])]\n",
            "weights\n",
            "[[ 47.20660191]\n",
            " [-13.15958577]\n",
            " [  6.0146865 ]\n",
            " [ -7.86614792]\n",
            " [ -7.17648676]\n",
            " [ -2.93489073]\n",
            " [ -2.27061845]\n",
            " [ -9.64854988]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6QR4oAW1xdyu"
      },
      "source": [
        "## Stretch Goals:\n",
        "\n",
        "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
        "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
        "- Try and implement your own backpropagation algorithm.\n",
        "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
      ]
    }
  ]
}