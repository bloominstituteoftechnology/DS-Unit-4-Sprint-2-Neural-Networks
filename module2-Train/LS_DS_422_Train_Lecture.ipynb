{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aS4GZ37Wgcjr"
   },
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 4, Sprint 2, Module 2*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etFf1WLWgcjt",
    "toc-hr-collapsed": false
   },
   "source": [
    "# Train (Prepare)\n",
    "__*Neural Network Foundations*__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXB80QOhgcju"
   },
   "source": [
    "## Learning Objectives\n",
    "* <a href=\"#p1\">Part 1</a>: Student should be able to explain the intuition behind backpropagation and gradient descent\n",
    "* <a href=\"#p2\">Part 2</a>: Student should be able to discuss the importance of batch size\n",
    "* <a href=\"#p3\">Part 3</a>: Student should be able to discuss the importance of learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8YuQu2lfgcju"
   },
   "source": [
    "## Summary of Yesterday\n",
    "\n",
    "Yesterday, we learned about some of the principal components of Neural Networks: Neurons, Weights, Activation Functions, and layers (input, output, & hidden). Today, we will reinforce our understanding of those components and introduce the mechanics of training a neural network. Feed-forward neural networks, such as multi-layer perceptrons (MLPs), are almost always trained using some variation of gradient descent where the gradient has been calculated by backpropagation.\n",
    "\n",
    "  <center><img src=\"https://raw.githubusercontent.com/LambdaSchool/DS-Unit-4-Sprint-2-Neural-Networks/main/module1-Architect/IMG_0167.jpeg\" width=400></center>\n",
    "\n",
    "- There are three kinds of layers: input, hidden, and output layers.\n",
    "- Each layer is made up of **n** individual neurons (aka activation units) which have a corresponding weight and bias.\n",
    "- Signal is passed from layer to layer through a network by:\n",
    " - Taking in inputs from the training data (or previous layer)\n",
    " - Multiplying each input by its corresponding weight (think arrow/connecting line)\n",
    " - Adding a bias to this weighted some of inputs and weights\n",
    " - Activating this weighted sum + bias by squishifying it with sigmoid or some other activation function. With a single perceptron with three inputs, calculating the output from the node is done like so:\n",
    "\\begin{align}\n",
    " y = sigmoid(\\sum(weight_{1}input_{1} + weight_{2}input_{2} + weight_{3}input_{3}) + bias)\n",
    "\\end{align}\n",
    " - this final activated value is the signal that gets passed onto the next layer of the network.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpi4R03rgcjv"
   },
   "source": [
    "## Training a Neural Network: *Formal Summary*\n",
    "\n",
    "0. Pick a network architecture\n",
    "   - No. of input units = No. of features\n",
    "   - No. of output units = Number of Classes (or expected targets)\n",
    "   - Select the number of hidden layers and number of neurons within each hidden layer\n",
    "1. Randomly initialize weights\n",
    "2. Implement forward propagation to get $h_{\\theta}(x^{(i)})$ for any $x^{(i)}$\n",
    "3. Implement code to compute a cost function $J(\\theta)$\n",
    "4. Implement backpropagation to compute partial derivatives $\\frac{\\delta}{\\delta\\theta_{jk}^{l}}{J(\\theta)}$\n",
    "5. Use gradient descent (or other advanced optimizer) with backpropagation to minimize $J(\\theta)$ as a function of parameters $\\theta\\$\n",
    "6. Repeat steps 2 - 5 until cost function is 'minimized' or some other stopping criteria is met. One pass over steps 2 - 5 is called an iteration or epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aM4CK1IarId4",
    "toc-hr-collapsed": false
   },
   "source": [
    "------\n",
    "# Backpropagation & Gradient Descent (Learn)\n",
    "<a id=\"p1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ktm8Fmoagcjy",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Overview\n",
    "\n",
    "Backpropagation is short for [\"Backwards Propagation of errors\"](https://en.wikipedia.org/wiki/Backpropagation) and refers to a specific (rather calculus intensive) algorithm for how weights in a neural network are updated in reverse order at the end of each training epoch. Our purpose today is to demonstrate the backpropagation algorithm on a simple Feedforward Neural Network and in so doing help you get a grasp on the main process. If you want to understand all of the underlying calculus of how the gradients are calculated then you'll need to dive into it yourself, [3Blue1Brown's video is a great starting place](https://www.youtube.com/watch?v=tIeHLnjs5U8). I also highly recommend this Welch Labs series [Neural Networks Demystified](https://www.youtube.com/watch?v=bxe2T-V8XRs) if you want a rapid yet orderly walk through of the main intuitions and math behind the backpropagation algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXI2tEO9gcjy"
   },
   "source": [
    "### What is a Gradient?\n",
    "\n",
    "> In vector calculus, the gradient is a multi-variable generalization of the derivative. \n",
    "\n",
    "The gradients that we will deal with today will be vector representations of the derivative of the activation function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review: A basic derivative \n",
    "\n",
    "![](https://ginsyblog.files.wordpress.com/2017/02/derivativelimitdef.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients: Multi-dimensional derivatives\n",
    "\n",
    "![](data:image/png;base64,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)\n",
    "\n",
    "\n",
    "\n",
    "Because a derivative can have a component in multiple dimensions, we define a gradient as a multi-dimensional derivative that takes on the form of a vector. Why a vector? Because gradients have both direction and magnitude. \n",
    "\n",
    "**In short, gradients point in the direction of greatest change.**\n",
    "![](https://i.stack.imgur.com/OI6Gy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent Formula \n",
    "This is a general formula for standard Gradient Descent. \n",
    "![](https://media.geeksforgeeks.org/wp-content/uploads/20200611183120/1406-7.png)\n",
    "\n",
    "There are more sophisticated version of gradient descent commonly referred to as [**Adaptive Gradient Descent**](https://ruder.io/optimizing-gradient-descent/). \n",
    "\n",
    "\n",
    "These adaptive models all build off of this simple equation, so it's best to first understand the standard form of Gradient Descent, then move on to more sophisticated versions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometry of Gradient Descent \n",
    "\n",
    "![](https://i.stack.imgur.com/yk1mk.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convex vs. Non-Convex\n",
    "\n",
    "Loss curves won't always have a single minimum point for gradient descent to converge towards. Sometimes there are multiple minimums. \n",
    "\n",
    "**Global Minimum:** The absolute minimum location of a curve (or surface). \n",
    "\n",
    "**Local Minimum:** The local minimum location of a curve (or surface). \n",
    "\n",
    "### In 2-Dimensions\n",
    "![](https://lh3.googleusercontent.com/o0J1qW2PhvrgsPbYzKgnuNGDyjZF7wug3OBwDPwY5LXD0Vjg3t3otN6ecZ64K8J62sNonpvZxzKTs0pMr9YniDUmQC5J-IFXmSNvRJTbxr9kyAfNP-_A7HdC8hEa9x1dDgnf9jSp)\n",
    "\n",
    "\n",
    "### In 3-Dimensions \n",
    "In Calculus, those ambiguous points that take on the form of both local mins and local maxs are known as [**Saddle points**](https://en.wikipedia.org/wiki/Saddle_point). It's not necessary to dive into the mathematics, the key take away is that non-convex error curves (and surfaces) have this global/local minimum issue. \n",
    "\n",
    "![](https://www.oreilly.com/radar/wp-content/uploads/sites/3/2019/06/convex-non-convex-9c8cb9320d4b0392c5f67004e8832e85.jpg)\n",
    "\n",
    "\n",
    "**Take Away:** The issue is that you might think that gradient descent has converged toward a global minimum but it might actually be stuck in a local minimum. \n",
    "\n",
    "There are at least 2 possible solutions to this problem: \n",
    "\n",
    "1) Use different appraoches to randomly initalizing your model weights\n",
    "For this check out [Keras's docs on Weight Initializers](https://keras.io/api/layers/initializers/). Treat these weight initializers as just another hyper-parameter to include in your gridsearch. It's a good idea to get into the practice of including these in your gridsearches. \n",
    "\n",
    "\n",
    "2) Use non-gradient descent optimizers such as [Particle Swarm](https://en.wikipedia.org/wiki/Particle_swarm_optimization) or [Genetic Algorithms](https://en.wikipedia.org/wiki/Genetic_algorithm). Feel free to read up on these appraoches but know that **you are not expected to know these appraoches** and they are outside the scope of this course. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZY66kiUgcjz",
    "toc-hr-collapsed": true
   },
   "source": [
    "------\n",
    "## Follow Along\n",
    "\n",
    "In this section, we will again build a simple neural network using base TensorFlow. We'll focus on using a __Feed Forward Neural Network__ to predict test scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4d4tzpwO6B47"
   },
   "source": [
    "### Generate some Fake Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ERyVgeO_IWyV"
   },
   "outputs": [],
   "source": [
    "# ploting \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# dataset iimport\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# tensorflow imports for building \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "ERyVgeO_IWyV",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "acd731cd43e78a23018666747114a6ed",
     "grade": false,
     "grade_id": "cell-13ede96854baf6e5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Imagine that our data is drawn from a linear function\n",
    "\n",
    "# linear regression assumes normally distributed data \n",
    "\n",
    "# y_hat =  x * w  + b  + error/noise\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bCJesGEUgcj4"
   },
   "source": [
    "### Loss Function\n",
    "Here we will use Mean Squared Error (MSE), because this is a regression problem. We are trying to predict a continuous target.\n",
    "\n",
    "![](https://miro.medium.com/max/808/1*-e1QGatrODWpJkEwqP4Jyg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(target_y, predicted_y, model, reg_strength=0.0):\n",
    "    \"\"\"\n",
    "    Implements Mean Square Error (MSE) as the loss function\n",
    "    \"\"\"\n",
    "    return tf.reduce_mean(tf.square(target_y - predicted_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgTf6vTS69Sw"
   },
   "source": [
    "### Neural Network Architecture\n",
    "Lets create a Neural Network class called \"Model\" to contain this functionality. Note: This is essentially a linear regression whose coefficients are trained by gradient descent. In practice, gradient descent works on much more complex function like the multi-layer networks we constructed yesterday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RUI8VSR5zyBv"
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.W = tf.Variable(8.0)\n",
    "        self.b = tf.Variable(40.0)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # returns m*x + b \n",
    "        return self.W * x + self.b\n",
    "\n",
    "model = Model()\n",
    "\n",
    "assert model(3.0).numpy() == 64.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gbyT_FJ88IlK"
   },
   "source": [
    "### Initial Weights\n",
    "The initial weights in our model were arbitrary. In practice, weights are initialized randomly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "IreIDe6P8H0H",
    "outputId": "7d8d53d6-b056-477a-ece9-6732702a6338"
   },
   "outputs": [],
   "source": [
    "plt.scatter(inputs, outputs, c='b', label = \"data\")\n",
    "plt.scatter(inputs, model(inputs), c='r', label = \"model predictions\")\n",
    "plt.legend()\n",
    "plt.show();\n",
    "\n",
    "print('Current loss: %1.6f' % loss(model(inputs), outputs, model).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16Ujj6vNYQyX",
    "toc-hr-collapsed": true
   },
   "source": [
    "### Update Weights Based on Gradient\n",
    "\n",
    "> *Assigning blame for bad predictions and delivering justice - repeatedly and a little bit at a time*\n",
    "\n",
    "You should also know that with neural networks it is common to have gradients that are not convex (like what we saw when we applied gradient descent to linear regression). \n",
    "\n",
    "Due to the high complexity of these models and their nonlinearity, it is common for gradient descent to get stuck in a local minimum, but there are ways to combat this:\n",
    "\n",
    "1) Stochastic Gradient Descent\n",
    "\n",
    "2) More advanced Gradient-Descent-based \"Optimizers\" - See Stretch Goals on assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "ZgaGD6YlHoid",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e49d40d84aa2cec0b7a5a207a4cd4a15",
     "grade": false,
     "grade_id": "cell-100d1b1df12abe63",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    " def train(model, inputs, outputs, learning_rate):\n",
    "    with tf.GradientTape() as t: \n",
    "        \n",
    "        \n",
    "    # calculate the loss/error value from our model's predictions\n",
    "        \n",
    "    # calculate the gradient of the loss function wrt to W and wrt B \n",
    "    \n",
    "    # update the value of W using the lr * the rate of change of the loss function wrt W \n",
    "    \n",
    "    # update the value of b using the lr * the rate of change of the loss function wrt b        \n",
    "        \n",
    "        \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iziWWURgck8"
   },
   "source": [
    "### Train the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4zn_HgFuHhTr",
    "outputId": "ee89f8a9-798e-428a-b6a9-08f5a364ffea"
   },
   "outputs": [],
   "source": [
    "model = Model()\n",
    "\n",
    "# Store Some history of weights\n",
    "Ws, bs = [], []\n",
    "epochs = range(15)\n",
    "\n",
    "for epoch in epochs:\n",
    "    Ws.append(model.W.numpy())\n",
    "    bs.append(model.b.numpy())\n",
    "    y_hat =  model(inputs)\n",
    "    current_loss = loss(outputs, y_hat, model)\n",
    "\n",
    "    train(model, inputs, outputs, learning_rate=0.1)\n",
    "    print('Epoch %2d: W=%1.2f b=%1.2f loss=%2.5f' % (epoch, Ws[-1], bs[-1], current_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "FSEt07wdHvi2",
    "outputId": "c41e84eb-fb5e-4ac3-c08b-e085ebf411f4"
   },
   "outputs": [],
   "source": [
    "plt.plot(epochs, Ws, 'r', epochs, bs, 'b')\n",
    "plt.plot([TRUE_W] * len(epochs), 'r--',\n",
    "         [TRUE_b] * len(epochs), 'b--')\n",
    "plt.legend(['W', 'b', 'True W', 'True b'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "6pKDfpplbUxN",
    "outputId": "826bd434-5c5f-4071-88ca-729ddca6daa9"
   },
   "outputs": [],
   "source": [
    "plt.scatter(inputs, outputs, c='b', label = \"data\")\n",
    "plt.scatter(inputs, model(inputs), c='r', label = \"model predictions\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('Current loss: %1.6f' % loss(model(inputs), outputs, model).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKUVGoRxgck_"
   },
   "source": [
    "## Challenge\n",
    "\n",
    "In the module project, you will be asked to explain the logic of backpropagation and gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTqZg-6igclA",
    "toc-hr-collapsed": true
   },
   "source": [
    "# Batch Size (Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nrm-racgclA"
   },
   "source": [
    "## Overview\n",
    "\n",
    "The What - Stochastic Gradient Descent calculates an approximation of the gradient over the entire dataset by reviewing the predictions of a random sample. \n",
    "\n",
    "The Why - *Speed*. Calculating the gradient over the entire dataset is extremely expensive computationally. \n",
    "\n",
    "### Batch Size\n",
    "Batches are the number of observations our model is shown to make predictions and update the weights. Batches are selected randomly during epoch. All observations are considered when passing thru an epoch at some point.\n",
    "\n",
    "* Smaller Batch = Slower Run Time (but maybe more accurate results)\n",
    "* Default Batch = Balance between speed and accuracy\n",
    "* Large Batch = Very fast, but not nearly as accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oNQ2ZCi7I4i6"
   },
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "mZjW2lYVI9Q2",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a1b0af5b2611f98d63f8bb6b9427bb93",
     "grade": false,
     "grade_id": "cell-1c90a81f1eece31b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "7a5f9379-453d-459c-d5d0-5194a19070a8"
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "# scale data\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing data helps your model learn\n",
    "\n",
    "Whenever all data is normalized to values within 0 and 1, that ensures that the update to all the weights are updated in equal proportions which can lead to quicker convergence on the optimal weight values. \n",
    "\n",
    "**Hint:** if your dataset's values range accross multiple orders of magnitude (i.e. $10^1,~~10^2,~~10^3,~~10^4$), then gradient descent will update the weights in grossly uneven proportions.  \n",
    "\n",
    "\n",
    "![](https://quicktomaster.com/wp-content/uploads/2020/08/contour_plot.png)\n",
    "\n",
    "There's more to be said about Normalization and Gradient Descent, however there's not enough time! So I highly encourage you to [**read throught this very well written article that explores the impact of normalization on Gradient Descent in much greater detail.**](https://www.jeremyjordan.me/batch-normalization/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "o7x17kDKJSy5",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e7d8340907fc4785bc1f1335632de37c",
     "grade": false,
     "grade_id": "cell-38ed3365b403af52",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# create a function called create_model that accepts a learing rate for SGD as an input parameter \n",
    "# it should return a complied, 2 hidden layer neural net that uses SGD as the optimizer \n",
    "# Import SGD as discussed here: https://keras.io/api/optimizers/sgd/\n",
    "\n",
    "# create create_model\n",
    "def create_model(lr=.01):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr: float\n",
    "        Learing rate parameter used for Stocastic Gradient Descent \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    model: keras object \n",
    "        A complied keras model \n",
    "    \"\"\"\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W-HsAQ-9jgUM",
    "outputId": "a26a7ff3-18f7-49a8-8af3-9f9bc29ee119"
   },
   "outputs": [],
   "source": [
    "create_model().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZF7UE-KluPsX"
   },
   "source": [
    "## Follow Along\n",
    "Let's run a series of experiments for a default, small, and large batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VhpDaVFRJl3U"
   },
   "source": [
    "### Default\n",
    "Batch Size is 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P-ChVGikgclD",
    "outputId": "d5012af3-4ead-4612-c62d-e58fbf2312ae"
   },
   "outputs": [],
   "source": [
    "# instantiate a model and fit it with batch size of 32\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KvsbOFnDJuG0"
   },
   "source": [
    "### Small Batch Size\n",
    "Batch Size is 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "diDzvb-UJ1je",
    "outputId": "caf5ead4-ece1-4f30-e357-6fa0753e0689"
   },
   "outputs": [],
   "source": [
    "# instantiate a model and fit it with batch size of 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_iPvvvt5J2Xl"
   },
   "source": [
    "### Large Batch Size\n",
    "Batch Size is 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7h8Z5293KABT",
    "outputId": "4eb4ea99-a5c3-4451-f61c-a71ea4d65042"
   },
   "outputs": [],
   "source": [
    "# instantiate a model and fit it with batch size of 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0ujUz6BKUGz"
   },
   "source": [
    "### Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "o-5DOZNMKYt-",
    "outputId": "05813451-6c69-4ac5-e28d-166dcdd1dd5b"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "batch_sizes = []\n",
    "\n",
    "for exp, result in zip([bt_default, bt_small, bt_large], [\"32_\", \"8_\", \"512_\"]):\n",
    "\n",
    "    df = pd.DataFrame.from_dict(exp.history)\n",
    "    df['epoch'] = df.index.values\n",
    "    df['Batch Size'] = result\n",
    "\n",
    "    batch_sizes.append(df)\n",
    "\n",
    "df = pd.concat(batch_sizes)\n",
    "df['Batch Size'] = df['Batch Size'].astype('str')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "Dlg9uSEEmIJB",
    "outputId": "f5913781-2944-4b3f-c8e3-ad15bd595538"
   },
   "outputs": [],
   "source": [
    "sns.lineplot(x='epoch', y='val_accuracy', hue='Batch Size', data=df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "94bJYgz3nkp0",
    "outputId": "8c214962-52ba-4cd9-b714-9e6b4758acb4"
   },
   "outputs": [],
   "source": [
    "sns.lineplot(x='epoch', y='val_loss', hue='Batch Size', data=df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kZ2vUYYgclS"
   },
   "source": [
    "## Challenge\n",
    "\n",
    "You will be expected to experiment with batch size on today's assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46cP9Pm_gclS"
   },
   "source": [
    "# Learning Rate (Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.jeremyjordan.me/content/images/2018/02/Screen-Shot-2018-02-24-at-11.47.09-AM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bna67ADZgclT",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Overview\n",
    "\n",
    "Learning Rate controls the size of the update to our weights that the optimization algorithm makes. VERY IMPORTANT hyperparameter.\n",
    "\n",
    "* Too high of a learning rate causes unstable results\n",
    "* Too Low of a learning rate the model will underfit\n",
    "* Goldy Locks parameters - it needs be \"just right\"\n",
    "* Scale of 0-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsVYOn7bgcle",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Follow Along\n",
    "\n",
    "Same experiment with Batch but different learning rates:\n",
    "* High Learning = .75\n",
    "* Default Learning = .01\n",
    "* Low Learning Rate = .0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CI_H8Em1NOii"
   },
   "source": [
    "### Default Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Se8cb_ZUNVtL"
   },
   "outputs": [],
   "source": [
    "# instantiate a model and fit it with a learning rate value of 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQZ4SZdKNMRO"
   },
   "source": [
    "### High Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ny72mU_dNWMR"
   },
   "outputs": [],
   "source": [
    "# instantiate a model and fit it with a learning rate value of 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAqDmTVBNSMR"
   },
   "source": [
    "### Low Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ech1ER64NXBn",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "8e359ae7-baf4-4c5d-ea53-4172741e7569"
   },
   "outputs": [],
   "source": [
    "# instantiate a model and fit it with a learning rate value of 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZe6DyhANXdU"
   },
   "source": [
    "### Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "Bn-BdFdMNph-",
    "outputId": "0600ccfa-7a78-4aaa-c1a5-7087c21c31e4"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "batch_sizes = []\n",
    "\n",
    "for exp, result in zip([lr_default, lr_low, lr_high], [\".01_\", \".0001_\", \".75_\"]):\n",
    "\n",
    "    df = pd.DataFrame.from_dict(exp.history)\n",
    "    df['epoch'] = df.index.values\n",
    "    df['Learning Rate'] = result\n",
    "\n",
    "    batch_sizes.append(df)\n",
    "\n",
    "df = pd.concat(batch_sizes)\n",
    "df['Learning Rate'] = df['Learning Rate'].astype('str')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "QgDX7htLpTlQ",
    "outputId": "48537fa8-39a3-4853-ec92-5d8f684df907"
   },
   "outputs": [],
   "source": [
    "sns.lineplot(x='epoch', y='val_loss', hue='Learning Rate', data=df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "D8GPXqf_qGs9",
    "outputId": "4a105d15-cd8c-4fb2-a3c9-16ecd7e4084c"
   },
   "outputs": [],
   "source": [
    "sns.lineplot(x='epoch', y='val_accuracy', hue='Learning Rate', data=df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kb2aiw_Sgcl7"
   },
   "source": [
    "## Challenge\n",
    "\n",
    "You will be expected to experiment with different learning rates today.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GszSNVwUrmXy"
   },
   "source": [
    "# Bonus: How do I know if my neural net is overfitting ?\n",
    "\n",
    "Compare train & test losses (or learning metric like accuracy) and look for the gap between the curves. \n",
    "\n",
    "\n",
    "See [**this resource**](https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/) for further details "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "eP5u3_ZWr3Mn",
    "outputId": "257570fb-8c60-4662-86ab-1b97082d2e02"
   },
   "outputs": [],
   "source": [
    "sns.lineplot(x='epoch', y='val_loss', data=df[df['Learning Rate']=='.01_'], label=\"test_loss\")\n",
    "sns.lineplot(x='epoch', y='loss', data=df[df['Learning Rate']=='.01_'], label=\"train_loss\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "aBt1q68UsHoq",
    "outputId": "52398826-906c-43e3-af57-acd00953f7d0"
   },
   "outputs": [],
   "source": [
    "sns.lineplot(x='epoch', y='val_accuracy', data=df[df['Learning Rate']=='.01_'])\n",
    "sns.lineplot(x='epoch', y='accuracy', data=df[df['Learning Rate']=='.01_']);"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_422_Train_Lecture.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
