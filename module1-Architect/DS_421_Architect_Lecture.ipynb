{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "DS_421_Architect_Lecture.ipynb",
   "provenance": [
    {
     "file_id": "1TfLcPv5n-9ukGDWL5KMHS-gQSbOeA046",
     "timestamp": 1630635000354
    }
   ],
   "collapsed_sections": [
    "esemlZKH0ueQ",
    "4bG2tO3M0uen",
    "E2uVSeYHiPMK",
    "QBYQ8-g7jdyT",
    "9lbw3rtjprqC",
    "-9x3x5XgtD3i",
    "Q2d6IwFNFEnx",
    "z8gb9aN9prqD",
    "rEThlcbkEHHT",
    "doDjyANkEmcG",
    "EHzykedbprqD",
    "ibksGlxedU1p",
    "AW5rujQ4DWkv",
    "NhmkTB0HGTnH",
    "EULJBiUZDWlT",
    "WaMLtERVRLWb"
   ],
   "private_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iAJaRSseDCrU"
   },
   "source": [
    "*Unit 4, Sprint 2, Module 1*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3nUN1Sb0ueP",
    "toc-hr-collapsed": true
   },
   "source": [
    "# Architect (Prepare)\n",
    "__*Neural Network Foundations*__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "esemlZKH0ueQ"
   },
   "source": [
    "## Learning Objectives\n",
    "* <a href=\"#p1\">Part 1</a>: Student should be able to describe the foundational components of a neural network\n",
    "* <a href=\"#p2\">Part 2</a>: Student should be able to introduce the Keras Sequential Model API\n",
    "* <a href=\"#p3\">Part 3</a>: Student should be able to learn how to select a model architecture\n",
    "\n",
    "Neural Networks are the most powerful modeling techniques that we possess in machine learning today. In spite of the hype surrounding these topics, we hope that you will come to see them as just another tool in your tool bag with their own strengths and weaknesses. They are useful, but they are not a silver bullet, and they are not always preferable to other -- perhaps more simple -- machine learning methods. \n",
    "\n",
    "The goal of this week is to familiarize you with the fundamental theory, terminology and libraries that will enable you to build and use neural network architectures. This week will not be a run-through of the history of Neural Networks and each of the individual advancements leading up to current technologies -- we don't have time for that. We will spend some time on some older methods, but only to the degree that they will help introduce us to relevant terminology and understand more complex versions of these technologies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dGb0yyBtBCBD",
    "toc-hr-collapsed": false
   },
   "source": [
    "# Foundational Neural Network Components (Learn)\n",
    "<a id=\"p1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bG2tO3M0uen"
   },
   "source": [
    "## Overview\n",
    "\n",
    "### Major Components\n",
    "- Neurons\n",
    "- Weight and Bias Parameters\n",
    "- Activation Function\n",
    "- Loss Function\n",
    "- Layers: collections of neurons with the same inputs\n",
    "\n",
    "\n",
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/LambdaSchool/DS-Unit-4-Sprint-2-Neural-Networks/main/module1-Architect/IMG_0167.jpeg\" width=400></center>\n",
    "\n",
    "\n",
    "#### Let's zoom in on the first of these components, the neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cEMHWi_Oprp5"
   },
   "source": [
    "----\n",
    "\n",
    "## 1. Dissecting the Artificial Neuron (a.k.a Perceptron)\n",
    "\n",
    "Every branch of science has a fundamental unit, a baseline model of a physical system, that is used as the starting point (the first principle) of that science. Every idea, experiment result, and hypothesis in a branch of science rests upon the building block of that science; unless you're doing purely theoretical work that is explicitly looking to introduce a new building block, or challenge the first priciples. So it's important that we understand the builidng block of any science that we wish to study. \n",
    "\n",
    "In **Physics**, the fundamental building block is the **particle**.\n",
    "\n",
    "In **Chemistry**, the fundamental building block is the **chemical element**. \n",
    "\n",
    "In **Biology**, the fundamental building block is the **cell**.\n",
    "\n",
    "In **Neuroscience**, the fundamental building block is the **Neuron**.\n",
    "\n",
    "**You are about to learn computational Neuronscience!** \n",
    "\n",
    "This image has a side-by-side comparison of a biological model of a neuron and a computational model of a neuron.\n",
    "![](https://miro.medium.com/max/610/1*SJPacPhP4KDEB1AdhOFy_Q.png)\n",
    "\n",
    "\n",
    "Let's dive deeper into the computational model of the neuron. \n",
    "![](https://images.squarespace-cdn.com/content/v1/59d9b2749f8dce3ebe4e676d/1547561883197-ZO8CJILFNGZMORIJZOJ1/ke17ZwdGBToddI8pDm48kAuxETKhxDsgKuKi-UGpnEIUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcofcaXKJYLRX9EzsQWIgTsTayKHY9LJ3-BRv5jWxoI5y-JkyZtODn1AMLPOg8sn20/Artificial-Neuron.png )\n",
    "\n",
    "**Note:** The $\\mathbf{^{T}}$ superscript on the $\\mathbf{w}$ weight vector stands for a vector/matrix transpose. Sometimes these are necessary in order to get the dimensions of a vector or matrix product to align so that a valid product can take place. \n",
    "\n",
    "**This fundamental equation describes how an artificial neuron combines multiple inputs to compute a single output!**\n",
    "\n",
    "This is the neuron in a single equation, all the relevant terms are present. This equation will continue to reappear as we continue our study of various neural network architectures, as well as the techniques for training neural networks such as **gradient descent** and **back-propagation**. \n",
    "\n",
    "\n",
    "$${y = f(\\mathbf{w^T} \\mathbf{x} + b )}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-iI8cgoTFGYt"
   },
   "source": [
    "In this equation, $\\textbf{f}$ is the **activation function** of the neuron <br>\n",
    "$\\textbf{x}$ is a column vector of the inputs to the neuron (i.e. features).<br>\n",
    "The neuron is characterized by a column vector of weights $\\textbf{w}$, and a bias $b$<br>\n",
    "The expression $\\textbf{w}^{\\textbf{T}}\\textbf{x}+b$ is the **signal** from the neuron. <br> \n",
    "Here, $\\textbf{w}^{\\textbf{T}}$ is the transpose of the weights vector -- recall that the transpose of a column vector is a row vector. <br>\n",
    "The signal is a weighted average of the inputs $\\{x_{1}, x_{2},...,x_{n}\\}$ plus a bias term $b$:\n",
    "\n",
    "`signal = weight1*input1 + weight1*input2 + weight3*input3 + bias`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ziqLNMhMprp7"
   },
   "source": [
    "$${y} = f(w_0  x_0 + w_1   x_1 + w_2   x_2 + b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udeS5_wSprp7"
   },
   "source": [
    "----\n",
    "## 2. Mathematics of vector-vector and matrix-vector dot products\n",
    "In this section we'll review vector-vector and matrix-vector multiplication <br>\n",
    "As an application, we'll develop the mathematical machinery to create a `perceptron` model and understand its operation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2uVSeYHiPMK"
   },
   "source": [
    "### 2.1 First let's define some terms and notation!\n",
    "\n",
    "Let $\\mathcal {X}$ be defined as the set of all vectors in an ${N}$-dimensional vector space denoted as $\\mathbb{R}^{N}$ \n",
    "\n",
    "Let $\\mathbf {x}$ be a vector $\\in \\mathcal {X}$<br>here, $ \\in $ means \"is a member of\"\n",
    "\n",
    "\n",
    "Let $\\mathcal {W}$ be defined as the set of all weight vectors in $\\mathbb{R}^{N}$  \n",
    "\n",
    "Let $\\mathbf {w}$ be a vector $\\in \\mathcal {W}$\n",
    "\n",
    "Let ${b}$ be a scalar on the real line, denoted by $\\mathbb{R}^{1}$, or just $\\mathbb{R}$<br>\n",
    "\n",
    "In all the work to follow in this course, we will strive to follow standard notation:\n",
    "* *matrix* variables by bold capital letters, i.e. $\\textbf{X}$\n",
    "* *vector* variables by bold lower case letters, i.e. $\\textbf{w}$\n",
    "* *scalar* variables by plain lower case letters, i.e. $b$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkdxrFNTMKqi"
   },
   "source": [
    "### 2.2 Representing a neuron's input and parameters<br>\n",
    "We'll specialize to a toy example in two dimensions.<br>\n",
    "Suppose we have 3 examples (data points) $\\{ \\textbf{x}_{1}, \\textbf{x}_{2},\\textbf{x}_{3} \\}$, each of which is a two dimensional row vector. <br>\n",
    "We can stack these input vectors into a $3 \\text{x} 2$ input matrix $\\textbf{X}$ <br>\n",
    "The neuron's weights vector $\\textbf{w}$ always has the same dimension as the input vector, so it is also two dimensional.<br>\n",
    "The neuron's bias $b$ is a scalar.<br>\n",
    "Let's create code for our toy example:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ynroOTF_prp8"
   },
   "source": [
    "# Although the math holds for any feature dimensionality n, \n",
    "#     let's keep things simple and start with n = 2 dimensional input vectors\n",
    "import numpy as np\n",
    "\n",
    "# define our 2-dim input vectors and input matrix \n",
    "x1 = np.array([10, 20])\n",
    "x2 = np.array([-10, -20])\n",
    "x3 = np.array([100, 111])\n",
    "\n",
    "# stack the input vectors to form the input matrix\n",
    "X = np.array([x1, \n",
    "              x2, \n",
    "              x3])\n",
    "\n",
    "# define our 2-dim weight vector\n",
    "w = np.array([0.2, 0.4]) \n",
    "\n",
    "# define our bias term \n",
    "b = 1"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EsEa6LeAVN8b"
   },
   "source": [
    "Input matrix $\\textbf{X}$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XR-HFNT2prp9"
   },
   "source": [
    "X"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fD_Id3hIVTjQ"
   },
   "source": [
    "Weights vector $\\textbf{w}$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "H4CVpj29prp-"
   },
   "source": [
    "w"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqtF9tZrVa09"
   },
   "source": [
    "Scalar bias $b$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qD5SdD55prp-"
   },
   "source": [
    "b"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SS09o4dwiBEy"
   },
   "source": [
    "###2.3 The dot product of two vectors\n",
    "*The dot product of two vectors is the sum of the products of their corresponding components:*<br><br>\n",
    "Example: Let $\\textbf{a} = [1, 2, 3]$ and $\\textbf{b} = [5, 6, 7]$  <br><br>\n",
    "Then\n",
    "$\\textbf{a} \\cdot \\textbf{b} = 1\\times5 + 2\\times6 + 3\\times7 = 5 + 12 + 21 = 38$<br><br>\n",
    "Note that the dot product of two vectors is just a single number, i.e. a **scalar**<br>\n",
    "In `python` we can compute the dot product like so:<br> \n",
    "`numpy.dot(a,b)`\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "S-H1P5bAprp-"
   },
   "source": [
    "# to compute the dot product between vectors a and b:\n",
    "# multiply element-wise, then sum\n",
    "\n",
    "a = np.array([1,2,3])\n",
    "b = np.array([5,6,7])\n",
    "\n",
    "np.dot(a,b)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2Ci6j58prp_"
   },
   "source": [
    "A dot product between an input vector $\\mathbf x$ and a weight vector $\\mathbf w$ <br>can be thought of as a *weighted sum* of the input vectors's components:\n",
    "\n",
    "$${\\displaystyle \\mathbf {w} \\cdot \\mathbf {x} }~~=~~{\\displaystyle \\sum _{i=1}^{m}w_{i}x_{i}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sii0frHoRu4_"
   },
   "source": [
    "### 2.4 A bit of `pythonic` weirdness explained \n",
    "\"one-dimensional vector\" vs. <br>\n",
    "\"two-dimensional row vector\" vs.<br>\n",
    "\"two-dimensional column vector\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1Fu5C7yRxFH6"
   },
   "source": [
    "# one dimensional vector\n",
    "print(f'w looks like this {w} and is a one-dimensional vector and has shape {w.shape}\\n')\n",
    "\n",
    "# reshape w to a two-dimensional row vector!\n",
    "print(f'w.reshape(1,2) looks like this {w.reshape(1,2)} and is a two-dimensional array, a row vector (1 row,2 columns) with shape {w.reshape(1,2).shape}\\n')\n",
    "\n",
    "# check out this strange alternative way to reshape!\n",
    "print(f'w[None,:] also looks like this {w[None,:]} and is a two-dimensional array, a row vector with shape {w[None,:].shape}\\n')\n",
    "\n",
    "# reshape w to a two-dimensional column vector!\n",
    "print(f'w.reshape(2,1) looks like this\\n {w.reshape(2,1)} \\n and is a two-dimensional array, a column vector, with shape {w.reshape(2,1).shape}\\n')\n",
    "\n",
    "# check out this strange alternative way to reshape!\n",
    "print(f'w[:,None] also looks like this \\n {w[:,None]} \\n and is a two-dimensional array, a column vector, with shape {w[:,None].shape}\\n')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NATq4kKYiSU5"
   },
   "source": [
    "### 2.5 The dot product of a matrix and a vector\n",
    "\n",
    "If $\\textbf{X}$ is an $\\text{m} \\times \\text{n}$ matrix and $\\textbf{w}$ is an $\\text{n} \\times \\text{1}$ column vector, <br>\n",
    "then the matrix-vector product $\\textbf{X}\\cdot\\textbf{w}$ is an $\\text{m} \\times \\text{1}$ column vector<br>\n",
    "whose entries are the dot products of each row of $\\textbf{X}$ with $\\textbf{w}$\n",
    "\n",
    "Note that the *first* and *second* dimensions of the matrix-vector product are:<br> the *first* dimension of the matrix and the *second* dimension of the vector<br>\n",
    "Here $m$ refers to the number of examples and $n$ refers to the number of features, i.e. the dimensionality of each input example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8jFNq_S4Xo4z"
   },
   "source": [
    "#### 2.5.1 The `numpy.dot()` method is *overloaded* to handle the product of a matrix with a vector\n",
    "Caveat: when the `.dot` method is used this way, the result is a vector, not a scalar."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kn62L1lNprp_"
   },
   "source": [
    "# The dot product of matrix X with column vector w\n",
    "np.dot(X,w)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5Q52-AIXv4J"
   },
   "source": [
    "####2.5.2 Alternatively, we can use `python`'s  `@` operator for matrix muliplication. <br>\n",
    "To compute the product of two matrices `A@B`, we must make sure that `A` and `B` have the proper shapes. <br>\n",
    "For example to compute `X@w`, we must cast (reshape) w as a two-dimensional column array. <br>\n",
    "Note that the result vector is the same list of numbers as the previous answer."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MOmAczGycha8"
   },
   "source": [
    "X@w[:,None]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHHBWvq-yOYw"
   },
   "source": [
    "## 3. A `perceptron` is a mathematical model of the neuron\n",
    "A neuron takes in a number of inputs through its dendrites. Depending on its sensitivity to each of the inputs, the neuron responds by either firing or not firing along the axon. We can model this process in code! <br><br>\n",
    "The inputs to the neuron are real numbers, and the neuron generates an output that is either $\\text{ON}$ (fires) or $\\text{OFF}$ (doesn't fire). <br>\n",
    "\n",
    "The inputs for a single example can be described by a vector $\\textbf{x}$. <br>\n",
    "The neuron has internal weights $\\textbf{w}$  describing its sensitivity to each input and also a bias $b$ describing its overall sensitivity.<br>\n",
    "\n",
    "We can think of the neuron as performing three steps to generate its binary output. <br><br>\n",
    "### Step 1: the neuron combines the inputs linearly to produce a response<br><br>\n",
    "$Z = \\textbf{w} \\cdot \\textbf{x} + b$. <br><br>\n",
    "Here, $Z$ is the response, a real number that can be positive or negative: $Z \\in [-\\infty,+ \\infty]$<br><br>\n",
    "The neuron ultimately needs to convert the response to a decision <br>whether or not to fire, i.e. to a binary output of either $0$ or $1$. Hence<br><br>\n",
    "### Step 2: the neuron \"squishes\" $Z$ down to a number between $0$ and $1$. <br>\n",
    "We'll need a special function $S$ to do this job for us.<br>\n",
    "For now, let's think about what we want $S$ to do: <br><br>\n",
    "$S(Z)$ maps any real number $Z$ onto the unit interval, so that <br><br>$S(Z)=z$, where $z \\in [0,1]$ <br><br>\n",
    "We'll have more to say about $S$ in a bit.<br><br>\n",
    "### Step  3: the neuron maps the number $z$ to a binary $\\text{output} \\in \\{0,1\\}$<br><br>\n",
    "How would the neuron do this? <br>\n",
    "We can imagine applying a threshold (say $0.5$) to $z$, producing a result <br><br>\n",
    "$\\text{output} = (z > 0.5) = { \\begin{cases}1& {\\text{if }}\\  {z} >0.5\\\\ 0& {\\text{if }}\\  {z} \\le 0.5\\\\ \\end{cases}} $<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBYQ8-g7jdyT"
   },
   "source": [
    "### 3.1 Summary of the `perceptron` model\n",
    "1. Map the vector input $\\textbf{x}$ to a linear response $Z = \\textbf{w} \\cdot \\textbf{x} + b$\n",
    "2. Map $Z$ to a number $z \\in [0,1]$\n",
    "3. Apply a threshold to $z$ to produce a binary output of either $1$ or $0$<br><br>\n",
    "\n",
    "The `perceptron` output ($0$ or $1$) can be expressed in terms of \n",
    "* the inputs $\\textbf{x}$,<br>\n",
    "* the neuron's internal parameters (weights $\\textbf{w}$ and bias $b$)\n",
    "* the activation function $S$<br><br>\n",
    "$\\text{output} = S(\\textbf{w} \\cdot \\textbf{x} + b) > 0.5$<br><br>\n",
    "where $S$ is the $\\text{sigmoid}$ function. <br><br>\n",
    "\n",
    "We are now close to a complete understanding of the `perceptron`!<br> The only missing piece is a discussion of the **sigmoid** function $\\textbf{S}$, which follows in the next sections.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m19Foy2Gpbk-"
   },
   "source": [
    "### 3.2 Linear Response of a neuron\n",
    "A neuron's *linear response* is the weighted sum of the inputs plus the bias:<br><br>\n",
    "$Z = \\textbf{w} \\cdot \\textbf{x} + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynJobpvzajay"
   },
   "source": [
    "Let's code up the linear response function."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "657H-FFApsRX"
   },
   "source": [
    "def Z(X,w,b):\n",
    "  # combine the inputs, weights and bias to compute the linear response of the neuron\n",
    "  \n",
    "  # reshape w into a columnn vector, of shape [len(w), 1]\n",
    "  w=w.reshape((len(w),1))\n",
    "\n",
    "  return X@w + b"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6bAbEzwVashU"
   },
   "source": [
    "Try it out and and see what $Z$ looks like. <br>\n",
    "Note that the output of a neuron given an input data point (or example) is a single number. <br>\n",
    "Here Z is a column vector, giving the neuron outputs for each of the three input examples."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "o8kyEE5r9ulF"
   },
   "source": [
    "Z(X,w,b)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYJsQ3emjRML"
   },
   "source": [
    "### 3.3 The [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function)\n",
    "The sigmoid function $$S(x) = \\frac{1}{1+exp(-x)}$$ <br>\n",
    "does the job we referred to earlier: <br>\n",
    "$S$ maps a real number $x \\in [-\\infty,+\\infty]$ onto the unit interval $[0,1]$.<br><br>\n",
    "The sigmoid is an example of an *activation function* -- which is a nonlinear function that is applied to a neuron's linear response $Z$ to compute the output. *Activation functions* are an essential component of every neural network, and there are a variety of others besides the **sigmoid**. We'll encounter other examples of *activation functions* later on.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "375bfc9e6bea83c59ac9de18a0380866",
     "grade": false,
     "grade_id": "cell-eff2dc3416d41e68",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "id": "dneM1sqmprqA"
   },
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"Calculate the output of a sigmoid\n",
    "    Input can be a scalar or an array \n",
    "    If the input is an array, the output will be an array of the same size\n",
    "    whose values are the sigmoid of each input element \"\"\"\n",
    "\n",
    "    return 1 / (1 + np.exp(-x))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hrtnRzmv0Ara"
   },
   "source": [
    "sigmoid(11)\n",
    "sigmoid(0)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XuA_CbdHprqA"
   },
   "source": [
    "sigmoid(Z(X,w,b))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cj6NqdNYprqB"
   },
   "source": [
    "$$\\textbf{Sigmoid formula}$$\n",
    "$${\\displaystyle S(x)={\\frac {1}{1+e^{-x}}}}$$\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/1200px-Logistic-curve.svg.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nf3ju6KWxtWs"
   },
   "source": [
    "### 3.4 Exercise: Familiarize yourself with the sigmoid function\n",
    "1. With the code above, try out a few examples, computing the sigmoid function applied to large and small positive and negative numbers.<br>\n",
    "Convince yourself that the sigmoid function really does have property of \"squishing\" the real line into the interval $[0,1]]$ as shown in the above plot.<br>\n",
    "2. Without coding, compute the following: <br>\n",
    "$S(0) = $ ?<br>\n",
    "$S(-\\infty) = $ ?<br>\n",
    "$S(+\\infty) = $ ? \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "k3_2VGbDxn3o"
   },
   "source": [
    "# YOUR ANSWER HERE \n",
    "# hint np.exp(0) = e**0 = 1"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNYQH8Y7CARR"
   },
   "source": [
    "### 3.5 The output of the neuron!\n",
    "We can now compute the final outputs of the neuron for out data set of three examples <br>by applying a *threshold* to the sigmoid result, producing a boolean value.<br>\n",
    "If we set the threshold value to $0.5$, then we get an output of $1$ for positive $Z$ values <br>\n",
    "and an output of $0$ for negative $Z$ values. <br>\n",
    "See the graph of the sigmoid function above."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "S0NmAPLMprqB"
   },
   "source": [
    "# The neuron fires whenever the linear response (input to the sigmoid) is positive\n",
    "# The neuron does not fire whenever the linear response (input to the sigmoid) is negative\n",
    "threshold = 0.5\n",
    "output = sigmoid(Z(X,w,b))>threshold\n",
    "output"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xc15YHxwBJGK"
   },
   "source": [
    "### 3.6 Implement the `perceptron` model of the neuron in code\n",
    "Congratulations! You have now understood the `perceptron` model of the neuron! <br>The `perceptron` is the simplest example of a neural network. <br><br>\n",
    "The next step: think of `perceptrons` as the $\\text{Lego}^{TM}$ bricks for building more complex neural networks<br><br>\n",
    "Let's stitch together our results so far, and code up a `perceptron` model<br><br>\n",
    "As we'll see in our code, the `perceptron` is composed of a sequence of mathematical operations, each one acting upon the result of the previous one. For that reason, `perceptrons` (and neural networks composed of them) are often referred to as **Feed-Forward Neural Networks** (FFNN), since the data \"feeds forward\", i.e. flows from left to right through the network as it is processed by successive operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f46f3fe25e6e9326649ac138dc6025c4",
     "grade": false,
     "grade_id": "cell-26e6dfe6d2fcdd5e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "id": "ybFrzYZMprqB"
   },
   "source": [
    "# implement the perceptron model\n",
    "def perceptron(w, X, b):\n",
    "    \"\"\"\n",
    "    Calculates the sigmoid of a weighted sum plus a bias term w * x + b\n",
    "    and returns a classification for the input data (i.e. a prediction)\n",
    "    \n",
    "    Returns a 1 if sigmoid output is greated than the threshold\n",
    "    Returns a 0 if sigmoid output is less than the threshold\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    w: numpy array \n",
    "        weight vector\n",
    "        \n",
    "    X: numpy 2D array\n",
    "        Input data, one row for each example \n",
    "        \n",
    "    b: scalar (i.e. constant)\n",
    "        Bias term \n",
    "        \n",
    "    Returns \n",
    "    -------\n",
    "    boolean value \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # 1. reshape w into a columnn vector, of shape [len(w), 1]\n",
    "    w = w.reshape((len(w),1))\n",
    "    \n",
    "    # 2. compute the linear response of the neuron\n",
    "    Z = X@w + b\n",
    "    \n",
    "    # 3. apply the sigmoid activation function to the linear response\n",
    "    #      to get a number between 0 and 1\n",
    "    z = sigmoid(Z)\n",
    "\n",
    "    # 4. set the threshold\n",
    "    threshold = 0.5 \n",
    "\n",
    "    # 5. apply the threshold to determine whether the or not the neuron should fire\n",
    "    # the output is 1 if the neuron fires, 0 if not\n",
    "    output =  z > threshold\n",
    "    return output\n",
    "   "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0fILmL-fprqC"
   },
   "source": [
    "# Calculate the perceptron output for the inputs, weights and bias we defined earlier\n",
    "perceptron(w, X, b)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "d9dBzzS515ES"
   },
   "source": [
    "b"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ef1M0iOfprqC"
   },
   "source": [
    "# what happens if we make the bias large and positive?\n",
    "b = 70\n",
    "perceptron(w, X, b)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "N4QlKZ5pFQJB"
   },
   "source": [
    "# what happens if we make the bias large and negative?\n",
    "b = -70\n",
    "perceptron(w, X, b)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9lbw3rtjprqC"
   },
   "source": [
    "-----\n",
    "\n",
    "### 3.7 What kind of predictive model is the perceptron?\n",
    "\n",
    "The perceptron is a **linear binary classifier** - just like Logistic Regression \n",
    "\n",
    "**Binary** in the sense that it can only distinguish between two classes in a classification task. \n",
    "\n",
    "**Linear** in the sense that it can only separate two classes that have a linear decision boundary. \n",
    "\n",
    "By combining multiple neurons into a neural network we can overcome both of these limitations. <br>\n",
    "More on this in a bit. For now, let's zoom back out and look at the big picture again. \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9x3x5XgtD3i",
    "toc-hr-collapsed": true
   },
   "source": [
    "### 3.8 Essential aspects of neural networks\n",
    "\n",
    "<img src=\"https://camo.githubusercontent.com/41ebef1f914f92a03a75d5c468c10b49d9f7ba4fb7fac455b3f9ba891953889c/68747470733a2f2f7777772e7079696d6167657365617263682e636f6d2f77702d636f6e74656e742f75706c6f6164732f323031362f30382f73696d706c655f6e657572616c5f6e6574776f726b5f6865616465722e6a7067\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2d6IwFNFEnx"
   },
   "source": [
    "### 3.8.1 What does a Neuron Do?\n",
    "A neuron is often called a \"unit\" which is shorthand for \"activation unit\". <br>\n",
    "According to the `perceptron` model, each neuron in a network calculates a weighted sum of its inputs, <br>\n",
    "adds a bias term and passes the resulting value through an activation function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8gb9aN9prqD"
   },
   "source": [
    "### 3.8.2 Layers\n",
    "A single *dense* layer is a collection of neurons. There are three common types of layers:\n",
    "- **Input** -- neurons in this layer do nothing. They simply \"pass through\", or connect, the corresponding inputs rightward to the first hidden layer.<br> Assertion: for an input layer, the weights are all $1$s, the bias is $0$, and the activation function is simply multiplication by $1$.<br>\n",
    "\n",
    "- **Hidden** -- there are one or more hidden layers, connected left to right.\n",
    "- **Output** -- the output layer computes the final output of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEThlcbkEHHT"
   },
   "source": [
    "### 3.8.3 Weights\n",
    "are *parameters* within our neural network. In the diagram above, weights are represented as arrows. During the training process, the weights are adjusted to minimize the loss function (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "doDjyANkEmcG"
   },
   "source": [
    "### 3.8.4 Bias\n",
    "is a trainable *parameter* within our neural network. The bias term is a constant allowing greater flexibility in the output of a neuron. During training the bias is adjusted along with the weights to minimize the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHzykedbprqD"
   },
   "source": [
    "### 3.8.5 Loss Function\n",
    "The *loss function* measures the error: how close your model predictions are to the target values.<br> \n",
    "The *derivative* (or *slope*, or *gradient*) of the loss function informs you how you should update the weights and biases in order to decrease the error. This is the method of *Gradient Descent*. <br><br>\n",
    "We train the network by iteratively adjusting the weights and biases until we *minimize* (i.e. reach the minimum value of) the loss function. This iterative process is called *Back-Propagation*.<br><br>\n",
    "This is a lot of information to take in! Don't worry if you don't understand this yet. <br>\n",
    "We'll explain *Gradient Descent* and *Back-Propagation* in detail in the next Module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYZ3c3nmFdw6"
   },
   "source": [
    "### 3.8.6 Activation Function\n",
    "The activation function controls the output of any given neuron.\n",
    "Its most important feature is its derivative or slope,\n",
    "which provides information that we use to update the neuron's weights and bias during model training with **Gradient Descent**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8QnjfTHCCwr"
   },
   "source": [
    "\n",
    "**Sigmoid Curve and its Derivative**\n",
    "\n",
    "![](https://i.stack.imgur.com/inMoa.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFJW3baCtRgU"
   },
   "source": [
    "A number of activation functions are commonly used in neural networks. <br>\n",
    "The most important of these are the `sigmoid`, `relu`, `tanh` and `softmax` activation functions<br>\n",
    "Here are some short reference articles explaining activation functions:\n",
    "* [Everything you need to know about activation functions in deep learning](https://towardsdatascience.com/everything-you-need-to-know-about-activation-functions-in-deep-learning-models-84ba9f82c253)\n",
    "* [7 popular activation functions you should know in deep learning](https://towardsdatascience.com/7-popular-activation-functions-you-should-know-in-deep-learning-and-how-to-use-them-with-keras-and-27b4d838dfe6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibksGlxedU1p"
   },
   "source": [
    "### 3.8.7 Training a Neural Network\n",
    "Training is the process of iteratively finding the optimal value of weights and bias for each neuron in the network, i.e. the values that minimize the loss function. The methods used in training are called *Gradient Descent* and *Back-propagation* and will be discussed in the next Module <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AW5rujQ4DWkv"
   },
   "source": [
    "## 4. The Keras Sequential API (Learn)\n",
    "The Keras Sequential API enables you to easily to build, compile, fit and evaluate your own neural network models. <br><br>\n",
    "Warning: learning to use Keras is exhilarating!<br>\n",
    "The feeling is a bit like when your parents gave you the keys to the car when you were a teenager...<br>\n",
    "In the remainder of Unit 4, we'll learn to build, drive, and apply the machinery of neural networks to solve important and interesting problems.\n",
    "\n",
    "To get an intuitive feeling for how neural networks work, try [TensorFlow playground](https://playground.tensorflow.org/)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdqgNz_ADWkw",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Overview\n",
    "\n",
    "> \"Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research. Use Keras if you need a deep learning library that:\n",
    "\n",
    "> Allows for easy and fast prototyping (through user friendliness, modularity, and extensibility).\n",
    "Supports both convolutional networks and recurrent networks, as well as combinations of the two.\n",
    "Runs seamlessly on CPU and GPU.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NhmkTB0HGTnH"
   },
   "source": [
    "### !!!!! STOP -- You need to access GPU Resources on Colab before running the codes below!!!!!\n",
    "\n",
    "From Colab's **Runtime** menu, select **Change Runtime type**, and choose the **GPU hardware accelerator**. <br>\n",
    "Without the GPU, neural network model training is slow as molasses!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEMjxPiGDWkx"
   },
   "source": [
    "### 4.1 Example: the XOR problem\n",
    "In computer science XOR is a logical operation called \"Exclusive Or\".<br>\n",
    "Fot two logical inputs $a$ and $b$, <br>\n",
    "$a~~\\text{XOR}~~b$ has the same outputs as $a~~\\text{OR}~~b$, <br>\n",
    "with the exception that if both inputs are $\\text{True}$, the output is $\\text{False}$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hm7YHCA3DWkx"
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = { 'x1': [0,1,0,1],\n",
    "         'x2': [0,0,1,1],\n",
    "         'y':  [0,1,1,0]\n",
    "       }\n",
    "\n",
    "df = pd.DataFrame.from_dict(data).astype('int')\n",
    "X = df[['x1', 'x2']].values\n",
    "y = df['y'].values"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dM_ZUS3I5Mt6"
   },
   "source": [
    "XOR truth table: inputs are $x1$ and $x2$, output is $y$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cpHYbznNahw-"
   },
   "source": [
    "df.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRJb4bEr5dOk"
   },
   "source": [
    "We can think of XOR as a binary classification problem"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lPh8bYEeprqF"
   },
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# notice that we can't just draw a line to separate the two classes\n",
    "sns.relplot(x=\"x1\", y=\"x2\", hue=\"y\",\n",
    "            sizes=(40, 400), alpha=1, palette=\"muted\",s=200,\n",
    "            height=5,  data=df);\n",
    "plt.grid()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m66Wg--3oltE"
   },
   "source": [
    "#### 4.1.1 Let's try to solve the $\\text{XOR}$ problem\n",
    "Our first approach will be to use a `perceptron` model to predict the output $y$ given the inputs $x_1$ and $x_2$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8W0IPhOiprqE"
   },
   "source": [
    "!pip install keras"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9zW_GYH3DWk0"
   },
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KrGr9O0NprqE"
   },
   "source": [
    "## Build a perceptron with keras \n",
    "# The perceptron can't get to 100% accuracy because it can only fit linear boundaries between classes\n",
    "\n",
    "# instantiate a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# add a dense layer\n",
    "# with some layer-specific hyperparameters\n",
    "\n",
    "model.add(Dense(1,                     # 1 neuron in the hidden layer\n",
    "                input_dim=2,           # input_dim is the only place where we say anything about the input layer\n",
    "                activation='sigmoid')) # selecting our activation function\n",
    "\n",
    "# compile the model \n",
    "# locks the model architecture. \n",
    "# indicate network-level hyperparameters\n",
    "model.compile(loss='binary_crossentropy', # We're doing binary classification\n",
    "             optimizer='sgd',             # stochastic gradient descent \"vanilla\" optimizer\n",
    "             metrics=['accuracy']) \n",
    "\n",
    "# fit the model \n",
    "model.fit(X, y, epochs=100) # training data X and y"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2-o90HjSnau"
   },
   "source": [
    "What went wrong?<br> \n",
    "The `perceptron` model cannot do the job, because (as we saw above) the classes in this problem cannot be separated with a line!<br>\n",
    "A `perceptron` model with a sigmoid activation function is equivalent to Logistic Regression, which is a linear model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owxdV2hoTzjC"
   },
   "source": [
    "#### 4.1.2 Let's try again to solve the $\\text{XOR}$ problem\n",
    "This time we'll build a multilayer `perceptron` model!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "deletable": false,
    "id": "X7OsYwmPCCwr",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94dc769fcb59dcac32fcf0b30fa0a8d7",
     "grade": false,
     "grade_id": "cell-61f9da5f7fa91510",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "%%time\n",
    "# This is a Deep Neural Network with multiple hidden layers\n",
    "# Deep neural networks are any NNs with more than 1 hidden layer\n",
    "\n",
    "# instantiate a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# add a dense layer\n",
    "# with some layer-specific hyperparameters\n",
    "\n",
    "model.add(Dense(10,  \n",
    "                input_dim=2, # input_dim is the only place where we say anything about the input layer\n",
    "                activation='relu')) # selecting our activation function\n",
    "\n",
    "model.add(Dense(8, activation='relu')) # selecting our activation function\n",
    "\n",
    "model.add(Dense(5, activation='relu')) # selecting our activation function\n",
    "\n",
    "# output layer with sigmoid function for binary classification\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model \n",
    "# locks the model architecture. \n",
    "# indicate network-level hyperparameters\n",
    "model.compile(loss='binary_crossentropy', # We're doing binary classification\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy']) # stochastic gradient descent \"vanilla\" optimzer\n",
    "\n",
    "# fit the model \n",
    "model.fit(X, y, epochs=100) # training data X and y"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DQ-KdVJ5DWk2"
   },
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(X, y)\n",
    "print(f\"{model.metrics_names[1]}: {scores[1]*100}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpdOaBJgWT1v"
   },
   "source": [
    "##### BOOM! The Multilayer Perceptron (MLP) model solved the $\\text{XOR}$ problem perfectly!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BIRJQfX2YQMp"
   },
   "source": [
    "model.summary()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJToyLLmprqG"
   },
   "source": [
    "The perceptron is only effective for linearly separable data sets. <br>\n",
    "But if we combined two or more of them in a multi-perception model (i.e. a neural network) then they can handle non-linear data!\n",
    "\n",
    "![](https://www.edureka.co/blog/wp-content/uploads/2017/07/Linear-528x264.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0TfFvipdDWk4",
    "toc-hr-collapsed": true
   },
   "source": [
    "### 4.2 Follow Along: Neural Network Modeling Workflow with the Keras Sequential API\n",
    "\n",
    "In the Keras `Sequential` API, you specify a model architecture by sequentially adding layers. This type of architecture works well for feed forward neural networks in which the data flows in one direction (forward propagation) and the error flows in the opposite direction (backwards propagation). The Keras `Sequential` API follows a standardarized workflow to estimate a neural network model: \n",
    "\n",
    "1. Load Data\n",
    "2. Define Model\n",
    "3. Compile Model\n",
    "4. Fit Model\n",
    "5. Plot metrics\n",
    "5. Evaluate Model\n",
    "6. Get Predictions for the test data\n",
    "\n",
    "You saw these steps in our Keras Perceptron examples above, but let's walk through each step in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KWq4PE6K2ZrE"
   },
   "source": [
    "### MNIST digit classification\n",
    "As our next example of neural networks, we will use a multilayer perceptron (MLP) model  to solve the famous **MNIST digit classification problem**. The [MNIST](https://en.wikipedia.org/wiki/MNIST_database) database is a publicly available collection of 60,000 images, each of which contains a singe handwritten decimal digit (0 - 9). The ten digit classes are roughtly equally represented. We want to train a model that can classify each image into the correct digit class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMvL4D3mDWk5",
    "toc-hr-collapsed": false
   },
   "source": [
    "#### 4.2.1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pnk2YNVODWk5"
   },
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "import numpy as np"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "D27iFEuRDWk7"
   },
   "source": [
    "# Load the Data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xbZ3MTDCb3mW"
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X_train[16]);"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "p0FgRxPVi7oP"
   },
   "source": [
    "X_train.shape"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kckh0ZGoprqH"
   },
   "source": [
    "28*28"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Xr89pKZaprqH"
   },
   "source": [
    "pd.options.display.max_columns=30\n",
    "pd.DataFrame(data = X_train[16])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Sl4Fso_PzVD"
   },
   "source": [
    "### Each pixel has a value that is represented by 8 bits\n",
    "The maximum value that can be stored in 8 bits is $2^8-1=255$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "deletable": false,
    "id": "gpIYmn6MDWlC",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f4ec7033e74f9668ebf3e31a97fbaf1b",
     "grade": false,
     "grade_id": "cell-1bd3c5df73f77628",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "# normalize the pixel values by dividing by  the  max value\n",
    "\n",
    "max_value = 255\n",
    "\n",
    "X_train = X_train.astype('float32') / max_value\n",
    "X_test = X_test.astype('float32') / max_value\n",
    "\n",
    "# normalize the pixel values "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FGTeDTKRKv7U"
   },
   "source": [
    "X_train.shape"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "deletable": false,
    "id": "XJGguQn3dAKj",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "381cfcec4b6145530df8aa43fa48ad2d",
     "grade": false,
     "grade_id": "cell-54c049d576c55a6f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "# flatten image \n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 784)\n",
    "X_test = X_test.reshape(X_test.shape[0], 784)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DUnIOxGIaK4p"
   },
   "source": [
    "X_train.shape"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-RX0HiBVlZ2S"
   },
   "source": [
    "y_train.shape"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7agLJp4dlheB"
   },
   "source": [
    "y_train[16]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "slDLmvaZ9Qop"
   },
   "source": [
    "np.unique(y_train)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2EW1FGMDWlE"
   },
   "source": [
    "#### 4.2.2 Build the Neural Network Model\n",
    "[Read this brief and informative introduction to the Keras Sequential API](https://keras.io/getting-started/sequential-model-guide/)!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "m5IHQYTbDWlE"
   },
   "source": [
    "from keras import Sequential"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CEXDUmUfDWlG"
   },
   "source": [
    "We instantiate a `Sequential` model. We'll then build the model's architecture one layer at a time."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hnGHcljbDWlH"
   },
   "source": [
    "model = Sequential()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDaQJ3wZDWlJ"
   },
   "source": [
    "We construct our neural network model by adding `perceptron` layers one at a time. Networks  composed of perceptron layers in which each neuron connects to all the neurons in the previous layer and to all the neurons in the next layer are also called \"fully-connected\", \"dense\", or \"densely-connected\" layers. \n",
    "\n",
    ">[Keras layers API](https://keras.io/layers/core/)\n",
    " \n",
    " When building a `perceptron` layer in Keras, the first argument specifies how many neurons we want to have in that layer. We'll create an \"input\" layer for this problem using $32$ neurons. The second argument specifies the type of activation function to use: here we'll use the `relu` activation function. The third argument specifies the number of inputs to the layer. In the MNIST data set, each input is a flattened $28\\times28$ image, so there are  $784 = 28\\times28$ inputs coming into this layer from our dataset image. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "deletable": false,
    "id": "e7K6IjqZDWlJ",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "744b7928d170a590f22d8058698075d3",
     "grade": false,
     "grade_id": "cell-fa08b4e31e0b7849",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "# Start with an initial layer of 32 neurons\n",
    "# The number of neurons in a layer is also the number of outputs. \n",
    "# Note: neural network layers before the output layer are often referred to as \"hidden\" layers)\n",
    "model.add(Dense(32, activation='relu', input_dim=784))\n",
    "model.summary()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hKd2AHTrglIt"
   },
   "source": [
    "We want the second layer to combine its inputs (which, remember, are the outputs of the neurons in the first layer) to compute a set of $10$ outputs. Why $10$? You guessed it -- one for each digit class! <br>\n",
    "Using the `softmax` activation function creates 10 outputs representing the respective probabilities that the image corresponds to each digit. I.e., the $3\\text{rd}$ and $10\\text{th}$ outputs are the probabilities that the handwritten digit in the image is a $2$ or a $9$, respectively, etc. <br>\n",
    "So our neural network doesn't actually classify an input image, instead it computes a set of probabilities over the $10$ output digit classes. Of course, the $10$ probabilities sum to $1$, as they should! We can then easily add a final layer to classify each image by picking the digit class with the highest probability.<br>\n",
    "Why do you suppose it's better to get the $10$ digit class probabilities as outputs rather than just the single class prediction?\n",
    "\n",
    "Reference: [Softmax Activation Function with Python](https://keras.io/api/layers/activation_layers/softmax/#softmax-layer) by Jason Brownlee\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "deletable": false,
    "id": "CtDiAZ2DeYch",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "69f2bab23ffbe7d18434384195cc4394",
     "grade": false,
     "grade_id": "cell-4ee33acb86df23bc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "# add an output layer with a softmax activation function\n",
    "# Sequential() knows that the inputs to this layer are the outputs from the previous layer, so we don't have to specify the number of inputs\n",
    "model.add(Dense(10, activation='softmax'))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nDWlyt2aexJj"
   },
   "source": [
    "model.summary()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vOavpBcBDWlL",
    "toc-hr-collapsed": false
   },
   "source": [
    "#### 4.2.3 Compile the Neural Network Model\n",
    "When compiling a model in Keras, there are three inputs to specify: the `optimizer`, the `loss function` and the `metric`. <br>\n",
    "We discuss them in this section.\n",
    "\n",
    "#### The `loss function` \n",
    "is a special function quantifying the error between the targets $y$ from the training data and the network's predicted targets $\\hat{y}$. We train the network to **minimize the loss function**, that is, to make the prediction error as small as possible!<br>\n",
    "In a **binary** classification problem (i.e. with two clases), we would use the `binary_cross_entropy` loss function. <br>\n",
    "For more than two categories (the MNIST problem has 10), the [appropriate loss function](https://stats.stackexchange.com/questions/326065/cross-entropy-vs-sparse-cross-entropy-when-to-use-one-over-the-other) is  `sparse_categorical_crossentropy` when the targets are expressed as integers, or `categorical_crossentropy` when the targets are one-hot-encoded. \n",
    "\n",
    "#### Training a neural network\n",
    "means finding the \"optimal\" values of the weights and biases of all the neurons, i.e. the values that lead to the lowest prediction error. This is usually done via some variation on the basic methods of **gradient descent** and **back-propagation**, which you will learn about soon!\n",
    "\n",
    "#### The `metric`\n",
    "is the number we use to gauge the performance of our neural network model. In this case, we specify that we want to report model `accuracy` as our metric for each *epoch*. An *epoch* is a complete training run that uses either a subset of the data (a *mini-batch*), or all the data. In training neural networks, we usually run through multiple epochs, until the performance stops improving. We will also be able to see the overall accuracy once the model has finished training. \n",
    "\n",
    "#### The `optimizer` \n",
    "Chooses which method to use in training the network. There are many optimizers available in Keras, but the **Adam Optimizer** usually gives great overall performance. Best practice: don't waste time trying out a lot of optimizers to find out which is best for your model, just pick **Adam**!\n",
    "\n",
    "#### Adam Optimizer\n",
    "For more background on the **Adam** optimizer, check out these references \n",
    "* [Adam Optimization Algorithm](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)\n",
    "* [Adam Optimizer - original paper](https://arxiv.org/abs/1412.6980)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ECMdYu63DWlM"
   },
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ziRD3mfDDWlO",
    "toc-hr-collapsed": false
   },
   "source": [
    "#### 4.2.4 Fit the Neural Network Model\n",
    "\n",
    "Lets train the MNIST neural network model that we just built to predict the digit classes! `model.fit()` has a `batch_size` parameter that we can use if we want to do *mini-batch* epochs, but since this tabular dataset is pretty small we'll use the default value `batch_size = None` so that each epoch will consist of the entire batch of input data. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZIOYRNyyDWlO"
   },
   "source": [
    "history = model.fit(X_train, \n",
    "                    y_train, \n",
    "                    epochs=5, \n",
    "                    validation_data=(X_test, y_test))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKS0tjCSnWl9"
   },
   "source": [
    "#### 4.2.5 Plot the Learning Curves\n",
    "Acuracy vs. Epoch and Loss vs. Epoch"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QKUd5b9_ARO9"
   },
   "source": [
    "type(history)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vjpNDJORAWtV"
   },
   "source": [
    "history.history"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9rs0Q4ixAdWn"
   },
   "source": [
    "history.history['val_accuracy']"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0YzUK4Mfh4An"
   },
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "epochs = [i for i in range(len(history.history['loss']))]\n",
    "sns.lineplot(x=epochs, y=history.history['loss'], label=\"train\")\n",
    "sns.lineplot(x=epochs, y=history.history['val_loss'], label=\"test\")\n",
    "plt.title('Loss')\n",
    "plt.grid()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Elv2gJDpDWlQ"
   },
   "source": [
    "#### 4.2.6 Evaluate the Model\n",
    "The `model.evaluate()` computes the loss and metric values for the input data and labels."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vmF6OL6UDWlR"
   },
   "source": [
    "model.evaluate(X_test,y_test)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1MWm-wQvoAIe"
   },
   "source": [
    "#### 4.2.7 Get Model Predictions on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ywh9AzX5CET9"
   },
   "source": [
    "To see what a model prediction looks like -- say for the first example in the test set, we can use the `model.predict()` method:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5ZKLPTOPBbPo"
   },
   "source": [
    "print(f'Model outputs are class probabilities from the softmax activation function:\\n {model.predict(X_test[0:1])}')\n",
    "print(f'\\nSum of the class probabilities: {model.predict(X_test[0:1]).sum()}')\n",
    "print(f'\\nPredicted label: {model.predict(X_test[0:1]).argmax()}')\n",
    "print(f'\\nTrue label: {y_test[0]}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L08VIhoTq8tJ"
   },
   "source": [
    "Get the predicted class probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5p8zODXpDWlS"
   },
   "source": [
    "You'll notice that if we rerun fitting, the results might differ slightly from run to run. This is due to the many effects of randomness in the modeling process. Interested to know more? Read the article [Embrace Randomness in Machine Learning](https://machinelearningmastery.com/randomness-in-machine-learning/), by Jason Brownlee."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "h57lpjt0qHMn"
   },
   "source": [
    "predictions = model.predict(X_test)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iuVVTaHKq5TN"
   },
   "source": [
    "Get class predictions for the whole `test` data set"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ps2qyKMLqTzx"
   },
   "source": [
    "predicted_digits = [predictions[index,:].argmax() for index in range(len(X_test))]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IuUh0v56rHRv"
   },
   "source": [
    "predicted_digits[:20]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xD5kCAiW8bw"
   },
   "source": [
    "#### Let's look at some of the misclassified images"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JWHNGX-lCn0o"
   },
   "source": [
    "predicted_digits=np.array(predicted_digits)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VQoVzzIsCvmQ"
   },
   "source": [
    "# boolean indicator that is True when the predicted digit label agrees with the actual digit label\n",
    "indicator = (predicted_digits == y_test)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BjiXp2R_C-jC"
   },
   "source": [
    "# get pixel vectors and the labels for the misclassified examples \n",
    "misclassified_examples = X_test[~indicator]\n",
    "misclassified_labels = predicted_digits[~indicator]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5B_Z5isbDXqe"
   },
   "source": [
    "# reshape the pixel vectors back to arrays (so we can display them as images)\n",
    "print(misclassified_examples.shape)\n",
    "misclassified_images = misclassified_examples.reshape((misclassified_examples.shape[0], 28, 28))\n",
    "print(misclassified_images.shape)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i2UuWBeSkqFh"
   },
   "source": [
    "True digit labels for the first 10 misclassified images"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uEbZm0BOkD_7"
   },
   "source": [
    "true_labels = y_test[~indicator]\n",
    "true_labels[:10]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bMtdStPkzkt"
   },
   "source": [
    "Let's examine the True vs. Predicted digit labels for the first 10 misclassified images.<br> \n",
    "Can you explain why each image was misclassified?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "U34dLuRNkPk8"
   },
   "source": [
    "misclassified_labels[:10]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xlChjdowS8ho"
   },
   "source": [
    "for image, label, true_label in zip(misclassified_images[:10,:,:],misclassified_labels[:10], true_labels[:10]):\n",
    "  plt.figure()\n",
    "  plt.title(f'digit: true {true_label}, predicted {label}')\n",
    "  plt.imshow(np.squeeze(image));"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EULJBiUZDWlT"
   },
   "source": [
    "## Challenge\n",
    "\n",
    "You will be expected to use the Keras `Sequential` API to estimate (i.e. build, compile, fit, and evaluate) a feed forward neural network on a dataset.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ueDVpctAzvy8"
   },
   "source": [
    "# 5. Choosing An Architecture (Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDFKiEkkRLU1"
   },
   "source": [
    "## Overview\n",
    "\n",
    "Choosing an architecture for a neural network is almost more an art than a science. The best way to choose an architecture is through research and experimentation. \n",
    "\n",
    "Let's do a few experiments. To track our results we'll use a tool called [TensorBoard](https://www.tensorflow.org/tensorboard), which is a way to interactively visualize the results of our various experiments. Here is our previous model with TensorBoard incorporated: "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GbEPV2H0RLVJ"
   },
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "deletable": false,
    "id": "DaHvGD7dRLVf",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ca239a26f1f1f368074e6fc8d6b3ea5",
     "grade": false,
     "grade_id": "cell-676b61a3530bd003",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "## build a 1 layer neural network \n",
    "\n",
    "# instantiate a sequential model\n",
    "\n",
    "# add a dense layer\n",
    "\n",
    "# add an output layer\n",
    "\n",
    "# compile the model \n",
    "\n",
    "# fit the model \n",
    "\n",
    "### BEGIN SOLUTION\n",
    "model = Sequential()\n",
    "\n",
    "model.add(\n",
    "    # Hidden Layer 1\n",
    "    Dense(32, activation=\"relu\", input_dim=784)\n",
    ")\n",
    "\n",
    "model.add(\n",
    "    Dense(10, activation='softmax')\n",
    ")\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test), callbacks=[tensorboard_callback])\n",
    "### END SOLUTION "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_H28UC7tKdhx"
   },
   "source": [
    "In `tensorboard`, click on \"SCALARS\" to see plots of metric (accuracy) and loss vs. epoch"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0BC3ZPfRRLVn"
   },
   "source": [
    "%tensorboard --logdir logs"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Sb7O8sVRLVt"
   },
   "source": [
    "## Follow Along\n",
    "\n",
    "Let's run a couple of experiments in groups based on your birthday: \n",
    "1. Jan - March:  Try adding an additional layer to the model\n",
    "2. April - June: Add 2 additional hidden layers with identical number of neurons\n",
    "3. July - Sept: Change the activation functions in the hidden layers (use as many layers as you want)\n",
    "4. Oct - December: Try changing the optimization function and use any architecture that you want. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4HXj5hgrRLVu"
   },
   "source": [
    "### 1. Additional Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "deletable": false,
    "id": "TpotMnV5RLVw",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1a2b8e229e4214a308a86c78db60d664",
     "grade": false,
     "grade_id": "cell-0697978adf354af0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "## build a single hidden layer neural network \n",
    "\n",
    "# instantiate a sequential model\n",
    "\n",
    "# add 1st dense layer\n",
    "\n",
    "# add 2nd dense layer\n",
    "\n",
    "# add an output layer\n",
    "\n",
    "# compile the model \n",
    "\n",
    "# fit the model \n",
    "\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "model = Sequential()\n",
    "\n",
    "model.add(\n",
    "    # Hidden Layer 1\n",
    "    Dense(32, activation=\"relu\", \n",
    "          input_dim=784)\n",
    ")\n",
    "\n",
    "model.add(\n",
    "    # Hidden Layer 2\n",
    "    Dense(5, activation=\"relu\")\n",
    ")\n",
    "\n",
    "# output layer\n",
    "model.add(\n",
    "    Dense(10, activation='softmax')\n",
    ")\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test), callbacks=[tensorboard_callback])\n",
    "### END SOLUTION"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTK3XmnCK4up"
   },
   "source": [
    "In `tensorboard`, click on \"SCALARS\" to see plots of metric (accuracy) and loss vs. epoch"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HmWpIWqlRLV6"
   },
   "source": [
    "%tensorboard --logdir logs"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jcFtoqlDRLV_"
   },
   "source": [
    "### 2 Additional Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "deletable": false,
    "id": "FHXfIJwKZBkE",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e8e6208c1761646cbdf6b29e5c583319",
     "grade": false,
     "grade_id": "cell-9dff4977bb8f9c10",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "## build a 3 hidden layer neural network \n",
    "\n",
    "# instantiate a sequential model\n",
    "\n",
    "# add 1st dense layer\n",
    "\n",
    "# add 2nd dense layer\n",
    "\n",
    "# add 3rd dense layer\n",
    "\n",
    "# add an output layer\n",
    "\n",
    "# compile the model \n",
    "\n",
    "# fit the model \n",
    "\n",
    "\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "model = Sequential()\n",
    "\n",
    "model.add(\n",
    "    # Hidden Layer 1\n",
    "    Dense(32, activation=\"relu\")\n",
    ")\n",
    "\n",
    "model.add(\n",
    "    # Hidden Layer 2\n",
    "    Dense(32, activation=\"relu\")\n",
    ")\n",
    "\n",
    "model.add(\n",
    "    # Hidden Layer 3\n",
    "    Dense(32, activation=\"relu\")\n",
    ")\n",
    "\n",
    "model.add(\n",
    "    Dense(10, activation='softmax')\n",
    ")\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test), callbacks=[tensorboard_callback])\n",
    "### END SOLUTION"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LKaY2R_SK7zH"
   },
   "source": [
    "In `tensorboard`, click on \"SCALARS\" to see plots of metric (accuracy) and loss vs. epoch"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lQrKE7KCRLWE"
   },
   "source": [
    "%tensorboard --logdir logs"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5-wkFBWRLWK"
   },
   "source": [
    "### 3. Different Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "deletable": false,
    "id": "80KQ9C5XRLWL",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8d6bc32a916aedf8add24ad2af90911f",
     "grade": false,
     "grade_id": "cell-75ff0e7f5620ada6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "## build a 3 hidden layer neural network with different activation functions\n",
    "\n",
    "# instantiate a sequential model\n",
    "\n",
    "# add 1st dense layer\n",
    "\n",
    "# add 2nd dense layer\n",
    "\n",
    "# add 3rd dense layer\n",
    "\n",
    "# add an output layer\n",
    "\n",
    "# compile the model \n",
    "\n",
    "# fit the model \n",
    "\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "model = Sequential()\n",
    "\n",
    "model.add(\n",
    "    # Hidden Layer 11\n",
    "    Dense(32, activation=\"sigmoid\", input_dim=784)\n",
    ")\n",
    "\n",
    "model.add(\n",
    "    # Hidden Layer 2\n",
    "    Dense(32, activation=\"sigmoid\")\n",
    ")\n",
    "\n",
    "model.add(\n",
    "    # Hidden Layer 3\n",
    "    Dense(32, activation=\"sigmoid\")\n",
    ")\n",
    "\n",
    "model.add(\n",
    "    Dense(10, activation='softmax')\n",
    ")\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test), callbacks=[tensorboard_callback])\n",
    "### END SOLUTION"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bifz6A6HLG8R"
   },
   "source": [
    "In `tensorboard`, click on \"SCALARS\" to see plots of metric (accuracy) and loss vs. epoch"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wd2wnkyjRLWP"
   },
   "source": [
    "%tensorboard --logdir logs"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RtXWHxjWRLWV"
   },
   "source": [
    "### 4. Different Optimization Functions\n",
    "[Keras Optimizers Docs](https://keras.io/api/optimizers/)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "deletable": false,
    "id": "6nnMIhSyZIDP",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "706f7679f7d118f179a91d0942482f7c",
     "grade": false,
     "grade_id": "cell-4fb011c210ce491a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "## build a 3 hidden layer neural network with a different optimizer\n",
    "\n",
    "# instantiate a sequential model\n",
    "\n",
    "# add 1st dense layer\n",
    "\n",
    "# add 2nd dense layer\n",
    "\n",
    "# add 3rd dense layer\n",
    "\n",
    "# add an output layer\n",
    "\n",
    "# compile the model \n",
    "\n",
    "# fit the model \n",
    "\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "model = Sequential()\n",
    "\n",
    "model.add(\n",
    "    # Hidden Layer 1\n",
    "    Dense(32, activation=\"relu\", input_dim=784)\n",
    ")\n",
    "\n",
    "model.add(\n",
    "    # Hidden Layer 2\n",
    "    Dense(32, activation=\"relu\")\n",
    ")\n",
    "\n",
    "model.add(\n",
    "    # Hidden Layer 3\n",
    "    Dense(32, activation=\"relu\")\n",
    ")\n",
    "\n",
    "model.add(\n",
    "    Dense(10, activation='softmax')\n",
    ")\n",
    "\n",
    "# reference the docs to see optimizer options such as rmsprop, sgs, adadelta, etc ... \n",
    "model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test), callbacks=[tensorboard_callback])\n",
    "### END SOLUTION"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQyJ3PumLJvo"
   },
   "source": [
    "In `tensorboard`, click on \"SCALARS\" to see plots of metric (accuracy) and loss vs. epoch"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "k32YJ0G4RLWW"
   },
   "source": [
    "%tensorboard --logdir logs"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WaMLtERVRLWb"
   },
   "source": [
    "## Challenge\n",
    "\n",
    "You will have to choose your own architectures in today's module project. In the next module, we will discuss hyperparameter optimization which can help you handle these numerous choices. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucUP72uiyQ_0",
    "toc-hr-collapsed": false
   },
   "source": [
    "# Sources\n",
    "\n",
    "\n",
    "### Academic References -- background on the perceptron\n",
    "- McCulloch, W.S. & Pitts, W. Bulletin of Mathematical Biophysics (1943) 5: 115. https://doi.org/10.1007/BF02478259\n",
    "- Rosenblatt, F. (1958). The perceptron: A probabilistic model for information storage and organization in the brain. Psychological Review, 65(6), 386–408. https://doi.org/10.1037/h0042519\n",
    "- Goodfellow, I., Bengio, Y., & Courville, A. (2016). [Deep learning](https://www.deeplearningbook.org/).\n",
    "\n",
    "### Implementation Reference\n",
    "- [NN-SVG](http://alexlenail.me/NN-SVG/index.html) by Alex Lenail. Used to generate diagrams for this notebook. \n",
    "- Alammar, Jay (2016). [A Visual and Interactive Guide to the Basics of Neural Networks](https://jalammar.github.io/visual-interactive-guide-basics-neural-networks/).\n",
    "- [SINGLE LAYER NEURAL NETWORK - PERCEPTRON MODEL ON THE IRIS DATASET USING HEAVISIDE STEP ACTIVATION FUNCTION](https://www.bogotobogo.com/python/scikit-learn/Perceptron_Model_with_Iris_DataSet.php) by K Hong. For Perceptron Demo.\n",
    "\n",
    "### Supplementary Videos\n",
    "These videos build a deeper level of understanding of neural networks\n",
    "\n",
    "- [3 Blue 1 Brown Neural Networks Playlist](https://www.3blue1brown.com/topics/neural-networks)  -- these 4 short videos do a great job introducing neural networks in understandable terms. The first 3 videos give you an intuitive feel for neural networks and how backpropagation works. The last video goes into the details of backpropagation using calculus, if you're interested in a deeper dive.\n",
    "- [Andrew Ng Neural Network Introduction Videos](https://www.youtube.com/watch?v=1ZhtwInuOD0&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=43)\n"
   ]
  }
 ]
}
