{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"LS_DS_421_Architect_Assignment_with_corrections.ipynb","provenance":[{"file_id":"https://github.com/ryanleeallred/DS-Unit-4-Sprint-2-Neural-Networks/blob/main/module1-Architect/LS_DS_421_Architect_Assignment.ipynb","timestamp":1635791066977}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ObyHCH8HvHSf"},"source":["\n","# *Data Science Unit 4 Sprint 2 Assignment 1*\n","\n","Use TensorFlow Keras and a sample of the [Quickdraw dataset](https://github.com/googlecreativelab/quickdraw-dataset) to build a sketch classification model. The dataset has been sampled to only 10 classes and 10000 observations per class. \n","\n","### Objective\n","\n","- Build a baseline classification model then, run a few experiments with different optimizers and learning rates. \n","- Don't forget to [**switch to GPU if you're running your notebook on Colab!**](https://colab.research.google.com/notebooks/gpu.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"s-Tc3ovEyQ9b"},"source":["## Load Your Data"]},{"cell_type":"code","metadata":{"id":"CkU0pAYCvU8o"},"source":["# imports in first cell \n","import seaborn as sns\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf \n","import matplotlib.pyplot as plt\n","from sklearn.utils import shuffle\n","\n","# use Sequential to build out your model\n","from tensorflow.keras.models import Sequential\n","\n","# Dense layer is used for Fully Connected Forward Feeding networks\n","from tensorflow.keras.layers import Dense"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S1XYJRQKLj1B"},"source":["# load the quickdraw data set\n","!git clone https://github.com/LambdaSchool/DS-Unit-4-Sprint-2-Neural-Networks\n","data = np.load('/content/DS-Unit-4-Sprint-2-Neural-Networks/quickdraw10.npz')\n","\n","X = data['arr_0']\n","y = data['arr_1']\n","\n","print(X.shape)\n","print(y.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c8qsDqdqvHDd"},"source":["class_names = ['apple',\n","             'anvil',\n","             'airplane',\n","             'banana',\n","             'The Eiffel Tower',\n","             'The Mona Lisa',\n","             'The Great Wall of China',\n","             'alarm clock',\n","             'ant',\n","             'asparagus']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Owbm1EbxvA5A"},"source":["# these are your 10 unique images\n","plt.figure(figsize=(10,5))\n","start = 0\n","\n","for num, name in enumerate(class_names):\n","    plt.subplot(2,5, num+1)\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.grid(False)\n","    plt.imshow(X[start].reshape(28,28), cmap=plt.cm.binary)\n","    plt.xlabel(name)\n","    start += 10000\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Av6i6t5QLj1D"},"source":["Say hello to the Quickdraw dataset. \n","\n","You'll be using this dataset a lot this week as an alternative to the mnist, which we'll use in the guided projects. The nice thing about this dataset is that it's simple, which allows us to focus on our model, its various components, and gradually come to a better understanding of how to build neural networks without worrying about cleaning and prepping our image data much."]},{"cell_type":"code","metadata":{"id":"c97_M1WNvTNY"},"source":["# always a good idea to shuffle your dataset \n","X, y = shuffle(X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jb70CbLVyK65"},"source":["-----\n","\n","## Build Your Baseline Model\n","\n","Make sure that you\n","\n","- **Determine** the dimensionality of your input data by investigating **X**\n","- **Normalize** your input data to values between 0 and 1 \n","- **Determine** the number of neurons in your output layer by investigating **Y**\n","- **Select** `sparse_categorical_crossentropy` as your loss function.\n","- **Select** `sgd` as your optimizer.\n","- **Add** 3 hidden layers to your model with the following number of nodes\n","    - h1 has 500 nodes\n","    - h2 has 250 nodes\n","    - h3 has 100 nodes\n","    \n","- **Set** epochs to 20 \n","- **Use** the `validation_split` keyword in `model.fit()` to automatically create a training / validation dataset from within the model. Specify a percentage, such as `validation_split = 0.2`\n"," \n","Not sure what the various parameters are for or what values to assign to them?\n","\n","- Reference the guided project notebook for Sprint 2 Module 1\n","- Reference the [**Keras documentation**](https://keras.io/api/)\n","- Google other examples\n","- Discuss your results with classmates "]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"ef20dd34df6998e0a50e394d59d58659","grade":false,"grade_id":"cell-907b9348d7a2ebb3","locked":false,"schema_version":3,"solution":true,"task":false},"id":"f133nwpOLj1G"},"source":["# get dim of image row vectors and save to input_dim\n","\n","# get number of unique labels and save to n_output_labels\n","\n","# normalize image data to values between 0 and 1 (by dividing by max pixel value)\n","\n","\n","# YOUR CODE HERE\n","raise NotImplementedError()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vOufsWS3Lj1G"},"source":["# a check on your data prep \n","assert  X_scaled.max(), \"Max pixel value should be 1.0, make sure you normalize your data\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"id":"zHWblzsMyNkU","nbgrader":{"cell_type":"code","checksum":"f0ba174cb72f491f73c3aa7df8ae7ac4","grade":false,"grade_id":"cell-b7c96fc46d86725f","locked":false,"schema_version":3,"solution":true,"task":false}},"source":["# instantiate a sequential object and call it model, then add layers to your model\n","\n","# add a compile layer but don't fit your model yet \n","\n","# YOUR CODE HERE\n","raise NotImplementedError()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ojLnGDklLj1H"},"source":["# a check on your model architecture \n","n_layers = len(model.get_config()[\"layers\"])\n","assert n_layers == 5, \"You should have 5 layers: input, h1, h2, h3, and output\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wMK2_DtJLj1I"},"source":["# check out your model summary \n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DXTSbEiyLj1I"},"source":["# fit your model and save training resuts to history \n","history = model.fit(X_scaled, y, \n","                    epochs=epochs, \n","                    # test set will be generated within the model\n","                    validation_split=0.2\n","                   )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b0QJURWh-9uv"},"source":["----\n","### Visualize the Results\n","\n","- Move results in `history` to a dataframe. \n","- Use [**Seaborn**](https://seaborn.pydata.org/generated/seaborn.lineplot.html) to create line plots for both loss and accuracy by epoch. \n","- Analyze the results and write a couple of observations. \n","\n","At what point should we have stopped training the model and why? "]},{"cell_type":"code","metadata":{"deletable":false,"id":"ijAlzfYKAFaY","nbgrader":{"cell_type":"code","checksum":"6ef8cde40701c2ef57cf853b19455125","grade":false,"grade_id":"cell-16e647cfc3291a01","locked":false,"schema_version":3,"solution":true,"task":false}},"source":["# YOUR CODE HERE\n","raise NotImplementedError()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jFTzaMcALj1J"},"source":["# a check on our model training\n","assert df.shape[0] == 20, \"df should have the training results from 20 epochs\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"424d241660318a72ecda935be10485d7","grade":false,"grade_id":"cell-96dba18873c4cffc","locked":false,"schema_version":3,"solution":true,"task":false},"id":"i6NnocuJLj1J"},"source":["# use the plotting method in your dataframe to plot the modeling results \n","\n","# YOUR CODE HERE\n","raise NotImplementedError()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GmjyuTlrLj1K"},"source":["### Observations \n","\n","Based on the plot of the training and validation loss, answer the following questions. "]},{"cell_type":"markdown","metadata":{"id":"NF_mrNmPLj1K"},"source":["**Was our model able to learn throughout the 20 epochs? Why or why not?**"]},{"cell_type":"markdown","metadata":{"deletable":false,"nbgrader":{"cell_type":"markdown","checksum":"b7713eb32ad3a910a58dfdb9494a9db3","grade":true,"grade_id":"cell-820990a8232a858a","locked":false,"points":0,"schema_version":3,"solution":true,"task":false},"id":"yL42yLWELj1K"},"source":["YOUR ANSWER HERE"]},{"cell_type":"markdown","metadata":{"id":"_xKObFanLj1K"},"source":["**Is our model overfitting? Why or why not?**"]},{"cell_type":"markdown","metadata":{"deletable":false,"nbgrader":{"cell_type":"markdown","checksum":"1d30c64465cd35081d70578c20ecf96d","grade":true,"grade_id":"cell-33868e7ef6e401b5","locked":false,"points":0,"schema_version":3,"solution":true,"task":false},"id":"rx7xa4Z1Lj1L"},"source":["YOUR ANSWER HERE"]},{"cell_type":"markdown","metadata":{"id":"FIvb1mftLj1L"},"source":["**Could the model score benefit from additional epochs? Why or why not?**"]},{"cell_type":"markdown","metadata":{"deletable":false,"nbgrader":{"cell_type":"markdown","checksum":"0fc2208e2c78461578fc7a05224066b8","grade":true,"grade_id":"cell-5c883f0a3161e469","locked":false,"points":0,"schema_version":3,"solution":true,"task":false},"id":"Ww1hBmfMLj1L"},"source":["YOUR ANSWER HERE"]},{"cell_type":"markdown","metadata":{"id":"MAhBrcE4yOZe"},"source":["-----\n","## Change Optimizers\n","\n","Let's compare model performance between different optimizers. \n","- Build a new model, identical to the last one but using `adam` for the optimizer. \n","- Visualize the training results just as we did for the last model. \n","- Save modeling results to adam_history, so we don't erase the results from the previous model run"]},{"cell_type":"code","metadata":{"deletable":false,"id":"jIW_spOZ0cxy","nbgrader":{"cell_type":"code","checksum":"6d30d8c6f4521861a77c9ef04dbde904","grade":false,"grade_id":"cell-56663c8a5e75b71f","locked":false,"schema_version":3,"solution":true,"task":false}},"source":["# train same model as above but use sgd if you used adam previously (or use adam if you used sgd previously)\n","\n","# YOUR CODE HERE\n","raise NotImplementedError()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u9DQypQOLj1L"},"source":["# a check that we're using the correct optimizer in this model\n","opt_name = model.optimizer.get_config()[\"name\"]\n","assert opt_name == \"Adam\", \"you need to use adam for the optimizer in this model.\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"1a4c01aa7dbcc832d75b05bffaeaacc8","grade":false,"grade_id":"cell-20118d1646215346","locked":false,"schema_version":3,"solution":true,"task":false},"id":"wBVj55zVLj1L"},"source":["# YOUR CODE HERE\n","raise NotImplementedError()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XECc5OtzLj1M"},"source":["### Observations \n","\n","This plot should look very different from the previous plot. Based on the plot of the training and validation loss, answer the following questions. "]},{"cell_type":"markdown","metadata":{"id":"tK8B5CszLj1M"},"source":["**Is our model overfitting? Why or why not?**"]},{"cell_type":"markdown","metadata":{"deletable":false,"nbgrader":{"cell_type":"markdown","checksum":"fa4141497af675d227aa510feb8db732","grade":true,"grade_id":"cell-7e524ccac970c33e","locked":false,"points":0,"schema_version":3,"solution":true,"task":false},"id":"Fn6C8NWTLj1M"},"source":["YOUR ANSWER HERE"]},{"cell_type":"markdown","metadata":{"id":"ygXQYH8ILj1M"},"source":["**Could our model's performance improve by training on more than 20 epochs? Why or why not?**"]},{"cell_type":"markdown","metadata":{"deletable":false,"nbgrader":{"cell_type":"markdown","checksum":"ae03b8b897eb5d052590574ab3bba839","grade":true,"grade_id":"cell-456c40e885652955","locked":false,"points":0,"schema_version":3,"solution":true,"task":false},"id":"Sa5SRkB1Lj1M"},"source":["YOUR ANSWER HERE"]},{"cell_type":"markdown","metadata":{"id":"bNxtLggeLj1M"},"source":["**Assuming that you want to stick to this model architecture, what can you do to avoid overfitting?**"]},{"cell_type":"markdown","metadata":{"deletable":false,"nbgrader":{"cell_type":"markdown","checksum":"9f5a9393314231d3ce253363c1df1223","grade":true,"grade_id":"cell-74d7def19c66702b","locked":false,"points":0,"schema_version":3,"solution":true,"task":false},"id":"tfztMm3rLj1M"},"source":["YOUR ANSWER HERE"]},{"cell_type":"markdown","metadata":{"id":"ViaTP68cLj1M"},"source":["------\n","# Conclusion \n","\n","You have observed by comparing the two models that are identical except for the optimizer, that the choice of the optimizer can considerably influence the training outcome of ML models in general and in neural networks in particular. \n","\n","You might not know \n","\n","- why changing the optimizer from Stochastic Gradient Descent over to Adam made a difference.\n","- how Gradient Descent works \n","- or that Adam is known as Adaptive Gradient Descent (i.e., is it a different version of ordinary Gradient Descent). \n","\n","But that's ok - tomorrow's lesson will be a deep dive into Gradient Descent. You'll learn the theory of Gradient Descent, we'll code up Gradient Descent from scratch, and we'll talk about how Adam is different from ordinary Gradient Descent. "]},{"cell_type":"markdown","metadata":{"id":"8v_TuVH5Lj1N"},"source":["-----\n","# Preparation for Tomorrow \n","\n","In preparation for tomorrow, you might need to watch a few videos. I say it might because you may or may not already be comfortable with multi-variate calculus. \n","\n","### Theory of Calculus \n","The theory of Gradient Descent rests on [**the derivative from Calculus**](https://www.youtube.com/watch?v=WUvTyaaNkzM). If you've never taken a Calculus course before, or are a bit rusty, definitely watch this video in preparation for tomorrow's lesson. The takeaway here is to understand the derivative conceptually; you won't be asked to calculate any derivatives by hand. \n","\n","### The Gradient - a Multi-Dimensional Derivative\n","Once you've watched the previous video, you will be ready to understand how to conceptualize a derivative in an N-dimensional space (where N is any number ranging from 2, 3, 4, ..., all the way to a very large number of dimensions N ). [**This video visually explains the multi-dimensional derivative called the Gradient visually**](https://www.youtube.com/watch?v=GkB4vW16QHI) - and that's the takeaway. The point of this video is to help you understand the Gradient visually using 3D surfaces. You should also know that the Gradient is made up of partial derivatives, more on this tomorrow. \n","\n","### Contour Maps \n","It is very common to visual 3D surfaces as 2D contour maps - and we'll be making a lot of use of contour maps to understand Gradient Descent. Watch this video to [**understand the relationship between 3D surfaces and 2D contour maps.**](https://www.youtube.com/watch?v=acdX4YamDtU)\n","\n","### The Calculus of Backpropagation \n","\n","Unlike standard Sklearn ML models, neural networks use Gradient Descent and something called Backpropagatin to learn from the data. [**In order to understand how backpropagation works, you need to understand the Chain Rule in Calculus**](https://www.youtube.com/watch?v=acdX4YamDtU). The takeaway here is to understand how a partial derivative can be decomposed into a product of multiple derivatives. "]},{"cell_type":"markdown","metadata":{"id":"XCjBp0vTLj1N"},"source":["------\n","# Stretch Goals\n","\n","### This Section is Optional!\n","\n","Only after you've completed the above work and watched the recommended videos are you then encouraged to experiment with building a few more models and analyze their results. \n","\n","Here are some suggestions to help you get started. \n","\n","- Train the same model using the Sigmoid and Relu activation functions and note the difference in their learning outcomes. [**Keras docs on activation functions**](https://keras.io/api/layers/activations/). We'll cover the Relu activation function in Sprint 2 Module 3.\n","- Train the same model using normalized and non-normalized data and note the difference in their learning outcomes. \n","\n","\n","The objective here is to get more practice with the Keras API (i.e., building models) and run a couple of experiments to help set you up for future lessons. In order words, we'll be discussing different activation functions and the effect of normalized data on model training."]},{"cell_type":"code","metadata":{"id":"5F1VQLmALj1N"},"source":[""],"execution_count":null,"outputs":[]}]}