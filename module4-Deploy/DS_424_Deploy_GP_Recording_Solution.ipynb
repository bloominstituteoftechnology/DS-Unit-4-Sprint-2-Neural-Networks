{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "toc-autonumbering": false,
    "toc-showmarkdowntxt": false
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJzTIkYAsLxw"
      },
      "source": [
        "*Unit 4, Sprint 2, Module 4*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCf3jDMVQHuI"
      },
      "source": [
        "# Neural Network Frameworks (Prepare)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR0XBF5HQHuI"
      },
      "source": [
        "## Learning Objectives\n",
        "* <a href=\"#p1\">Part 1</a>: Implemenent Regularization Strategies\n",
        "* <a href=\"#p2\">Part 2</a>: Deploy a Keras Model\n",
        "* <a href=\"#p3\">Part 3</a>: Write a Custom Callback Function (Optional)\n",
        "\n",
        "Today's class will also focus heavily on Callback objects. We will use a variety of callbacks to monitor and manipulate our models based on data that our model produces at the end of an epoch.\n",
        "\n",
        "> A callback is an object that can perform actions at various stages of training (e.g. at the start or end of an epoch, before or after a single batch, etc). -- [Keras Documentation](https://keras.io/api/callbacks/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWuoXZCCKCI7"
      },
      "source": [
        "# 1. Regularization Strategies (Learn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3aMJZuPQHur"
      },
      "source": [
        "## Overview\n",
        "\n",
        "###Neural Networks are highly parameterized models and can be easily overfit to the training data.###\n",
        "\n",
        "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/02/Regularization.svg/1920px-Regularization.svg.png\" width=600></center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSKGTq74OxT8"
      },
      "source": [
        "###When the model is overfitting, the training error (loss) continues to decrease with each epoch <br>while the test error begins to increase.###\n",
        "\n",
        "<center><img src=\"https://hackernoon.com/hn-images/1*vuZxFMi5fODz2OEcpG-S1g.png\" width=80000></center>\n",
        "\n",
        "###The most salient way to combat this problem is with regularization strategies.###\n",
        "\n",
        "There are several common ways of regularization in neural networks which we cover briefly. Here's a quick summary:\n",
        "\n",
        "1. Always use EarlyStopping. This strategy will prevent parameters from being fitted too closely to the training data.\n",
        "2. Use Weight Decay\n",
        "3. Use Weight Constraint\n",
        "4. Weight Constraint together with Dropout\n",
        "\n",
        "Weight Decay and Weight Constraint accomplish similar purposes - preventing model overfitting by \"regularizing\" the values of the weights. The mechanics are just slightly different. Use one or the other, but not both."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gYuH-Ljur5T"
      },
      "source": [
        "### Imports we'll need"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgQR_iCHSF1x"
      },
      "source": [
        "%load_ext tensorboard\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FFhK0tLQHus"
      },
      "source": [
        "## Follow Along\n",
        "\n",
        "For our experiments, we'll use the [Fashion MNIST data set](https://www.tensorflow.org/datasets/catalog/fashion_mnist), available from from tensorflow datasets. Fashion MNIST, a twist on the familiar MNIST set, consists of $28\\times 28$ images of $10$ classes of Fashion items, such as shoes, purses, shirts, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-xLR4Asmy3A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c833e526-ad99-4b24-a088-eef8209ac0c9"
      },
      "source": [
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "# load our dataset\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BKJZkpuldUp",
        "outputId": "ac22543a-83d0-4d98-dbea-2306d40aa1eb"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "LErVFAGhnUaj",
        "outputId": "8bf72595-9e49-42fc-c949-d6defc69f98c"
      },
      "source": [
        "image_id = 500\n",
        "plt.imshow(X_train[image_id],cmap='gray');\n",
        "print(y_train[image_id])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jY7KV_7xnip_"
      },
      "source": [
        "# normalize pixel values between 0 and 1\n",
        "max_pixel_value = 255\n",
        "X_train, X_test = X_train / max_pixel_value, X_test / max_pixel_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PlFA0s3OFCm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "577701b2-d7ca-469d-f26f-dd15448b12d5"
      },
      "source": [
        "X_train.min(), X_train.max()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10 unique y values, corresponding to 10 different classes of fashion items"
      ],
      "metadata": {
        "id": "m6tpt0aP3Kps"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvxvNKqtOFCm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4384e24-4dac-4950-eaad-519ca0d05b9d"
      },
      "source": [
        "print(np.unique(y_train))\n",
        "y_train[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 2 3 4 5 6 7 8 9]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9, 0, 0, 3, 0, 2, 7, 2, 5, 5], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO74ukb-QHus"
      },
      "source": [
        "##1.1  Early Stopping Callback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_q8aZVqOFCn"
      },
      "source": [
        "### 1.1.1 Build and train a Neural Network on the Fashion MNIST data set\n",
        "Without the `EarlyStopping` callback\n",
        "\n",
        "Note: since the target classes are represented as integer indexes, use the  `sparse_categorical_cross_entropy` loss function. <br>\n",
        "We would use `categorical_cross_entropy` if the target labels were one-hot encoded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENuh6ux6OFCn"
      },
      "source": [
        "# Clear any logs from previous runs for tensorboard\n",
        "!rm -rf ./logs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PihGfEPgOFCo"
      },
      "source": [
        "%%time\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
        "from tensorflow.keras.layers import Flatten, Dense\n",
        "from tensorflow.keras.layers import ReLU\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "# 1) Create 2 dir for logging files\n",
        "# create 2 dir -- one for tensorboard results and one for early stopping\n",
        "logdir = os.path.join('logs', 'Default-No-Callbacks')\n",
        "\n",
        "# 2) Instantiate the callbacks\n",
        "# instantiate a tensorboard callback object\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "# 3) Build the model architecture\n",
        "model = Sequential([\n",
        "  Flatten(input_shape=(28,28)),\n",
        "  Dense(128, activation='relu'),\n",
        "  Dense(128, activation='relu'),\n",
        "  Dense(128, activation='relu'),\n",
        "  Dense(10, activation='softmax'),\n",
        "])\n",
        "\n",
        "# compile\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train,\n",
        "         epochs=30,\n",
        "         validation_data=(X_test, y_test),\n",
        "         verbose=1,\n",
        "         callbacks=[tensorboard_callback]) # not using the early stopping callback"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlVLtFW9OFCp"
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2mvpKhYIFFX"
      },
      "source": [
        "### 1.1.2 Build and train a Neural Network on Fashion MNIST data set\n",
        "With the `EarlyStopping` callback"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "id": "AY1HomhxQHus",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "2e861d2ed558a4732f43d25fa52ecd4c",
          "grade": false,
          "grade_id": "cell-086a5f430505c39e",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "%%time\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# 1) Create 2 dir for logging files\n",
        "# create 2 dir -- one for tensorboard results and one for early stopping\n",
        "logdir = os.path.join('logs', 'EarlyStopping')\n",
        "\n",
        "# 2) Instantiate the callbacks\n",
        "# instantiate a tensorboard callback object\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "# 2a) Add early stopping callback\n",
        "stop = EarlyStopping(\n",
        "    monitor='val_accuracy', min_delta=0.0001, patience=2, verbose=0,\n",
        "    mode='auto', restore_best_weights=True\n",
        ")\n",
        "\n",
        "# 3) Build the model architecture\n",
        "model = Sequential([\n",
        "  Flatten(input_shape=(28,28)),\n",
        "  Dense(128, activation='relu'),\n",
        "  Dense(128, activation='relu'),\n",
        "  Dense(128, activation='relu'),\n",
        "  Dense(10, activation='softmax'),\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Fit model\n",
        "model.fit(X_train, y_train,\n",
        "         epochs=99,\n",
        "         validation_data=(X_test, y_test),\n",
        "         verbose=1,\n",
        "         callbacks=[tensorboard_callback, stop]) # include early stopping callback"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3LFxeg1pJFE"
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7ScqKUzQHuv"
      },
      "source": [
        "---\n",
        "\n",
        "## 1.2 L1 and L2 regularization (Weight Decay)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UweNNrj8MxKI"
      },
      "source": [
        "These forms of regularization explicitly prevent the model from fitting the data too closely, <br>\n",
        "by imposing constraints that tend to drive down the values of the parameters, hence the term \"weight decay\".<br>\n",
        "They work by adding a term to the loss function to create a modified loss function. <br>\n",
        "The model then tries to minimize the modified loss function instead of the original loss function.<br>\n",
        "\n",
        "For L2 regularization, the modified loss function is the original loss function <br>plus a constant times the sum of the _squares_ of all the parameters:<br><br>\n",
        "\n",
        "$J_2(\\theta) = J(\\theta) + C\\sum_{i=1}^{N} \\theta_i^2$,<br><br>\n",
        "where $\\theta$ is a parameter (i.e. a weight or bias), $C$ is the regularization <br>\n",
        "constant (a hyperparameter) and $N$ is the number of parameters in the model.<br>\n",
        "\n",
        "For L1 regularization, the modified loss function is the original loss function <br>plus a constant times the sum of the _absolute values_ of all the parameters:<br><br>\n",
        "$J_1(\\theta) = J(\\theta) + C\\sum_{i=1}^{N} |\\theta_i|$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzHG6BvVQriY"
      },
      "source": [
        "![](https://qph.fs.quoracdn.net/main-qimg-9d0dbf8074761b541ba80543ddfc9f73.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1Je7YXmOFCr"
      },
      "source": [
        "The above images show how the minimum of the modified loss function (red dots) <br>\n",
        "differs from the minimum of the original loss function (black dots).\n",
        "\n",
        "1. The X and Y axes represent possible values for model weights, in the case of this visualization the X and Y axes represent w1 and w2 respectively\n",
        "1. The elliptical contours represent the original loss function (error) surface, with the black dot representing the values of the weights that minimize the loss function.\n",
        "1. The blue shapes represent the weight constraints (the diamond shape for L1, and the circle shape for L2)\n",
        "1. The red dot represents the point of contact between the loss function contours and the weight constraints, where the modified loss function has a minimum.\n",
        "1. The distance metric determines the geometry of the blue shapes. The L1-norm determines the diamond shape and the L2-norm determines the circular shape.\n",
        "1. The norm of the weights determines where the point of contact will occur. And the norm of the weights is determined by which metric space **p** we are getting the norm equation from.\n",
        "\n",
        "$L_p$ norm:\n",
        "$${ \\left\\|x\\right\\|_{p}=\\left(|x_{1}|^{p}+|x_{2}|^{p}+\\dotsb +|x_{n}|^{p}\\right)^{1/p}}$$  \n",
        "\n",
        "$L_1$ norm (p = 1):\n",
        "$${ \\left\\|x\\right\\|_{1}=\\left(|x_{1}|+|x_{2}|+\\dotsb +|x_{n}|\\right)}$$\n",
        "\n",
        "$L_2$ norm (p = 2):\n",
        "$${ \\left\\|x\\right\\|_{2}=\\left(|x_{1}|^{2}+|x_{2}|^{2}+\\dotsb +|x_{n}|^{2}\\right)^{1/2}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCCZa9TYQrsc"
      },
      "source": [
        "### 1.2.1 $\\textbf{L1}$ vs. $\\textbf{L2}$ Regularization\n",
        "\n",
        "1. \"Ridge\" ($\\textbf{L2}$) and \"Lasso\" ($\\textbf{L1}$) are two out of possibly infinitely many ways to regularize a model by using a distance metric in [$\\text{Lp}$ space](https://en.wikipedia.org/wiki/Lp_space)\n",
        "\n",
        "2. Both $\\textbf{L1}$ and $\\textbf{L2}$ are used to help prevent overfitting.\n",
        "\n",
        "3. **Key difference** between $\\textbf{L1}$ and $\\textbf{L2}$\n",
        "* $\\textbf{L1}$ tends to drive some feature weights to zero (i.e. $w = 0$) effectively eliminating those features. So a side effect of $\\textbf{L1}$ is *dimensionality reduction*! This happens because redundant information is encoded in that subset of features. Mathematically, this situation is refered to as [**MultiCollinearity**](https://en.wikipedia.org/wiki/Multicollinearity).\n",
        "* $\\textbf{L2}$ will shrink the values of all feature weights but almost never down to zero.\n",
        "\n",
        "Check out this article for more insight [Visualizing regularization and the L1 and L2 norms](https://towardsdatascience.com/visualizing-regularization-and-the-l1-and-l2-norms-d962aa769932)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2pbTvNhQrvZ"
      },
      "source": [
        "![](https://i.stack.imgur.com/4KSgs.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CT3OD84MQrxq"
      },
      "source": [
        "The above image shows us the geometry of 4 specific Lp spaces."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNSH7R-6OFCs"
      },
      "source": [
        "### 1.2.2 Build a Neural Net using  $L2$ regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "id": "eQuacT-JQHuv",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "eccd417780af3f91fd3f3bea5bba0770",
          "grade": false,
          "grade_id": "cell-6689ef291b96ae6b",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "%%time\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "# 1) Create 2 dir for logging files\n",
        "# create 2 dir -- one for tensorboard results and one for early stopping\n",
        "logdir = os.path.join('logs', 'Weight-Decay-Plus-EarlyStopping')\n",
        "\n",
        "# 2) Instantiate the callbacks\n",
        "# instantiate a tensorboard callback object\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "# 2a) Add early stopping callback\n",
        "stop = EarlyStopping(\n",
        "    monitor='val_accuracy', min_delta=0.0001, patience=2, verbose=0,\n",
        "    mode='auto', restore_best_weights=True\n",
        ")\n",
        "\n",
        "# 3) Build the model architecture\n",
        "# 3a) Add regularization to model architecture\n",
        "model = Sequential([\n",
        "  Flatten(input_shape=(28,28)),\n",
        "  Dense(128, kernel_regularizer=regularizers.l2(0.01), activation='relu'),\n",
        "  Dense(128, kernel_regularizer=regularizers.l2(0.01), activation='relu'),\n",
        "  Dense(128, kernel_regularizer=regularizers.l2(0.01), activation='relu'),\n",
        "  Dense(10, activation='softmax'),\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Fit model\n",
        "model.fit(X_train, y_train,\n",
        "         epochs=99,\n",
        "         validation_data=(X_test, y_test),\n",
        "         verbose=1,\n",
        "         callbacks=[tensorboard_callback, stop])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPA7EHOpxg2z"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XP12lHBnxFYe"
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCJ1aUIfQHuy"
      },
      "source": [
        "----\n",
        "\n",
        "###1.3 Regularization via the `MaxNorm` Weight Constraint\n",
        "In this method, after each weight vector update, we compare the norm of the updated weight vector with a user specified value of `MaxNorm`. If the norm of the updated weight vector exceeds `MaxNorm`, we scale the weight vector by a factor that clips its norm back to `MaxNorm` <br>\n",
        "\n",
        "**Stretch Question**: How would you implement a function to apply `MaxNorm` in code?<br>\n",
        "I.e. given a vector $\\textbf{w}$, an update vector $\\textbf{dw}$, and the value of `max_norm`, can you write a function that returns the updated weight vector $\\textbf{w}_{clipped}$ with the `MaxNorm` weight constraint applied?\n",
        "\n",
        "`MaxNorm` has been found to be quite effective when used in combination with  **Dropout Regularization**, which we discuss in the next section.\n",
        "\n",
        "```python\n",
        "tf.keras.constraints.MaxNorm(\n",
        "    max_value=2, axis=0\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2aygDR6zf1b"
      },
      "source": [
        "### Exercise: <br>\n",
        "Modify the above code cell for L2 regularization with option to use MaxNorm regularization instead of L2 regularization. <br>\n",
        "To do this we need to instantiate MaxNorm and pass it into the layers as a `kernel_constraint`. See keras docs. <br>\n",
        "Run the model. How do the results differ from the results L2 regularization?<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "id": "gbXBxr1QQHuy",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d865fb010e778dc161413adaf9dc7f82",
          "grade": false,
          "grade_id": "cell-bbb50a7f009726ce",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "from tensorflow.keras.constraints import MaxNorm\n",
        "\n",
        "# 1) Create 2 dir for logging files\n",
        "# create 2 dir -- one for tensorboard results and one for early stopping\n",
        "logdir = os.path.join('logs', 'MaxNorm-Plus-EarlyStopping')\n",
        "\n",
        "# 2) Instantiate the callbacks\n",
        "# instantiate a tensorboard callback object\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "# 2a) Add early stopping callback\n",
        "stop = EarlyStopping(\n",
        "    monitor='val_accuracy', min_delta=0.0001, patience=2, verbose=0,\n",
        "    mode='auto', restore_best_weights=True\n",
        ")\n",
        "\n",
        "# 3) Build the model architecture\n",
        "# 3a) Add MaxNorm weight constraint to model architecture\n",
        "model = Sequential([\n",
        "  Flatten(input_shape=(28,28)),\n",
        "  Dense(128, kernel_constraint=MaxNorm(3), activation='relu'),\n",
        "  Dense(128, kernel_constraint=MaxNorm(3), activation='relu'),\n",
        "  Dense(128, kernel_constraint=MaxNorm(3), activation='relu'),\n",
        "  Dense(10, activation='softmax'),\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Fit model\n",
        "model.fit(X_train, y_train,\n",
        "         epochs=99,\n",
        "         validation_data=(X_test, y_test),\n",
        "         verbose=1,\n",
        "         callbacks=[tensorboard_callback, stop])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nfuTyj54Qjf"
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-xMnXMsQHu0"
      },
      "source": [
        "-----\n",
        "##1.4  Dropout Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adaztN9KOFCw"
      },
      "source": [
        "![](https://miro.medium.com/max/981/1*EinUlWw1n8vbcLyT0zx4gw.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlwPD2YRduBd"
      },
      "source": [
        "Dropout Regularization is a powerful and ingenious idea for controlling overfitting in neural networks. It is one of the key \"must-know\" techniques involved in building neural networks.\n",
        "\n",
        "Geoffrey Hinton, one of the inventors of Dropout, recorded an 8-minute lecture [Dropout: an efficient way to combine neural nets](https://youtu.be/vAVOY8frLlQ) as part of his Neural Networks for Machine Learning course on Coursera. At $06:36$ in the video, Hinton explains how he came up with the idea for Dropout.\n",
        "\n",
        "You are encouraged to skim through the original publication [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](https://paperswithcode.com/method/dropout), by Nitish Srivastava et al. (2014)<br>\n",
        "Don't expect to understand everything, but don't let that stop you. You will be pleasantly surprised that you can understand enough to get the gist of the dropout method!\n",
        "\n",
        "Here is the abstract:<br>\n",
        "**Dropout: A Simple Way to Prevent Neural Networks from Overfitting**\n",
        "\n",
        "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov; 15(56):1929−1958, 2014.\n",
        "Abstract\n",
        "\n",
        "_Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different thinned networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryCiVDIfOFCx"
      },
      "source": [
        "\n",
        "**Key Take aways:**\n",
        "\n",
        "1. During training, dropout will probabilistically \"turn off\" some neurons in the layer that dropout is implemented in.\n",
        "2. During inference (ie. making predictions on the test set) all neurons are used (i.e. no dropout is applied).\n",
        "3. Dropout works best when used with MaxNorm weight regularization\n",
        "\n",
        "Here are some excerpts from the dropout article.\n",
        "\n",
        "\"Dropout can be interpreted as a way of regularizing a neural network by adding noise to its hidden units.\" (page 2)\n",
        "\n",
        "\"Combining several models [model ensembles] is most\n",
        "helpful when the individual models are different from each other and in order to make\n",
        "neural net models different, they should either have different architectures or be trained\n",
        "on different data...It prevents overfitting and\n",
        "provides a way of approximately combining exponentially many different neural network\n",
        "architectures efficiently.\" (page 2)\n",
        "\n",
        "\"One particular form of regularization was found to be especially useful for dropout --\n",
        "constraining the norm of the incoming weight vector at each hidden unit to be upper\n",
        "bounded by a\n",
        "fixed constant $\\text{c}$. In other words, if $\\textbf{w}$ represents the vector of weights incident\n",
        "on any hidden unit, the neural network was optimized under the constraint $||\\textbf{w}||_{2} ≤ \\text{c}$. This\n",
        "constraint was imposed during optimization by projecting $\\textbf{w}$ onto the surface of a ball of\n",
        "radius $\\text{c}$, whenever $\\textbf{w}$ went out of it. This is also called **max-norm regularization** since it\n",
        "implies that the maximum value that the norm of any weight can take is $\\text{c}$.  The constant $\\text{c}$ is a tunable hyperparameter, which is determined using a validation set.\" (page 6)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "id": "MCRL8SmgQHu0",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "7a0485222190198be1429998af512fe0",
          "grade": false,
          "grade_id": "cell-269ab54e72cc7607",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "\n",
        "# 1) Create 2 dir for logging files\n",
        "# create 2 dir -- one for tensorboard results and one for early stopping\n",
        "logdir = os.path.join('logs', 'MaxNorm-Plus-Dropout-and-EarlyStopping')\n",
        "\n",
        "# 2) Instantiate the callbacks\n",
        "# instantiate a tensorboard callback object\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "# 2a) Add early stopping callback\n",
        "stop = EarlyStopping(\n",
        "    monitor='val_accuracy', min_delta=0.0001, patience=2, verbose=0,\n",
        "    mode='auto', restore_best_weights=True\n",
        ")\n",
        "\n",
        "# 3) Build the model architecture\n",
        "# 3a) Add MaxNorm weight contraint to model architecture\n",
        "# 3b) Add Dropout to model architecture\n",
        "model = Sequential([\n",
        "  Flatten(input_shape=(28,28)),\n",
        "  Dense(128, kernel_constraint=MaxNorm(3), activation='relu'),\n",
        "  Dropout(0.4),\n",
        "  Dense(128, kernel_constraint=MaxNorm(3), activation='relu'),\n",
        "  Dropout(0.4),\n",
        "  Dense(128, kernel_constraint=MaxNorm(3), activation='relu'),\n",
        "  Dropout(0.4),\n",
        "  Dense(10, activation='softmax'),\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Fit model\n",
        "model.fit(X_train, y_train,\n",
        "         epochs=99,\n",
        "         validation_data=(X_test, y_test),\n",
        "         verbose=1,\n",
        "         callbacks=[tensorboard_callback, stop])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFAkmZbVQHu2"
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbV6zsdQQHu4"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "You will apply regularization strategies inside your neural network today, as you try to avoid overfitting it.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MXbzdBdURod"
      },
      "source": [
        "# 2. Deploy (Learn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11LJtU9MUVD5"
      },
      "source": [
        "## Overview\n",
        "\n",
        "You've built a nice image classification model, but it's just sitting your Jupyter Notebook. What now? <br>\n",
        "Well you can deploy to some down stream application. TensorFlow supports three ways of deploying its models:\n",
        "\n",
        "- In-Browser with `TensorFlow.js`\n",
        "- API with TensorFlow Serving (TFX) or another Framework\n",
        "- On-Device with TensorFlow Lite\n",
        "\n",
        "You are already familiar with deploying a model as an API from Unit 3, so we will focus on deploying a model in browser. Both methods rely on the same core idea: save your weights and architecture information, load those parameters into application, and perform inference.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHca-4H2UU8p"
      },
      "source": [
        "## Follow Along"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mP0IIAe4LFW5"
      },
      "source": [
        "### ModelCheckpoint callback\n",
        "\n",
        "`save_weights_only = True` means the weights and biases are saved at the end of each epoch<br>\n",
        "`save_weights_only = False` means the model architecture is saved in addition to the weights and biases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmtMJBBriD2Q"
      },
      "source": [
        "%%time\n",
        "import tensorflow as tf\n",
        "\n",
        "# set up checkpoint callback\n",
        "checkpoint_filepath = \"./weights_best.h5\"\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    verbose=1,\n",
        "    save_best_only=True)\n",
        "\n",
        "def create_model():\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "          Flatten(input_shape=(28,28)),\n",
        "          Dense(128, activation='relu'),\n",
        "          Dense(128, activation='relu'),\n",
        "          Dense(128, activation='relu'),\n",
        "          Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "model = create_model()\n",
        "\n",
        "model.fit(X_train, y_train,\n",
        "          epochs=3,\n",
        "          validation_data=(X_test,y_test),\n",
        "          verbose=2,\n",
        "          callbacks=[checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkktHWRDo-1z"
      },
      "source": [
        "Evaluate the model on the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5QoKB14LNMb"
      },
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOaEJc5iJNrz"
      },
      "source": [
        "Check that instantiating an empty model followed by loading the previously<br>\n",
        "saved weights and then training the model recovers the same result!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eH9jo6gl-SMq"
      },
      "source": [
        "# create a fresh version of the compiled model\n",
        "new_model = create_model()\n",
        "\n",
        "# load the saved weights\n",
        "new_model.load_weights(checkpoint_filepath)\n",
        "new_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQl7g-ImLUY9"
      },
      "source": [
        "# Verify that evaluating the model using the saved weights gives the same result as before\n",
        "new_model.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGzTQU1_Lnsb"
      },
      "source": [
        "### Save Entire Model\n",
        "This time we'll save the model architecture as well as the weights & biases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdhgFQD-Hm11"
      },
      "source": [
        "%%time\n",
        "import tensorflow as tf\n",
        "\n",
        "# set up checkpoint callback\n",
        "checkpoint_filepath = \"./weights_best.h5\"\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=False,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    verbose=1,\n",
        "    save_best_only=False)\n",
        "\n",
        "# build and compile the model\n",
        "model = create_model()\n",
        "\n",
        "# fit the model\n",
        "model.fit(X_train, y_train, epochs=2,\n",
        "          validation_data=(X_test,y_test),\n",
        "          verbose=2,\n",
        "          callbacks=[checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evalute model\n",
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "z7PSCaHDfTVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kURHysaUL87B"
      },
      "source": [
        "Load the saved model and check that we recover the original result using the saved weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCgOlQalMIQr"
      },
      "source": [
        "# Load the saved model and the weights and biases\n",
        "saved_model = tf.keras.models.load_model(checkpoint_filepath)\n",
        "\n",
        "# Check that we retrieved the saved model architecture correctly\n",
        "saved_model.summary()\n",
        "\n",
        "# check that we get the original result using the saved weights\n",
        "saved_model.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtwERZP4KPCb"
      },
      "source": [
        "### Alternate way to save the model, \"manually\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXtxgTgYL0UC"
      },
      "source": [
        "# Save the entire model as a SavedModel.\n",
        "!mkdir -p saved_model\n",
        "model.save('saved_model/my_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FekOOGMOKEK5"
      },
      "source": [
        "# Load the saved model and the weights and biases\n",
        "saved_model = tf.keras.models.load_model('saved_model/my_model')\n",
        "\n",
        "# Check that we retrieved the saved model architecture correctly\n",
        "saved_model.summary()\n",
        "\n",
        "# check that we get the original result using the saved weights\n",
        "saved_model.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to access the weights and biases from your model"
      ],
      "metadata": {
        "id": "TqVtBhaMidbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wts = saved_model.weights\n",
        "print(len(wts))"
      ],
      "metadata": {
        "id": "pw0uFqDUgd-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Weights contain the weights and biases for each layer\n",
        "#   weights arranged in 2-D arrays, one column for each neuron\n",
        "#   biases for the layer come next in a vector\n",
        "\n",
        "for index in range(8):\n",
        "  print(wts[index].shape)"
      ],
      "metadata": {
        "id": "M2ALmujlgveZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wts[7]"
      ],
      "metadata": {
        "id": "qkJwBvfJhkP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i7OMOhzc6WT"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "You will be expected to be able to export your model weights and architecture on the assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXnXxPCZiAb2"
      },
      "source": [
        "# 3. Custom Callbacks (Stretch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6vO6xbkiFGb"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Custom callbacks all you to access data at any point during the training: on batch end, on epoch end, on epoch start, on batch start. Our use case today is a simple one. Let's stop training once we reach a benchmark accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCocRF5CiYG_"
      },
      "source": [
        "## Follow Along\n",
        "Review Tensorflow's [Guide to custom callbacks]( https://www.tensorflow.org/guide/keras/custom_callback), then try implementing some simple callbacks of your choice!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iV_Jko8J6aA0"
      },
      "source": [
        "import matplotlib\n",
        "# Specifying the backend to be used before importing pyplot\n",
        "# to avoid \"RuntimeError: Invalid DISPLAY variable\"\n",
        "matplotlib.use('agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "class CustomTrainingPlotCallback(tf.keras.callbacks.Callback):\n",
        "\n",
        "    # This function is called when the training begins\n",
        "    def on_train_begin(self, logs={}):\n",
        "        # Initialize the lists for holding the logs, losses and accuracies\n",
        "        self.losses = []\n",
        "        self.acc = []\n",
        "        self.val_losses = []\n",
        "        self.val_acc = []\n",
        "        self.logs = []\n",
        "\n",
        "    # This function is called at the end of each epoch\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "\n",
        "        # Append the logs, losses and accuracies to the lists\n",
        "        self.logs.append(logs)\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        self.acc.append(logs.get('accuracy'))\n",
        "        self.val_losses.append(logs.get('val_loss'))\n",
        "        self.val_acc.append(logs.get('val_accuracy'))\n",
        "\n",
        "    # This function is called at the end of training\n",
        "    def on_train_end(self, epoch, logs={}):\n",
        "\n",
        "        # Get number of epochs\n",
        "        N = np.arange(0, len(self.losses))\n",
        "\n",
        "        # You can chose the style of your preference\n",
        "        plt.style.use(\"seaborn\")\n",
        "\n",
        "        # Plot train loss, train acc, val loss and val acc against epochs passed\n",
        "        plt.figure()\n",
        "        plt.plot(N, self.acc, label = \"train_acc\")\n",
        "        plt.plot(N, self.val_acc, label = \"val_acc\")\n",
        "        plt.title(\"Training Loss and Accuracy [Epoch {}]\".format(epoch))\n",
        "        plt.xlabel(\"Epoch #\")\n",
        "        plt.ylabel(\"Loss/Accuracy\")\n",
        "        plt.legend()\n",
        "        plt.savefig('./Training-and-Validation-Performance.png')\n",
        "        plt.show()\n",
        "\n",
        "stop = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=3)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "      Flatten(input_shape=(28,28)),\n",
        "      Dense(128, activation='relu'),\n",
        "      Dense(128, activation='relu'),\n",
        "      Dense(128, activation='relu'),\n",
        "      Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='nadam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train,\n",
        "          epochs=20,\n",
        "          validation_data=(X_test,y_test),\n",
        "          callbacks=[CustomTrainingPlotCallback(), stop])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOisQxYlibOi"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "Experiment with improving our custom callback function."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Bonus) Deploy a Simple Interactive Application"
      ],
      "metadata": {
        "id": "KDN8C22oo3Sp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# normalize data\n",
        "max_pixel_value = 255\n",
        "X_train = X_train / max_pixel_value\n",
        "X_test = X_test / max_pixel_value\n",
        "\n",
        "X_train = X_train.reshape((60000, 784))\n",
        "X_test = X_test.reshape((10000, 784))"
      ],
      "metadata": {
        "id": "MkHa5XOBBeaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential([\n",
        "  Dense(64, activation='relu', input_dim=784),\n",
        "  Dense(32, activation='relu'),\n",
        "  Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=10,\n",
        "                    validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "Sh-zPTjzCSIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "lWgou0RfAG7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "from tensorflow.math import top_k\n",
        "\n",
        "def sketch_recognition(img):\n",
        "  img_reshaped = img.reshape((1, 784))\n",
        "  prediction = model.predict(img_reshaped)\n",
        "  result = top_k(prediction, k=5)\n",
        "  pred_labels = result.indices.numpy()\n",
        "  pred_probs = result.values.numpy()\n",
        "  return dict(zip(pred_labels.tolist()[0], pred_probs.tolist()[0]))\n",
        "\n",
        "label = gr.outputs.Label(num_top_classes=5)\n",
        "\n",
        "gr.Interface(fn=sketch_recognition,\n",
        "             inputs=\"sketchpad\",\n",
        "             outputs=label).launch(debug=True)"
      ],
      "metadata": {
        "id": "VeII2BXWAHhu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}