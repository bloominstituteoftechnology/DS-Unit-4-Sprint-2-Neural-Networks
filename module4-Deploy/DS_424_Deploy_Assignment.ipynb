{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DS_424_Deploy_Assignment.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"py37  (Python3)","language":"python","name":"py37"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"nteract":{"version":"0.22.4"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"NGGrt9EYlCqY"},"source":["\n","\n","\n","# *Data Science Unit 4 Sprint 2 Assignment 4*\n","\n","Continue to use TensorFlow Keras & a sample of the [Quickdraw dataset](https://github.com/googlecreativelab/quickdraw-dataset) to build a sketch classification model. The dataset has been sampled to only 10 classes and 10000 observations per class. Apply regularization techniques to your model. \n","\n","**Don't forget to switch to GPU on Colab!**\n","\n","\n","## Objectives \n","\n","In lecture, you were exposed to several regularization techniques: Lp space regularization, Max Norm weight constraints, and dropout regularization.\n","\n","1: Explore the effects of these regularization techniques on model performance and on the learned model weights. \n","\n","2: Review how to save a model and how to retrieve the saved model from a file.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ptJ2b3wk62Ud"},"source":["#### Import libraries"]},{"cell_type":"code","metadata":{"id":"USXjs7Hk71Hy","executionInfo":{"status":"ok","timestamp":1639160983492,"user_tz":480,"elapsed":2756,"user":{"displayName":"Joseph catanzarite","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmMRaw8NgoEDqzevEZ6b18iOYdEH9nWTZeaFBW=s64","userId":"16649206137414945374"}}},"source":["# native libraries \n","import os\n","from time import time \n","\n","# data analysis libraries \n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# deep learning libraries \n","import tensorflow as tf\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n","from tensorflow.keras.layers import Flatten, Dense, Dropout\n","from tensorflow.keras.layers import ReLU\n","from tensorflow.keras.initializers import GlorotUniform\n","from tensorflow.keras.utils import get_file\n","\n","\n","\n","# regularizers \n","from tensorflow.keras.regularizers import l2, l1\n","from tensorflow.keras.constraints import MaxNorm\n","\n","# required for compatibility between sklearn and keras\n","from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n","\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import train_test_split\n","\n","# native python unit test library\n","from unittest import TestCase\n","\n","%matplotlib inline"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nDjNx1aVBfGU"},"source":["-----\n","# Part 1: GridSearch Experiments \n","\n","The next set of experiments will involve gridsearching regularization parameter values. \n","\n","The rest of the notebook will actually require very little coding on your part. Instead, the focus is for you to run those gridsearches and answer the questions at the end of each experiment. Those questions are designed to help you capture the insights that there are to learn from each of the experiments. \n","\n","All of the following experiments are designed to help you better understand the relationship between the various regularization techniques and how they affect model performance. \n","\n","\n","### Build Model\n","\n","Let's build out the model that we'll be using all throughout our experiments. \n","\n","Remember that **the whole point of regularization is to prevent overfitting.**\n","\n","\n","![](https://hackernoon.com/hn-images/1*vuZxFMi5fODz2OEcpG-S1g.png)\n","\n","Overfitting happens when our models are too complex, so in order to see a benefit from the use of regularization techniques we need to build a relatively complex model. \n","\n","Having said that, you might not have the computational resource to be able to train a complex model in a reasonable amount of time. So if this describes you, then you might want to consider using `build_simple_model`. Otherwise, use `build_complex_model`. \n","\n","In this notebook, we'll use  `build_complex_model` to run our experiments. \n","\n","**NOTE:** Whichever function you end up using to build a model, take time to read through the code and make sure you understand what is happening. "]},{"cell_type":"markdown","source":["### Load the `quickdraw10` data set\n","using code provided in the Module 3 assignment."],"metadata":{"id":"BhyxXDgXfuHs"}},{"cell_type":"code","source":["# YOUR CODE HERE"],"metadata":{"id":"9m7GEmeWfzYr","executionInfo":{"status":"ok","timestamp":1639160983493,"user_tz":480,"elapsed":12,"user":{"displayName":"Joseph catanzarite","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmMRaw8NgoEDqzevEZ6b18iOYdEH9nWTZeaFBW=s64","userId":"16649206137414945374"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["### Build the model"],"metadata":{"id":"X3qyAkpDf2Uz"}},{"cell_type":"code","source":["def build_complex_model(Lp_reg=None, reg_penalty=None, dropout_prob=0.0, maxnorm_wc=None):\n","    \"\"\"\n","    Build and return a regularized 3 hidden layer FCFF model \n","    \n","    Parameters\n","    ----------\n","    Lp_reg: None or object\n","        If object, Lp_reg is either l1 or l2 regularization \n","        If None, that means that l1 or l2 regularization will not be used.\n","     \n","    reg_penalty: None or float\n","        If float, reg_penalty is a value typically between 1.0 and 0.0001\n","        This is the regularization strength for l1 or l2 \n","        \n","        \n","    dropout_prob: float\n","        This is the probability that dropout regularization will exclude a node from a training iteration. \n","        If this value is 0.0, that means that dropout will not be used. \n","        \n","    maxnorm_wc: None or float\n","        If float, maxnorm_wc is the weight constraint that is used for Max Norm regularization\n","        If None, that means that Max Norm regularization will not be used.\n","        \n","        \n","    Return\n","    ------\n","    model: compiled Keras model\n","    \"\"\"\n","\n","    # specify the input size\n","    input_dim = # YOUR CODE HERE\n","\n","    # specify the number of neurons in the output layer\n","    n_output = # YOUR CODE HERE\n","\n","    # if reg_type is not None, then pass in the penalty strength to whatever form of Lp space regularization this is \n","    if Lp_reg is not None:\n","        Lp_regularizer = Lp_reg(reg_penalty)\n","    else:\n","        Lp_regularizer = None\n","                \n","    if maxnorm_wc is not None:\n","        wc = MaxNorm(max_value=maxnorm_wc)\n","    else:\n","        wc = None\n","\n","\n","    # instantiate Sequential class\n","    model = Sequential([    \n","\n","    # hidden layer 1\n","    Dense(500, kernel_regularizer=Lp_regularizer , kernel_constraint=wc, input_dim=input_dim), # remember that Keras refers to weight matrix as a kernel, i.e. weights = kernel\n","    # act func 1\n","    ReLU(negative_slope=0.01),\n","    Dropout(dropout_prob),\n","\n","    # hidden layer 2\n","    Dense(250, kernel_regularizer=Lp_regularizer, kernel_constraint=wc),\n","    # act func 2\n","    ReLU(negative_slope=0.01),\n","    Dropout(dropout_prob),\n","\n","    # hidden layer 3\n","    Dense(100, kernel_regularizer=Lp_regularizer, kernel_constraint=wc),\n","    # act func 3\n","    ReLU(negative_slope=0.01),\n","    Dropout(dropout_prob),\n","\n","    # output layer   \n","    Dense(n_output, activation=\"softmax\")  \n","\n","    ])\n","    # compile model \n","    model.compile(loss=\"sparse_categorical_crossentropy\", \n","                 optimizer=\"adam\", \n","                 metrics=[\"accuracy\"])\n","    \n","    return model"],"metadata":{"id":"Rl7fR1Vvqauv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tf7sg_qWBfGV"},"source":["Again, only use `build_simple_model` instead of `build_complex_model` if you're working on a machine with very limited computational resources. "]},{"cell_type":"code","metadata":{"id":"2ZDOyeFUBfGV","executionInfo":{"status":"aborted","timestamp":1639160983494,"user_tz":480,"elapsed":10,"user":{"displayName":"Joseph catanzarite","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmMRaw8NgoEDqzevEZ6b18iOYdEH9nWTZeaFBW=s64","userId":"16649206137414945374"}}},"source":["# def build_simple_model(Lp_reg=None, reg_penalty=None, dropout_prob=0, maxnorm_wc=None):\n","#     \"\"\"\n","#     Build and return a regularized 1 hidden layer FCFF model \n","    \n","#     Parameters\n","#     ----------\n","#     Lp_reg: None or object\n","#         If object, Lp_reg is either l1 or l2 regularization \n","#         If None, that means that l1 or l2 regularization will not be used.\n","     \n","#     reg_penalty: None or float\n","#         If float, reg_penalty is a value typically between 1.0 and 0.0001\n","#         This is the regularization strength for l1 or l2 \n","        \n","        \n","#     dropout_prob: float\n","#         This is the probability that dropout regularization will exclude a node from a training iteration. \n","#         If this value is 0.0, that means that dropout will not be used. \n","        \n","#     maxnorm_wc: None or float\n","#         If float, maxnorm_wc is the weight constraint that is used for Max Norm regularization\n","#         If None, that means that Max Norm regularization will not be used.\n","        \n","        \n","#     Return\n","#     ------\n","#     model: compiled Keras model\n","#     \"\"\"\n","\n","#    # specify the input size\n","#    input_dim = # YOUR CODE HERE\n","\n","#    # specify the number of neurons in the output layer\n","#    n_output = # YOUR CODE HERE\n","\n","#     if Lp_reg is not None:\n","#         Lp_regularizer = Lp_reg(reg_penalty)\n","#     else:\n","#         Lp_regularizer = None\n","\n","#     # instantiate Sequential class\n","#     model = Sequential([\n","\n","#     # hidden layer 1\n","#     Dense(128,  kernel_regularizer=Lp_regularizer, kernel_constraint=maxnorm_wc, input_dim=input_dim), # remember that Keras refers to weight matrix as a kernel, i.e. weights = kernel\n","#     # act func 1\n","#     ReLU(negative_slope=0.01),\n","#     Dropout(p_dropout),\n","\n","#     # output layer   \n","#     Dense(n_output, activation=\"softmax\")  \n","\n","#     ])\n","#     # compile model \n","#     model.compile(loss=\"sparse_categorical_crossentropy\", \n","#                  optimizer=\"adam\", \n","#                  metrics=[\"accuracy\"])\n","    \n","#     return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RIAKpcTEBfGW"},"source":["Since we'll be using sklearn's `GridsearchCV` class, we need to wrap our Keras models in `KerasClassifier`"]},{"cell_type":"code","source":["# wrap KerasClassifier around build_model for compatibility with sklearn GridsearchCV \n","model = KerasClassifier(build_fn = build_complex_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IstAk4cgeOsX","executionInfo":{"status":"ok","timestamp":1639091862165,"user_tz":480,"elapsed":207,"user":{"displayName":"Joseph catanzarite","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmMRaw8NgoEDqzevEZ6b18iOYdEH9nWTZeaFBW=s64","userId":"16649206137414945374"}},"outputId":"ab6d12b0-948d-483a-fdfe-c2e8c480b521"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n","  \n"]}]},{"cell_type":"markdown","metadata":{"id":"6LJi-YVMBfGW"},"source":["-------\n","\n","# Experiment 1: Identify the relationship between model performance and L2 penalty strength\n","\n","![](https://www.researchgate.net/publication/334159821/figure/fig1/AS:776025558495234@1562030319993/Ridge-regression-variable-selection.png)\n","\n","_Note: <br>\n","In the right panel of the above diagram, **SSE** <br>\n","stands for \"Sum of Squared Errors\".<br>\n","In the left panel, **ESS** is a typo that should read **SSE**._<br>\n","\n","We are going to run a gridsearch solely on the L2 regularization penalty value and see the effect this has on model performance. \n","\n","By running a gridseach on only a single hyperparameter (while using the same data and model) we can isolate the effect of that hyperparameter. <br><br>\n"]},{"cell_type":"code","metadata":{"id":"dxJ5Bc6HF2C0","executionInfo":{"status":"ok","timestamp":1639092061185,"user_tz":480,"elapsed":170,"user":{"displayName":"Joseph catanzarite","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmMRaw8NgoEDqzevEZ6b18iOYdEH9nWTZeaFBW=s64","userId":"16649206137414945374"}}},"source":["# build out our hyperparameter dictionary \n","hyper_parameters = {\n","    # take note that Lp_reg penalty/strength values are in powers of 10 \n","    \"reg_penalty\": [10.0, 1.0, 0.1, 0.01, 0.001, 0.0001, 0.00001], \n","    # Since we only want to test l2, provide l2 as the sole option \n","    \"Lp_reg\": [l2],\n","    # default is 1, in order to change it we must provide value here because we can't provide a parameter value for model.fit() directly when using gridsearch\n","    # protip: consider changing epochs to 1 if the gridsearch run-time is too long for you\n","    \"epochs\": [3] \n","}"],"execution_count":7,"outputs":[]},{"cell_type":"code","source":["start=time()\n","# takes about 7 min on Colab with GPU\n","# Create and run Grid Search\n","grid = GridSearchCV(estimator=model, \n","                    param_grid=hyper_parameters, \n","                    n_jobs=-3, \n","                    verbose=1, \n","                    cv=3)\n","\n","grid_result = grid.fit(X_train, y_train)\n","end=time()"],"metadata":{"id":"qMRyjO6Gbl5I"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DZb-QpxvBfGX"},"source":["print(\"Gridsearch runtime {0:.3} mins\".format( (end-start)/60 ))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FS1Vlh21BfGY"},"source":["# use the mean accuracy from the CV splits for determining best model score \n","means = grid_result.cv_results_['mean_test_score']\n","stds = grid_result.cv_results_['std_test_score']\n","params = grid_result.cv_results_['params']\n","\n","# move l2 penalty values outside of dictionary and into a list\n","param_values = [param_dict[\"reg_penalty\"] for param_dict in params]\n","\n","# plot accuracy vs l2_reg_penalty\n","plt.figure(figsize=(20,6))\n","plt.grid()\n","\n","# this plot is using the std of the CV splits to plot error bars however those values are so small that they aren't visable\n","plt.errorbar(param_values, means, yerr=stds, ecolor=\"orange\")\n","plt.xscale(\"log\") # use a log scale for ease of reading, recall that l2_reg_penalty were in powers of 10 \n","plt.title(\"L2 Regularization: Model Accuracy vs L2 Penalty Strength\")\n","plt.ylabel(\"Validation Accuracy\", )\n","plt.xlabel(\"L2 Penalty Strength usng a Log Scale\");"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vc4hfY52BfGY"},"source":["### Observations\n","\n","Write down some observations. What do you notice from the plot?"]},{"cell_type":"markdown","metadata":{"deletable":false,"nbgrader":{"cell_type":"markdown","checksum":"089b55b5a84d9c96c51fd341d9e6c74f","grade":true,"grade_id":"cell-010212fc0915b976","locked":false,"points":0,"schema_version":3,"solution":true,"task":false},"id":"G0Z92bfrBfGY"},"source":["YOUR ANSWER HERE"]},{"cell_type":"markdown","metadata":{"id":"qKoqryhBBfGZ"},"source":["\n","## Compare Weights between the Best Model and a Worse Model \n","\n","Next, we are going to compare the hidden layer weights between the best and worst performing model while taking note of the respective L2 penalty strengths."]},{"cell_type":"code","metadata":{"id":"4Rb3l6GuBfGZ"},"source":["# get the best l2 penalty term \n","best_lr_penalty = grid_result.best_params_[\"reg_penalty\"]\n","\n","# get the best trained model\n","#     a model that is wrapped in the KerasClassifier wrapper doesn't have a .get_weights() method.\n","#     So this is the workaround to generate a version of the model that does have a .get_weights() method\n","best_model = grid_result.best_estimator_.build_fn(Lp_reg=l2, reg_penalty=best_lr_penalty)\n","\n","# get the weights from the best trained model \n","best_weights = best_model.get_weights()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eFxgfRA4BfGZ"},"source":["# train a model using the l2_reg_penalty value that scored the lowest \n","worse_l2_reg_penalty = 10.0\n","\n","worse_model = build_complex_model(Lp_reg=l2, reg_penalty=worse_l2_reg_penalty)\n","\n","# fit model \n","worse_model.fit(X_train, y_train, epochs=1)\n","\n","# get weights from worst performing model \n","worse_weights = worse_model.get_weights()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZJHi51iABfGZ"},"source":["-----\n","## Understanding how Weights and Biases are stored"]},{"cell_type":"markdown","metadata":{"id":"dgo_lKxFBfGZ"},"source":["Let's take a minute to understand that`.get_weights()` returns a list with 8 elements (if you're using `build_complex_model`)."]},{"cell_type":"code","metadata":{"id":"TMdhtUWRBfGZ"},"source":["len(best_weights)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6raIvOpOBfGa"},"source":["There are **weights matrices and bias vectors between each layer** and we have 5 layers. The last 4 layers are composed of neurons. \n","\n","- Input\n","- Hidden 1\n","- Hidden 2\n","- Hidden 3\n","- Output \n","\n","So we should have a weight matrix and a bias vector from each neuron layer, <br>which accounts for $4 + 4 = 8$ elements in the list.\n","\n","\n","#### Index for Weight Matrices \n","If you index for a weight matrix, you can see its shape and that they are indeed matrices. \n","\n","Notice how you can see the dims of the layers that the matrices are sandwiched between?\n","\n","The input layer has 784 dims and hidden layer 1 has 500 neurons. <br>\n","So the first weight matrix should have a column of weights for each neuron; and each column has 784 weights.\n","Given this understanding, the numbers you see in the shapes of the weight and bias arrays should make sense. "]},{"cell_type":"code","metadata":{"id":"p8Wmj_pjBfGa"},"source":["# bewteen input and 1st hidden layer\n","best_weights[0].shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bRnY8DHHBfGa"},"source":["# bewteen 1st and 2nd hidden layer\n","best_weights[2].shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aCQ-QZI4BfGb"},"source":["# bewteen 2nd and 3rd hidden layer\n","best_weights[4].shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LiD6Ij2BBfGb"},"source":["# bewteen 3rd hidden layer and output layer\n","best_weights[6].shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v9rAIBBuBfGb"},"source":["#### Index for the bias vectors\n","\n","The shapes of the bias vectors should exactly match up the dims/nodes of each layer (excluding the input layer). "]},{"cell_type":"code","metadata":{"id":"j5c97T5NBfGb"},"source":["# for hidden layer 1 \n","best_weights[1].shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YWABwgJgBfGc"},"source":["# for hidden layer 2 \n","best_weights[3].shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"juFKoCftBfGc"},"source":["# for hidden layer 3\n","best_weights[5].shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_uul71OiBfGc"},"source":["# for output layer\n","best_weights[7].shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7uwdFhiWBfGc"},"source":["-----\n","\n","### Back to our Analysis of L2 space regularization (also known as Ridge)\n","\n","To gauge the performance effect of L2 regularization, we'll compare the weights for the 1st hidden layer from the best and worst performing models from our hyperparameter search space, as well as with the initial weight values that are randomly sampled from the GlorotUniform distribution.\n","\n","[**Check out the Keras docs for the Dense layer**](https://keras.io/api/layers/core_layers/dense/), you'll see that GlorotUniform is the default weight initializer. \n","\n","Read the docs to figure out how to get the weights from a Keras dense layer.\n","Also have a look at this helpful post on StackOverflow: [**How to view initialized weights (i.e. before training)?**](https://stackoverflow.com/questions/46798708/how-to-view-initialized-weights-i-e-before-training)\n"]},{"cell_type":"markdown","metadata":{"id":"UdmuttujBfGd"},"source":["Before we compare weights, let's take note of the following. \n","\n","Both `best_weights[0]` or `worse_weights[0]` are matrices with shape `(784, 500)`. \n","\n","If we flatten them, then we get `784 * 500 = 392000` weights. What does this mean exactly?\n","\n","Remember that we are working with the Fully Connected Feed-Forward model which looks something like this. \n","\n","![](https://pyimagesearch.com/wp-content/uploads/2016/08/simple_neural_network_header.jpg)\n","\n","In Fully Connected neural network models, the outputs from a layer become the inputs for the next layer. Each output from a layer is passed as an input to all the nodes in next layer.<br>\n","Our input layer has $784$ output weights which are the pixel values in the image. Each neuron (or node) in hidden layer $1$ also has $784$ weights, one for each pixel input.  Hidden layer 1 has $500$ neurons. So the weights are conveniently represented as a **weights matrix** with $784$ rows (one row for each pixel in the input image) and $500$ columns (one column for each neuron in the layer). The $i$th column of the weights matrix holds the $784$ weights $\\textbf{w}_{i}$ belonging to the $i$th neuron in the layer.\n","\n","To keep our analysis simple, we are going to analyze only the weights corresponding to the first neuron in hidden layer $1$, whose weights $\\textbf{w}_{1}$ are the first column of the weight matrix.  <br><br>\n","\n","We will observe the effect of L2 regularization on neuron 1 in hidden layer 1.\n"]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"a3f75126a80a74dd71fb0a9ab2b8b89f","grade":false,"grade_id":"cell-7882876b8973bc7b","locked":false,"schema_version":3,"solution":true,"task":false},"id":"xomeoSW4BfGd"},"source":["# index for the 1st column (784 entries) in the 1st hidden layer weights in best_weights and save to best_hidden_weights\n","\n","# index for the 1st column (784 entries) in the 1st hidden layer weights in worse_weights and save to worse_hidden_weights\n","\n","# Keras models randomly samples from the GlorotUniform distribution for the initial values of model weights \n","# instantiate GlorotUniform and sample 784 weights and save to initial_weight_values\n","\n","# Build a data frame with these 3 vectors as columns\n","\n","# YOUR CODE HERE\n","raise NotImplementedError()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zaRHQzP2BfGe"},"source":["# move all weights to a dataframe for ease of analysis \n","cols = [\"best_hidden_weights\", \"worse_hidden_weights\", \"initial_weight_values\"]\n","data = [best_hidden_weights, worse_hidden_weights, initial_weight_values]\n","df = pd.DataFrame(data=data).T\n","df.columns = cols"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GTxYblbIBfGe"},"source":["# check out the statistics for each weight column \n","df.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4Np8LOGIBfGe"},"source":["# plot the distributions for each weight column \n","df.hist(figsize=(20,12));"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PAtwX9QfBfGe"},"source":["## Observations \n","\n","Take a look at the statistical table and the plots. Then answer the following questions. \n","\n","**How do the hidden layer weights from the best performing model compare to the initial weight values?**"]},{"cell_type":"markdown","metadata":{"deletable":false,"nbgrader":{"cell_type":"markdown","checksum":"5eacb66f89b216ae3b4bb3c4b7bd6d38","grade":true,"grade_id":"cell-6add7cc400c4c716","locked":false,"points":0,"schema_version":3,"solution":true,"task":false},"id":"GcB8C891BfGe"},"source":["YOUR ANSWER HERE"]},{"cell_type":"markdown","metadata":{"id":"6ssnJofqBfGf"},"source":["**What was the effect of using a small L2 penalty value (regularization constant)?**"]},{"cell_type":"markdown","metadata":{"deletable":false,"nbgrader":{"cell_type":"markdown","checksum":"92ac1689b72d727ab7f4d261c5e76daa","grade":true,"grade_id":"cell-5b4f11bba2d49639","locked":false,"points":0,"schema_version":3,"solution":true,"task":false},"id":"N3PyRkv_BfGf"},"source":["YOUR ANSWER HERE"]},{"cell_type":"markdown","metadata":{"id":"_dIM4qL5BfGf"},"source":["**What was the effect of using a large L2 penalty value?**"]},{"cell_type":"markdown","metadata":{"deletable":false,"nbgrader":{"cell_type":"markdown","checksum":"3048a6d2805f61d6fb58372c42c3ac54","grade":true,"grade_id":"cell-0a30b62e5e119555","locked":false,"points":0,"schema_version":3,"solution":true,"task":false},"id":"yhaLO6-CBfGf"},"source":["YOUR ANSWER HERE"]},{"cell_type":"markdown","metadata":{"id":"8rDYEPbsBfGg"},"source":["**Given what you know about L2 regularization, are you surprised by these results?**"]},{"cell_type":"markdown","metadata":{"deletable":false,"nbgrader":{"cell_type":"markdown","checksum":"d8a2034f67badfe53f601873f7026dc0","grade":true,"grade_id":"cell-c04d067161064011","locked":false,"points":0,"schema_version":3,"solution":true,"task":false},"id":"m3Nt5LmOBfGg"},"source":["YOUR ANSWER HERE"]},{"cell_type":"markdown","metadata":{"id":"0SI5IBq8BfGg"},"source":["----\n","\n","# Experiment 2: Identify the relationship between model performance and Max Norm Weight Constraint\n","\n","![](https://qph.fs.quoracdn.net/main-qimg-9d0dbf8074761b541ba80543ddfc9f73.webp)\n","\n","Recall from lecture that the **norm** of a vector is another word for the **length** of the vector.\n","\n","`MaxNorm` weight constraint puts a limit on the norm of the weight vector.\n","\n","The effect that Lp regularization and `MaxNorm` regularization have on the weights is similar, but they go about it in different ways. \n","\n","While Lp regularization (L1/Lasso and L2/Ridge) shrink the weight values by imposing constraints on their L1 and L2 norms, `MaxNorm` regularization shrinks the weight values by imposing a limit on the norm of a weight vector. Here's how it works: if an update would push the norm of a weight vector above MaxNorm, a scale factor is applied to all the weights so as to shrink the norm back to MaxNorm. \n","\n","In this experiment, we'll run another gridseach, similar to the one we ran for L2 regularization in the previous experiment. This time, we are going to gauge the effect of `MaxNorm` regularization on model performance and the distribution of the learned weights. As in the previous experiment, we will simplify our analysis by focusing on the weights for the first neuron in the first hidden layer."]},{"cell_type":"markdown","metadata":{"id":"ChP-sILzBfGg"},"source":["Since we already built our model, we just need to update the `hyper_parameters` dictionary. "]},{"cell_type":"code","metadata":{"id":"5xdqcr6WBfGg"},"source":["# build out our hyperparameter dictionary \n","hyper_parameters = {\n","    \n","    \"maxnorm_wc\": np.linspace(0.5, 10.0, num=20), \n","    # default is 1, in order to change it we must provide value here because we can't provide a parameter value for model.fit() directly when using gridsearch\n","    # protip: consider changing epochs to 1 if the gridsearche run-time are too long for you    \n","    \"epochs\": [1] \n","}\n","\n","hyper_parameters"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AGQiKwo0BfGh"},"source":["start=time()\n","# Create and run Grid Search\n","grid = GridSearchCV(estimator=model, \n","                    param_grid=hyper_parameters, \n","                    n_jobs=-2, \n","                    verbose=1, \n","                    cv3)\n","\n","grid_result = grid.fit(X_train, y_train)\n","end=time()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pslQN3T-BfGh"},"source":["print(\"Gridsearch runtime {0:.3} mins\".format( (end-start)/60 ))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cxaAl3cvBfGh"},"source":["# use the mean accuracy from the CV splits for determining best model score \n","means = grid.cv_results_['mean_test_score']\n","stds = grid.cv_results_['std_test_score']\n","params = grid.cv_results_['params']\n","\n","# move l2 penalty values outside of dictionary and into a list\n","param_values = [param_dict[\"maxnorm_wc\"] for param_dict in params]\n","\n","# plot accuracy vs l2_reg_penalty\n","plt.figure(figsize=(20,6))\n","plt.grid()\n","plt.errorbar(param_values, means, yerr=stds, ecolor=\"orange\")\n","plt.title(\"MaxNorm weight constraint: Model Accuracy vs MaxNorm\")\n","plt.ylabel(\"Validation Accuracy\", )\n","plt.xlabel(\"Max Norm for Weight Vector \");"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"cfe7bd54a7a14ba7ee63c88d6d1828b4","grade":false,"grade_id":"cell-f67372e0b9b30614","locked":false,"schema_version":3,"solution":true,"task":false},"id":"D3aeaN_NBfGi"},"source":["# get the best value of max norm from grid and save to best_max_norm_val\n","\n","# get the best trained model from grid and save to best_model\n","\n","# get the weights from the best trained model and save to best_weights\n","\n","# YOUR CODE HERE\n","raise NotImplementedError()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"udo9kxwWBfGi"},"source":["best_max_norm_val"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aYdmRUJSBfGi"},"source":["# we can verify that the norm of our weights are indeed below the maximum allowed value \n","np.linalg.norm(best_weights[0][0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"67f65bd636e3b3b3bc7d20c02ba6b666","grade":false,"grade_id":"cell-e752c1a8c853985d","locked":false,"schema_version":3,"solution":true,"task":false},"id":"3ew76uwZBfGi"},"source":["# train a model using the max_norm_val value that scored the lowest \n","\n","# build a model using build_complex_model and worse_max_norm_val and save it to worse_model\n","\n","# fit model \n","\n","# get weights from worse performing model \n","\n","\n","# YOUR CODE HERE\n","raise NotImplementedError()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Sample the initial weights from the `Glorot Uniform Weight Initializer`, for comparison"],"metadata":{"id":"GgVx0Yg-7OwJ"}},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"a38d5d4db707124b31a662fb4743b049","grade":false,"grade_id":"cell-5c1aa4543e68487d","locked":false,"schema_version":3,"solution":true,"task":false},"id":"DEx3m9vVBfGj"},"source":["# index for the 1st hidden layer weights in best_weights and save to best_hidden_weights\n","\n","# index for the 1st hidden layer weights in worse_weights and save to worse_hidden_weights\n","\n","# Keras models randomly samples from the GlorotUniform distribution for the initial values of model weights \n","\n","# instantiate GlorotUniform and sample 500 weights and save to initial_weight_values\n","# hint: use shape=(1, 500)\n","\n","\n","# YOUR CODE HERE\n","raise NotImplementedError()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ck8kH2LeBfGk"},"source":["# move all weights to a dataframe for ease of analysis \n","cols = [\"best_hidden_weights\", \"worse_hidden_weights\", \"initial_weight_values\"]\n","data = [best_hidden_weights, worse_hidden_weights, initial_weight_values]\n","df_maxnorm= pd.DataFrame(data=data).T\n","df_maxnorm.columns = cols"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jiT661fNBfGk"},"source":["df_maxnorm.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N6HmKEFHBfGl"},"source":["# plot the distributions for each weight column \n","df_maxnorm.hist(figsize=(20,12));"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GJLb71QuBfGm"},"source":["## Observations \n","\n","Take a look at the statistical table and the plots. Then answer the following questions. \n","\n","**How do the hidden layer weights from the best performing model compare to the initial weight values?**"]},{"cell_type":"markdown","metadata":{"deletable":false,"nbgrader":{"cell_type":"markdown","checksum":"f43fe1110cdcea8d1e4b432fe78d4e49","grade":true,"grade_id":"cell-40a44d19694941b8","locked":false,"points":0,"schema_version":3,"solution":true,"task":false},"id":"9Ow24epzBfGm"},"source":["YOUR ANSWER HERE"]},{"cell_type":"markdown","metadata":{"id":"1C-q89taBfGm"},"source":["**What was the effect of using the weight constraint value in MaxNorm in the best performing model?**"]},{"cell_type":"markdown","metadata":{"deletable":false,"nbgrader":{"cell_type":"markdown","checksum":"b1c59c58a5abdbc0b509983821198dba","grade":true,"grade_id":"cell-4f9e1e134124e512","locked":false,"points":0,"schema_version":3,"solution":true,"task":false},"id":"JEnRcaxEBfGm"},"source":["YOUR ANSWER HERE"]},{"cell_type":"markdown","metadata":{"id":"LY9joSwUBfGn"},"source":["**What was the effect of using the weight constraint value in MaxNorm in the worse performing model?**"]},{"cell_type":"markdown","metadata":{"deletable":false,"nbgrader":{"cell_type":"markdown","checksum":"0062b4ddfad487c39633c37f4710b752","grade":true,"grade_id":"cell-4c289ce70c34048a","locked":false,"points":0,"schema_version":3,"solution":true,"task":false},"id":"8_Pw4Jb1BfGn"},"source":["YOUR ANSWER HERE"]},{"cell_type":"markdown","metadata":{"id":"fWmTydgiBfGn"},"source":["**Given what you know about MaxNorm regularization, are you surprised by these results?**"]},{"cell_type":"markdown","metadata":{"deletable":false,"nbgrader":{"cell_type":"markdown","checksum":"3c36931d3532a8cbcb4ea0c956378728","grade":true,"grade_id":"cell-77366a912217da5d","locked":false,"points":0,"schema_version":3,"solution":true,"task":false},"id":"EiAQlNHuBfGn"},"source":["YOUR ANSWER HERE"]},{"cell_type":"markdown","metadata":{"id":"l_INtS2vBfGn"},"source":["-----\n","# Experiment 3: Identify the relationship between model performance and Dropout\n","\n","\n","![](https://miro.medium.com/max/981/1*EinUlWw1n8vbcLyT0zx4gw.png)\n","\n","In the 3rd experiment, we will use gridsearch to see how varying the value of the the dropout probability affects model performance. \n","\n","Recall from lecture that dropout tends to perform best when used with `MaxNorm` regularization. Since this is the case, we will gridsearch both dropout probability and the weight constraint for `MaxNorm`. \n","\n","If interested, feel free to read (or just skim) through the original publication on [**Drop Out**](https://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf). \n","\n","**Key Take aways:** \n","\n","1. During training, dropout will probabilistically \"turn off\" some neurons in the layer that dropout is implemented in. \n","2. During inference (ie. making predictions on the test set) all neurons are used (i.e. no dropout is applied).\n","3. Dropout works best when used with MaxNorm\n"]},{"cell_type":"code","source":["# build out our hyperparameter dictionary \n","hyper_parameters = {\n","    # for the sake of runtime, let's vary maxnorm_wc between 0.5 and 5.0\n","    \"maxnorm_wc\": np.linspace(0.5, 5, num=5),\n","    # take note that l1_reg_penalty values are in powers of 10 \n","    \"dropout_prob\": np.linspace(0.0, 0.6, num=5), \n","    \"epochs\": [1] # default is 1, in order to change it we must provide value here because we can provide a parameter value for model.fit() directly when using gridsearch\n","}\n","\n","hyper_parameters"],"metadata":{"id":"ce81CswWlP1N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["start=time()\n","# Create and run Grid Search\n","grid = GridSearchCV(estimator=model, \n","                    param_grid=hyper_parameters, \n","                    n_jobs=-2, \n","                    verbose=1, \n","                    cv=3)\n","\n","grid_result = grid.fit(X_train, y_train)\n","end=time()"],"metadata":{"id":"6CN5X-D2lTS4"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vg4ifWNtBfGo"},"source":["print(\"Gridsearch runtime {0:.3} mins\".format( (end-start)/60 ))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CqO15wkmBfGo"},"source":["# use the mean accuracy from the CV splits for determining best model score \n","means = grid.cv_results_['mean_test_score']\n","stds = grid.cv_results_['std_test_score']\n","params = grid.cv_results_['params']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2-v92GwGBfGo"},"source":["Since there are 2 independent hyperparameters which affect the validation accuracy (`dropout_prob` and `maxnorm_wc`), <br> \n","we need a two-dimensional plot. A heat map will work. "]},{"cell_type":"code","metadata":{"id":"I-W5UmNcBfGp"},"source":["dropout_prob_list = [  param_dict[\"dropout_prob\"]  for param_dict in params]\n","maxnorm_wc_list = [  param_dict[\"maxnorm_wc\"]  for param_dict in params]\n","data = [means, dropout_prob_list, maxnorm_wc_list ]\n","\n","cols = [\"val_acc\", \"dropout_prob\", \"maxnorm_wc\"]\n","df_exp3 =pd.DataFrame(data=data).T\n","df_exp3.columns = cols\n","df_exp3.dropout_prob = df_exp3.dropout_prob.round(2)\n","\n","# pivot dataframe in preparation for heat map\n","df_exp3 = df_exp3.pivot(\"maxnorm_wc\", \"dropout_prob\", \"val_acc\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FqilXbTYBfGp"},"source":["# Draw a heatmap with the val_acc values in each cell\n","f, ax = plt.subplots(figsize=(18, 8))\n","sns.heatmap(df_exp3, annot=True,  linewidths=.5, ax=ax);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9dTgA0LEBfGp"},"source":["### Observations \n","\n","We can see the dropout probabilities in the horizontal axis and the `MaxNorm` weight constraint values in the vertical axis. The values in the cells are the validation accuracy that corresponds to a pair of regularization values.\n","\n","Take a look at the heat map and answer the following questions. Note that depending on which model you used (the simple or complex one) your answers might be different from that of others. "]},{"cell_type":"markdown","metadata":{"id":"uz1meXeiBfGp"},"source":["**What range of dropout probability values tend to produce the highest validation accuracy?**"]},{"cell_type":"markdown","metadata":{"deletable":false,"nbgrader":{"cell_type":"markdown","checksum":"0f0013d4e07104a03b4d51664a308f53","grade":true,"grade_id":"cell-4e0cb7a9240b1531","locked":false,"points":0,"schema_version":3,"solution":true,"task":false},"id":"xjxa3AaoBfGp"},"source":["YOUR ANSWER HERE"]},{"cell_type":"markdown","metadata":{"id":"yiha7_G-BfGp"},"source":["**What range of maxnorm weight constraints tend to produce the highest validation accuracy?**"]},{"cell_type":"markdown","metadata":{"deletable":false,"nbgrader":{"cell_type":"markdown","checksum":"9fd88f0bb870a910b925d60b38d17694","grade":true,"grade_id":"cell-99539755d7d328f7","locked":false,"points":0,"schema_version":3,"solution":true,"task":false},"id":"ftoVpYqIBfGp"},"source":["YOUR ANSWER HERE"]},{"cell_type":"markdown","metadata":{"id":"4M7ZARjJBfGp"},"source":["**When taken together, what combinations of dropout probability and maxnorm weight constraints tend to produce the highest validation accuracy?**"]},{"cell_type":"markdown","metadata":{"deletable":false,"nbgrader":{"cell_type":"markdown","checksum":"fee1e09ed8f6d354bd7b6e2986c2b811","grade":true,"grade_id":"cell-5e19a56b4a2d975d","locked":false,"points":0,"schema_version":3,"solution":true,"task":false},"id":"rakqIiHNBfGq"},"source":["YOUR ANSWER HERE"]},{"cell_type":"markdown","metadata":{"id":"VnwcQRx3BfGr"},"source":["**Do you think that using dropout was helpful in increasing model performance?**"]},{"cell_type":"markdown","metadata":{"deletable":false,"nbgrader":{"cell_type":"markdown","checksum":"de9c1bcff3c5eb6266cc80632d0956f0","grade":true,"grade_id":"cell-d2a2f7b284c801dc","locked":false,"points":0,"schema_version":3,"solution":true,"task":false},"id":"mtdlzA8WBfGr"},"source":["YOUR ANSWER HERE"]},{"cell_type":"markdown","metadata":{"id":"r3qKJpF_BfGs"},"source":["_____\n","\n","#Part 2: Model Deployment\n","# Experiment 4: Train, Save, and Load a Keras model\n","\n","Let's get some practice with how to save and load trained Keras models \n","\n","For this experiment, review the section on Saving and Loading models from the guided project, then: \n","\n","- Build a model of your choosing\n","- Gridsearch the model with a method of your choosing\n","- Save the trained model to a file\n","- Load the trained model from the file\n","- Just as we did in the Guided Project, evaluate the loaded model using a test set and verify that the results of the loaded model match that of the original model that was saved."]},{"cell_type":"code","metadata":{"id":"aki0LkDpBfGs"},"source":["# YOUR CODE HERE"],"execution_count":null,"outputs":[]}]}