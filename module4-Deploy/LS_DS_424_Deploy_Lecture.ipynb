{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "LS_DS_424_Deploy_Lecture.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "py37  (Python3)",
      "language": "python",
      "name": "py37"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "toc-autonumbering": false,
    "toc-showmarkdowntxt": false
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryanleeallred/DS-Unit-4-Sprint-2-Neural-Networks/blob/main/module4-Deploy/LS_DS_424_Deploy_Lecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJzTIkYAsLxw"
      },
      "source": [
        "Lambda School Data Science\n",
        "\n",
        "*Unit 4, Sprint 2, Module 4*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCf3jDMVQHuI"
      },
      "source": [
        "# Neural Network Frameworks (Prepare)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR0XBF5HQHuI"
      },
      "source": [
        "## Learning Objectives\n",
        "* <a href=\"#p1\">Part 1</a>: Implement Regularization Strategies\n",
        "* <a href=\"#p2\">Part 2</a>: Deploy a Keras Model\n",
        "* <a href=\"#p3\">Part 3</a>: Write a Custom Callback Function (Optional)\n",
        "\n",
        "Today's class will also focus heavily on callback objects. We will use various callbacks to monitor and manipulate our models based on data that our model produces at the end of an epoch.\n",
        "\n",
        "> A callback is an object that can perform actions at various stages of training (e.g., at the start or end of an epoch, before or after a single batch, etc.). -- [Keras Documentation](https://keras.io/api/callbacks/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWuoXZCCKCI7"
      },
      "source": [
        "# Regularization Strategies (Learn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3aMJZuPQHur"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Neural networks are highly parameterized models and can easily overfit the training data. The most salient way to combat this problem is with regularization strategies.  \n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/thumb/0/02/Regularization.svg/1920px-Regularization.svg.png)\n",
        "\n",
        "There are four common ways of regularization in neural networks, which we will cover briefly.  Here's a quick summary of how to apply them: \n",
        "\n",
        "1. Always use EarlyStopping. This strategy will prevent your weights from being updated well past the point of their peak usefulness.\n",
        "2. Use EarlyStopping, Weight Decay, and Dropout\n",
        "3. Use EarlyStopping, Weight Constraint, and Dropout\n",
        "\n",
        "Weight Decay and weight constraint accomplish similar purposes - preventing overfitting the parameters by regularizing the values. However, the mechanics are just slightly different. That's why you would not necessarily want to apply them together."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FFhK0tLQHus"
      },
      "source": [
        "## Follow Along"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO74ukb-QHus"
      },
      "source": [
        "### Early Stopping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgQR_iCHSF1x"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-xLR4Asmy3A"
      },
      "source": [
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "# load in our dataset \n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LErVFAGhnUaj"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "image_id = 500\n",
        "plt.imshow(X_train[image_id]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jY7KV_7xnip_"
      },
      "source": [
        "# normalize pixel values between 0 and 1 \n",
        "max_pixel_value = 255\n",
        "X_train, X_test = X_train /max_pixel_value , X_test / max_pixel_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gc1izLZkF8hQ"
      },
      "source": [
        "### Build a Neural Network that Uses EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "id": "AY1HomhxQHus",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "2e861d2ed558a4732f43d25fa52ecd4c",
          "grade": false,
          "grade_id": "cell-086a5f430505c39e",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
        "from tensorflow.keras.layers import Flatten, Dense\n",
        "from tensorflow.keras.layers import ReLU\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "# 1) Create 2 dir for logging files \n",
        "# create 2 dir -- one for tensorboard results and one for early stopping\n",
        "\n",
        "\n",
        "# 2) Instantiate the callbacks \n",
        "# instantiate a tensorboard callback object\n",
        "\n",
        "\n",
        "# instantiate an early stopping callback object \n",
        "# docs: https://keras.io/api/callbacks/early_stopping/\n",
        "\n",
        "\n",
        "# 3) Build the model \n",
        "\n",
        "# instantiate Sequential class\n",
        "\n",
        "# flatten images \n",
        "\n",
        "# hidden layer 1\n",
        "\n",
        "# act func 1\n",
        "\n",
        "# hidden layer 2\n",
        "\n",
        "# act func 2\n",
        "\n",
        "# hidden layer 3\n",
        "\n",
        "# act func 3\n",
        "\n",
        "# output layer \n",
        "\n",
        "# compile model \n",
        "\n",
        "# fit model \n",
        "\n",
        "\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SkLOV087U9H"
      },
      "source": [
        "# Clear any logs from previous runs\n",
        "# !rm -rf ./logs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3LFxeg1pJFE"
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7ScqKUzQHuv"
      },
      "source": [
        "---\n",
        "\n",
        "### Weight Decay (a.k.a Weight Shrinkage)\n",
        "\n",
        "```python\n",
        "Dense(64, input_dim=64,\n",
        "            kernel_regularizer=regularizers.l2(0.01),\n",
        "            activity_regularizer=regularizers.l1(0.01)))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzHG6BvVQriY"
      },
      "source": [
        "![](https://qph.fs.quoracdn.net/main-qimg-9d0dbf8074761b541ba80543ddfc9f73.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhMiDociF8hT"
      },
      "source": [
        "In the above image with the blue diamond and circle, remember that: \n",
        "\n",
        "1. The X and Y-axis represent possible values for model weights. In the case of this visualization, we have w1 and w2.\n",
        "2. The red dot represents the tangent line (the point of contact) between the error surface (represented by the contour map) and the unit weights (represented by the blue shapes). \n",
        "3. The red dot also tells us the weight values at the point of contact. \n",
        "4. What determines the geometry of the blue shapes are their respective distance metrics. \n",
        "5. The norm of the weights determines where the point of contact will occur. And the norm of the weights is determined by which metric space **p** we are getting the norm equation from. \n",
        "\n",
        "$${\\displaystyle \\left\\|x\\right\\|_{p}=\\left(|x_{1}|^{p}+|x_{2}|^{p}+\\dotsb +|x_{n}|^{p}\\right)^{1/p}.}$$  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCCZa9TYQrsc"
      },
      "source": [
        "## Regularization Take-Aways\n",
        "\n",
        "Almost remember that:\n",
        "\n",
        "1. Ridge (l2) and Lasso (l1) are 2 out of possibly infinitely many ways to regularize a model by [**using a distance metric in Lp space.**](https://en.wikipedia.org/wiki/Lp_space) \n",
        "\n",
        "2. Both L2 and L1 are used to help prevent overfitting. \n",
        "\n",
        "3. **The key difference between L1 and L2** is that L1 will calculate zero-valued feature weights (i.e., **w = 0**) for a subset of features; usually, redundant information is encoded in that subset of features; mathematically, this is referred to as [**MultiCollinearity**](https://en.wikipedia.org/wiki/Multicollinearity). In contrast, L2 will shrink the value of all feature weights but rarely down to zero. \n",
        "\n",
        "**Take Away: L1 drops features while L2 shrinks them and keeps them.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2pbTvNhQrvZ"
      },
      "source": [
        "![](https://i.stack.imgur.com/4KSgs.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CT3OD84MQrxq"
      },
      "source": [
        "The above image shows us the geometry of 4 specific Lp spaces. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Em8GZ4xaF8hU"
      },
      "source": [
        "### Build a Neural Net Using  $L^p$ Regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "id": "eQuacT-JQHuv",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "eccd417780af3f91fd3f3bea5bba0770",
          "grade": false,
          "grade_id": "cell-6689ef291b96ae6b",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "from tensorflow.keras import regularizers\n",
        "\n",
        "# build a 3 hidden layer NN using Lp regularization and using both tensorboard and early stopping callbacks \n",
        "\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPA7EHOpxg2z"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XP12lHBnxFYe"
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCJ1aUIfQHuy"
      },
      "source": [
        "----\n",
        "\n",
        "### Weight Constraint\n",
        "\n",
        "```python\n",
        "tf.keras.constraints.MaxNorm(\n",
        "    max_value=2, axis=0\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GbpDVMPF8hW"
      },
      "source": [
        "![](https://qph.fs.quoracdn.net/main-qimg-9d0dbf8074761b541ba80543ddfc9f73.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9s5p320cF8hW"
      },
      "source": [
        "The weight constraint provides a maximum value for the normalized value of these weight norm shapes (represented by the mathematical notation at the bottom of each blue shape. The subscript (or superscript) indicates which $L^p$ space that norm was calculated.\n",
        "\n",
        "$${\\displaystyle \\left\\|x\\right\\|_{p}=\\left(|x_{1}|^{p}+|x_{2}|^{p}+\\dotsb +|x_{n}|^{p}\\right)^{1/p}.}$$  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "id": "gbXBxr1QQHuy",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d865fb010e778dc161413adaf9dc7f82",
          "grade": false,
          "grade_id": "cell-bbb50a7f009726ce",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "from tensorflow.keras.constraints import MaxNorm\n",
        "\n",
        "# build a 3 hidden layer NN using MaxNorm regularization and using both tensorboard and early stopping callbacks\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nfuTyj54Qjf"
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-xMnXMsQHu0"
      },
      "source": [
        "-----\n",
        "### Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEJcMv4IF8hX"
      },
      "source": [
        "![](https://miro.medium.com/max/981/1*EinUlWw1n8vbcLyT0zx4gw.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IR03VfQAF8hX"
      },
      "source": [
        "If interested, feel free to read through the original publication on [**Drop Out**](https://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf). \n",
        "\n",
        "**Key Takeaways:** \n",
        "\n",
        "1. During training, dropout will probabilistically \"turn off\" some neurons in the layer that dropout is implemented in. \n",
        "2. All neurons are used during inference (i.e., making predictions on the test set) (i.e., no dropout is applied).\n",
        "3. Dropout works best when used with MaxNorm\n",
        "\n",
        "Here are some article excerpts on how dropout works. \n",
        "\n",
        "\"Dropout can be interpreted as a way of regularizing a neural network by adding noise to\n",
        "its hidden units.\" (page 2)\n",
        "\n",
        "\"Combining several models [model ensembles] is most\n",
        "helpful when the individual models are different from each other and to make\n",
        "neural net models different, they should either have different architectures or be trained\n",
        "on different data...It prevents overfitting and\n",
        "provides a way of approximately combining exponentially many different neural network\n",
        "architectures efficiently.\" (page 2)\n",
        "\n",
        "\"Training the **norm of the incoming weight vector** at each hidden unit to be upper\n",
        "bounded by a fixed constant c. In other words, if w represents the vector of weights incident\n",
        "on any hidden unit, the neural network was optimized under the constraint **||w||2 â‰¤ c**. \n",
        "\n",
        "This constraint was imposed during optimization by **projecting w onto the surface of a ball of\n",
        "radius c, whenever w went out of it**. This is also called **max-norm regularization** since it\n",
        "implies that the maximum value that the norm of any weight can take is c. The constant\n",
        "c is a tunable hyperparameter, which is determined using a validation set.\" (page 6)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "id": "MCRL8SmgQHu0",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "7a0485222190198be1429998af512fe0",
          "grade": false,
          "grade_id": "cell-269ab54e72cc7607",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFAkmZbVQHu2"
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbV6zsdQQHu4"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "You will apply regularization strategies inside your neural network today as you try to avoid overfitting it. \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MXbzdBdURod"
      },
      "source": [
        "# Deploy (Learn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11LJtU9MUVD5"
      },
      "source": [
        "## Overview\n",
        "\n",
        "You've built a dope image classification model, but it's just sitting in your Jupyter Notebook. What now? Well, you deploy to some downstream application. TensorFlow supports three ways of deploying its models:\n",
        "\n",
        "- In-Browser with TensorFlow.js\n",
        "- API with TensorFlow Serving (TFX) or another Framework\n",
        "- On-Device with TensorFlow Lite\n",
        "\n",
        "You are already familiar with deploying a model as an API from Unit 3, so we will deploy a model in browser. Both methods rely on the same core idea:\n",
        "  - Save your weights and architecture information.\n",
        "  - Load those parameters into the application.\n",
        "  - Perform inference. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHca-4H2UU8p"
      },
      "source": [
        "## Follow Along"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mP0IIAe4LFW5"
      },
      "source": [
        "### Checkpoint\n",
        "Save the latest weights of your model at the end of each epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmtMJBBriD2Q"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "cpoint = tf.keras.callbacks.ModelCheckpoint(\"weights_best.h5\",\n",
        "                                            verbose=1, \n",
        "                                            save_weights_only=True)\n",
        "\n",
        "def create_model():\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "          Flatten(input_shape=(28,28)),\n",
        "          Dense(128),\n",
        "          ReLU(negative_slope=.01),\n",
        "          Dense(128),\n",
        "          ReLU(negative_slope=.01),\n",
        "          Dense(128),\n",
        "          ReLU(negative_slope=.01),\n",
        "          Dense(10, activation='softmax')\n",
        "        ])\n",
        "\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "model = create_model()\n",
        "\n",
        "model.fit(X_train, y_train, epochs=2, \n",
        "          validation_data=(X_test,y_test),\n",
        "          verbose=2,\n",
        "          callbacks=[cpoint])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5QoKB14LNMb"
      },
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eH9jo6gl-SMq"
      },
      "source": [
        "# create a compiled model and return it \n",
        "\n",
        "\n",
        "m = create_model()\n",
        "m.load_weights('./weights_best.h5')\n",
        "\n",
        "m.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQl7g-ImLUY9"
      },
      "source": [
        "m.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGzTQU1_Lnsb"
      },
      "source": [
        "### Save Entire Model\n",
        "This method includes both the weights and architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXtxgTgYL0UC"
      },
      "source": [
        "# Create and train a new model instance.\n",
        "model = create_model()\n",
        "model.fit(X_train,y_train, epochs=5)\n",
        "\n",
        "# Save the entire model as a SavedModel.\n",
        "!mkdir -p saved_model\n",
        "model.save('saved_model/my_model') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kURHysaUL87B"
      },
      "source": [
        "Load a fresh model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCgOlQalMIQr"
      },
      "source": [
        "new_model = tf.keras.models.load_model('saved_model/my_model')\n",
        "\n",
        "# Check its architecture\n",
        "new_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qo8zSTTypNC4"
      },
      "source": [
        "new_model.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYFdwhU4_Wm9"
      },
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i7OMOhzc6WT"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "You will be expected to be able to export your model weights and architecture on the assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXnXxPCZiAb2"
      },
      "source": [
        "# Custom Callbacks (Learn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6vO6xbkiFGb"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Custom callbacks allow you to access data at any point during the training: on batch end, on epoch end, on epoch start, on batch start. Our use case today is a simple one. Let's stop training once we reach a benchmark accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCocRF5CiYG_"
      },
      "source": [
        "## Follow Along"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOisQxYlibOi"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "Experiment with improving our custom callback function. "
      ]
    }
  ]
}